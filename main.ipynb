{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpkYHwCqk7W-"
      },
      "source": [
        "![MuJoCo banner](https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png)\n",
        "\n",
        "# <h1><center>Tutorial  <a href=\"https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "This notebook provides an introductory tutorial for [**MuJoCo XLA (MJX)**](https://github.com/google-deepmind/mujoco/blob/main/mjx), a JAX-based implementation of MuJoCo useful for RL training workloads.\n",
        "\n",
        "**A Colab runtime with GPU acceleration is required.** If you're using a CPU-only runtime, you can switch using the menu \"Runtime > Change runtime type\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['MUJOCO_GL'] = 'egl' # Ensure EGL rendering is used\n",
        "\n",
        "from datetime import datetime\n",
        "from etils import epath\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "import os\n",
        "from ml_collections import config_dict\n",
        "\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from flax.training import orbax_utils\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv6WUVUm78k"
      },
      "source": [
        "# Simple ENV with spider\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Version of Spider"
      ],
      "metadata": {
        "id": "oUAkjXphvPLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a simple spider XML model\n",
        "spider_xml = \"\"\"\n",
        "<mujoco>\n",
        "  <worldbody>\n",
        "    <light name=\"top\" pos=\"0 0 1\"/>\n",
        "    <geom name=\"floor\" type=\"plane\" size=\"10 10 .1\" rgba=\".9 .9 .9 1\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 0.25\">\n",
        "      <geom type=\"sphere\" size=\"0.1\" rgba=\"1 0 0 1\"/>\n",
        "      <joint type=\"free\"/>\n",
        "      <body name=\"leg1\" pos=\"0.1 0 0\">\n",
        "        <joint name=\"joint1\" type=\"hinge\" axis=\"0 1 0\" pos=\"0 0 0\"/>\n",
        "        <geom type=\"capsule\" fromto=\".0 .0 .0 .3 .0 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        <body name=\"foot1\" pos=\".3 0 0\">\n",
        "          <joint name=\"joint2\" type=\"hinge\" axis=\"0 1 0\" pos=\"0 0 0\"/>\n",
        "          <geom type=\"capsule\" fromto=\".0 .0 .0 .3 .0 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        </body>\n",
        "      </body>\n",
        "       <body name=\"leg2\" pos=\"-0.1 0 0\">\n",
        "        <joint name=\"joint3\" type=\"hinge\" axis=\"0 1 0\" pos=\"0 0 0\"/>\n",
        "        <geom type=\"capsule\" fromto=\".0 .0 .0 -.3 .0 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        <body name=\"foot2\" pos=\"-.3 0 0\">\n",
        "          <joint name=\"joint4\" type=\"hinge\" axis=\"0 1 0\" pos=\"0 0 0\"/>\n",
        "          <geom type=\"capsule\" fromto=\".0 .0 .0 -.3 .0 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        </body>\n",
        "      </body>\n",
        "       <body name=\"leg3\" pos=\"0 0.1 0\">\n",
        "        <joint name=\"joint5\" type=\"hinge\" axis=\"1 0 0\" pos=\"0 0 0\"/>\n",
        "        <geom type=\"capsule\" fromto=\".0 .0 .0 .0 .3 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "         <body name=\"foot3\" pos=\"0 .3 0\">\n",
        "          <joint name=\"joint6\" type=\"hinge\" axis=\"1 0 0\" pos=\"0 0 0\"/>\n",
        "          <geom type=\"capsule\" fromto=\".0 .0 .0 .0 .3 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        </body>\n",
        "      </body>\n",
        "       <body name=\"leg4\" pos=\"0 -0.1 0\">\n",
        "        <joint name=\"joint7\" type=\"hinge\" axis=\"1 0 0\" pos=\"0 0 0\"/>\n",
        "        <geom type=\"capsule\" fromto=\".0 .0 .0 .0 -.3 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        <body name=\"foot4\" pos=\"0 -.3 0\">\n",
        "          <joint name=\"joint8\" type=\"hinge\" axis=\"1 0 0\" pos=\"0 0 0\"/>\n",
        "          <geom type=\"capsule\" fromto=\".0 .0 .0 .0 -.3 .0\" size=\".02\" rgba=\"0 1 0 1\"/>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor name=\"act1\" joint=\"joint1\"/>\n",
        "    <motor name=\"act2\" joint=\"joint2\"/>\n",
        "    <motor name=\"act3\" joint=\"joint3\"/>\n",
        "    <motor name=\"act4\" joint=\"joint4\"/>\n",
        "    <motor name=\"act5\" joint=\"joint5\"/>\n",
        "    <motor name=\"act6\" joint=\"joint6\"/>\n",
        "    <motor name=\"act7\" joint=\"joint7\"/>\n",
        "    <motor name=\"act8\" joint=\"joint8\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PQW-aJKInf8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Version of spider"
      ],
      "metadata": {
        "id": "WDfVKuffvTDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spider_xml = \"\"\"\n",
        "<mujoco model=\"ant\">\n",
        "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
        "  <option integrator=\"RK4\" timestep=\"0.01\"/>\n",
        "  <custom>\n",
        "    <numeric data=\"0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0\" name=\"init_qpos\"/>\n",
        "  </custom>\n",
        "  <default>\n",
        "    <joint armature=\"1\" damping=\"1\" limited=\"true\"/>\n",
        "    <geom conaffinity=\"0\" condim=\"3\" density=\"5.0\" friction=\"1 0.5 0.5\" margin=\"0.01\" rgba=\"0.8 0.6 0.4 1\"/>\n",
        "  </default>\n",
        "  <asset>\n",
        "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
        "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
        "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
        "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
        "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
        "  </asset>\n",
        "  <worldbody>\n",
        "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
        "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 0.75\">\n",
        "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
        "      <geom name=\"torso_geom\" pos=\"0 0 0\" size=\"0.25\" type=\"sphere\"/>\n",
        "      <joint armature=\"0\" damping=\"0\" limited=\"false\" margin=\"0.01\" name=\"root\" pos=\"0 0 0\" type=\"free\"/>\n",
        "      <body name=\"front_left_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"aux_1_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_1\" pos=\"0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_1\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"left_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_1\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 0.4 0.0\" name=\"left_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"front_right_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"aux_2_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_2\" pos=\"-0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_2\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"right_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_2\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 0.4 0.0\" name=\"right_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"aux_3_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_3\" pos=\"-0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_3\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"back_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 -0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_3\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 -0.4 0.0\" name=\"third_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"right_back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"aux_4_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_4\" pos=\"0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_4\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"rightback_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 -0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_4\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 -0.4 0.0\" name=\"fourth_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_3\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_3\" gear=\"150\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sj_Kkg0Uqx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spider Env"
      ],
      "metadata": {
        "id": "6ZbTRbgovaJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtGMYNLE3QJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# HUMANOID_ROOT_PATH = epath.Path(epath.resource_path('mujoco')) / 'mjx/test_data/humanoid'\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=10.0,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=2.5,\n",
        "      terminate_when_unhealthy=False, # Set to False to prevent early termination\n",
        "      healthy_z_range=(0.1, 0.3), # Lowered healthy_z_range for a spider\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      episode_length: int = 1000, # <--- Added episode_length with default\n",
        "      **kwargs,\n",
        "  ):\n",
        "#\n",
        "\n",
        "\n",
        "    mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "    mj_data = mujoco.MjData(mj_model) # Create mj_data\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "    # Store episode_length as an attribute of the Humanoid instance\n",
        "    self.episode_length = episode_length\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "    self._torso_body_idx = mujoco.mj_name2id(\n",
        "        self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    # Store mj_data for access to qpos0 and qvel0\n",
        "    self._mj_data = mj_data\n",
        "\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = jp.asarray(self._mj_data.qpos) + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    # Calculate forward velocity using the torso's velocity\n",
        "    torso_velocity = data.cvel[self._torso_body_idx, 0]\n",
        "    forward_reward = self._forward_reward_weight * torso_velocity\n",
        "\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    obs = self._get_obs(data, action)\n",
        "    reward = forward_reward + healthy_reward - ctrl_cost\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=data.xpos[self._torso_body_idx, 0],\n",
        "        y_position=data.xpos[self._torso_body_idx, 1],\n",
        "        distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "        x_velocity=torso_velocity,\n",
        "        y_velocity=data.cvel[self._torso_body_idx, 1],\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "    # Simplified observation: torso z-position and x-velocity\n",
        "    # return jp.concatenate([\n",
        "    #     data.qpos[2:3],  # Torso z-position\n",
        "    #     data.cvel[self._torso_body_idx, 0:1], # Torso x-velocity\n",
        "    # ])\n",
        "\n",
        "    # More detailed observation for a spider: torso z-position, torso x-velocity,\n",
        "    # and joint positions and velocities\n",
        "    return jp.concatenate([\n",
        "        data.qpos[2:3],  # Torso z-position\n",
        "        data.cvel[self._torso_body_idx, 0:1], # Torso x-velocity\n",
        "        data.qpos[7:], # Joint positions (excluding free joint)\n",
        "        data.qvel[6:], # Joint velocities (excluding free joint)\n",
        "    ])\n",
        "\n",
        "\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "# Add code to get and print the observation shape\n",
        "env_test = Humanoid()\n",
        "dummy_data = mujoco.MjData(env_test.sys.mj_model) # Use mj_data\n",
        "dummy_obs = env_test._get_obs(env_test.pipeline_init(jp.asarray(dummy_data.qpos), jp.asarray(dummy_data.qvel)), jp.zeros(env_test.sys.nu))\n",
        "print(\"Observation shape:\", dummy_obs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(Optional) Show Data class for inspection"
      ],
      "metadata": {
        "id": "zd-wW1ANxPcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import jax\n",
        "# import mujoco\n",
        "# from brax import envs\n",
        "# import numpy as np\n",
        "\n",
        "# # Instantiate the environment\n",
        "# env_name = 'humanoid'\n",
        "# env = envs.get_environment(env_name)\n",
        "\n",
        "# # Define the jit reset function\n",
        "# jit_reset = jax.jit(env.reset)\n",
        "\n",
        "# # Reset the environment to get an initial state\n",
        "# rng = jax.random.PRNGKey(0)\n",
        "# state = jit_reset(rng)\n",
        "\n",
        "# # The mjx.Data object is stored in state.pipeline_state\n",
        "# data = state.pipeline_state\n",
        "\n",
        "# print(\"--- Contents of mjx.Data object ---\")\n",
        "# print(f\"Type of data: {type(data)}\\n\")\n",
        "\n",
        "# print(\"--- All attributes of mjx.Data object ---\\n\")\n",
        "# for attr_name in sorted(dir(data)):\n",
        "#     if not attr_name.startswith('_'): # Exclude private attributes\n",
        "#         attr_value = getattr(data, attr_name)\n",
        "#         attr_type = type(attr_value)\n",
        "#         if isinstance(attr_value, (np.ndarray, jax.Array)): # Check if it's a JAX or NumPy array\n",
        "#             print(f\"  Attribute: {attr_name}, Type: {attr_type}, Shape: {attr_value.shape}\")\n",
        "#         else:\n",
        "#             print(f\"  Attribute: {attr_name}, Type: {attr_type}\")\n",
        "\n",
        "# print(\"\\n--- Specific attributes shown before ---\")\n",
        "# print(\"1. Generalized positions (qpos):\")\n",
        "# print(f\"  Shape: {data.qpos.shape}\")\n",
        "# print(f\"  Value (first 10 elements): {data.qpos[:10]}\\n\")\n",
        "\n",
        "# print(\"2. Generalized velocities (qvel):\")\n",
        "# print(f\"  Shape: {data.qvel.shape}\")\n",
        "# print(f\"  Value (first 10 elements): {data.qvel[:10]}\\n\")\n",
        "\n",
        "# print(\"3. Cartesian position of bodies (xpos):\")\n",
        "# print(f\"  Shape: {data.xpos.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.xpos[:5]}\\n\")\n",
        "\n",
        "# print(\"4. Cartesian orientation of bodies (xquat):\")\n",
        "# print(f\"  Shape: {data.xquat.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.xquat[:5]}\\n\")\n",
        "\n",
        "# print(\"5. Center of mass velocity (cvel):\")\n",
        "# print(f\"  Shape: {data.cvel.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.cvel[:5]}\\n\")\n",
        "\n",
        "# print(\"6. Body indices (for reference, not part of data object directly but useful):\")\n",
        "# print(f\"  Torso body index: {env._torso_body_idx}\")"
      ],
      "metadata": {
        "id": "ikGX1ZRMv9t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Inspect action"
      ],
      "metadata": {
        "id": "D3icJ4W30vrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import jax\n",
        "# from brax import envs\n",
        "# import jax.numpy as jp\n",
        "# from brax.training.agents.ppo import networks as ppo_networks\n",
        "# from brax.training.agents.ppo import train as ppo\n",
        "# from brax.io import model\n",
        "\n",
        "# # Re-instantiate the environment if not already available in this scope\n",
        "# env_name = 'humanoid'\n",
        "# env = envs.get_environment(env_name)\n",
        "\n",
        "# # Define and train a minimal policy if jit_inference_fn is not defined\n",
        "# try:\n",
        "#     # Attempt to use jit_inference_fn if already defined (e.g., from a previous run of training cells)\n",
        "#     _ = jit_inference_fn\n",
        "# except NameError:\n",
        "#     print(\"jit_inference_fn not found, training a minimal policy...\")\n",
        "#     make_inference_fn, params, _ = ppo.train(\n",
        "#         environment=env, num_timesteps=5_000, num_evals=1, episode_length=env.episode_length # Minimal training just for this demo\n",
        "#     )\n",
        "#     model_path = '/tmp/mjx_brax_policy'\n",
        "#     model.save_params(model_path, params)\n",
        "#     params = model.load_params(model_path)\n",
        "#     inference_fn = make_inference_fn(params)\n",
        "#     jit_inference_fn = jax.jit(inference_fn)\n",
        "#     print(\"Minimal policy trained and jit_inference_fn defined.\")\n",
        "\n",
        "# # Initialize the state\n",
        "# current_rng = jax.random.PRNGKey(1) # Use a new RNG key\n",
        "# state = jax.jit(env.reset)(current_rng)\n",
        "\n",
        "# # Generate an action using the inference function\n",
        "# act_rng, current_rng = jax.random.split(current_rng)\n",
        "# ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "\n",
        "# print(\"--- Content of the 'action' variable (ctrl from policy) ---\")\n",
        "# print(f\"Type of action: {type(ctrl)}\")\n",
        "# print(f\"Shape of action: {ctrl.shape}\")\n",
        "# print(f\"Value of action: {ctrl}\")\n"
      ],
      "metadata": {
        "id": "wx_QtlDdzlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K6IznI2y83"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout.\n",
        "\n",
        "NOTE: Since episodes terminate early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKLFK54C1CH"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph8u-v2Q2xLS"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(100):\n",
        "  ctrl = jp.zeros(env.sys.nu) # Set control input to zero for standing still\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "media.show_video(env.render(rollout), fps=1.0 / env.dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O0zQEttGONfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation space:\", env.observation_size)"
      ],
      "metadata": {
        "id": "42pERvKYOOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDG6NQ1CbZD"
      },
      "source": [
        "# Train Humanoid Policy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importujemy 'networks' bezpośrednio z modułu agenta PPO\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "# Typ PPONetworks jest teraz w 'brax.training.types'\n",
        "from brax.training.agents.ppo.networks import PPONetworks\n",
        "from brax.io import html, model\n",
        "import flax.linen as nn\n",
        "\n",
        "def my_custom_network_factory(\n",
        "    observation_size: int,\n",
        "    action_size: int,\n",
        "    preprocess_observations_fn=None,\n",
        "    hidden_layer_sizes=(512, 256),\n",
        "    activation=nn.relu\n",
        ") -> PPONetworks:\n",
        "    \"\"\"Tworzy sieć PPO o niestandardowej architekturze.\"\"\"\n",
        "\n",
        "    return ppo_networks.make_networks(\n",
        "        observation_size=observation_size,\n",
        "        action_size=action_size,\n",
        "        preprocess_observations_fn=preprocess_observations_fn,\n",
        "        hidden_layer_sizes=hidden_layer_sizes,\n",
        "        activation=activation\n",
        "    )\n",
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=20_000_00, num_evals=5, reward_scaling=0.1,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=24, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=3072,\n",
        "    batch_size=512, seed=0)\n",
        "\n",
        "# starty, nagrody na wykresie\n",
        "x_data = []\n",
        "y_data = [] # eval/episode_reward\n",
        "ydataerr = [] # eval/episode_reward_std\n",
        "\n",
        "# New lists for other reward components\n",
        "y_data_forward = []\n",
        "ydataerr_forward = []\n",
        "y_data_quadctrl = []\n",
        "ydataerr_quadctrl = []\n",
        "y_data_alive = []\n",
        "ydataerr_alive = []\n",
        "\n",
        "times = [datetime.now()]\n",
        "\n",
        "max_y, min_y = 5000, -2000 # Adjusted max_y and min_y for potentially higher/lower rewards\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "\n",
        "  # Append data for new metrics, using .get() with default 0.0 to handle initial missing keys\n",
        "  y_data_forward.append(metrics.get('eval/episode_metrics/forward_reward', 0.0))\n",
        "  ydataerr_forward.append(metrics.get('eval/episode_metrics/forward_reward_std', 0.0))\n",
        "  y_data_quadctrl.append(metrics.get('eval/episode_metrics/reward_quadctrl', 0.0))\n",
        "  ydataerr_quadctrl.append(metrics.get('eval/episode_metrics/reward_quadctrl_std', 0.0))\n",
        "  y_data_alive.append(metrics.get('eval/episode_metrics/reward_alive', 0.0))\n",
        "  ydataerr_alive.append(metrics.get('eval/episode_metrics/reward_alive_std', 0.0))\n",
        "\n",
        "  plt.figure(figsize=(12, 16)) # Create a new figure for multiple subplots\n",
        "\n",
        "  # Plot 1: Total Episode Reward\n",
        "  plt.subplot(4, 1, 1) # 4 rows, 1 column, first plot\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('Reward per episode')\n",
        "  plt.title(f'Total Episode Reward: {y_data[-1]:.3f}')\n",
        "  plt.errorbar(x_data, y_data, yerr=ydataerr, label='Total Reward')\n",
        "  plt.legend()\n",
        "\n",
        "  # # Plot 2: Forward Reward\n",
        "  # plt.subplot(4, 1, 2)\n",
        "  # plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
        "  # plt.xlabel('# environment steps')\n",
        "  # plt.ylabel('Forward Reward')\n",
        "  # plt.title(f'Forward Reward: {y_data_forward[-1]:.3f}')\n",
        "  # plt.errorbar(x_data, y_data_forward, yerr=ydataerr_forward, label='Forward Reward', color='green')\n",
        "  # plt.legend()\n",
        "\n",
        "  # # Plot 3: Control Cost Reward\n",
        "  # plt.subplot(4, 1, 3)\n",
        "  # plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
        "  # plt.xlabel('# environment steps')\n",
        "  # plt.ylabel('Control Cost Reward')\n",
        "  # plt.title(f'Control Cost Reward: {y_data_quadctrl[-1]:.3f}')\n",
        "  # plt.errorbar(x_data, y_data_quadctrl, yerr=ydataerr_quadctrl, label='Control Cost', color='red')\n",
        "  # plt.legend()\n",
        "\n",
        "  # # Plot 4: Alive Reward\n",
        "  # plt.subplot(4, 1, 4)\n",
        "  # plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.25])\n",
        "  # plt.xlabel('# environment steps')\n",
        "  # plt.ylabel('Alive Reward')\n",
        "  # plt.title(f'Alive Reward: {y_data_alive[-1]:.3f}')\n",
        "  # plt.errorbar(x_data, y_data_alive, yerr=ydataerr_alive, label='Alive Reward', color='purple')\n",
        "  # plt.legend()\n",
        "\n",
        "  plt.tight_layout() # Adjust layout to prevent overlap\n",
        "  plt.show()\n",
        "\n",
        "print(\"Observation space size:\", env.observation_size)\n",
        "print(\"Action space size:\", env.action_size)\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ],
      "metadata": {
        "id": "XWV5jCfaUpIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIch0HEApBx"
      },
      "source": [
        "<!-- ## Save and Load Policy -->\n",
        "\n",
        "We can save and load the policy using the brax model API."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4Jk9BSbUbPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8gI6qH6ApBx"
      },
      "outputs": [],
      "source": [
        "#@title Save Model\n",
        "model_path = '/tmp/mjx_brax_policy'\n",
        "model.save_params(model_path, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4reaWgxApBx"
      },
      "outputs": [],
      "source": [
        "#@title Load Model and Define Inference Function\n",
        "params = model.load_params(model_path)\n",
        "\n",
        "inference_fn = make_inference_fn(params)\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G357XIfApBy"
      },
      "source": [
        "## Visualize Policy\n",
        "\n",
        "Finally we can visualize the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osYasMw4ApBy"
      },
      "outputs": [],
      "source": [
        "eval_env = envs.get_environment(env_name)\n",
        "\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-UhypudApBy"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "rng = jax.random.PRNGKey(0)\n",
        "state = jit_reset(rng)\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "n_steps = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "  if state.done:\n",
        "    break\n",
        "\n",
        "media.show_video(env.render(rollout[::render_every]), fps=1.0 / env.dt / render_every)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMHZ6IFlaSG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO - github implementation"
      ],
      "metadata": {
        "id": "2xIGm9hndmKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class NetworkBase(nn.Module, metaclass=ABCMeta):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        super(NetworkBase, self).__init__()\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Network(NetworkBase):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.relu,last_activation = None):\n",
        "        super(Network, self).__init__()\n",
        "        self.activation = activation_function\n",
        "        self.last_activation = last_activation\n",
        "        layers_unit = [input_dim]+ [hidden_dim]*(layer_num-1)\n",
        "        layers = ([nn.Linear(layers_unit[idx],layers_unit[idx+1]) for idx in range(len(layers_unit)-1)])\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.last_layer = nn.Linear(layers_unit[-1],output_dim)\n",
        "        self.network_init()\n",
        "    def forward(self, x):\n",
        "        return self._forward(x)\n",
        "    def _forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.last_layer(x)\n",
        "        if self.last_activation != None:\n",
        "            x = self.last_activation(x)\n",
        "        return x\n",
        "    def network_init(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.orthogonal_(layer.weight)\n",
        "                layer.bias.data.zero_()"
      ],
      "metadata": {
        "id": "SSTtLSBWdyj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Actor(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.tanh,last_activation = None, trainable_std = False):\n",
        "        super(Actor, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "        self.trainable_std = trainable_std\n",
        "        if self.trainable_std == True:\n",
        "            self.logstd = nn.Parameter(torch.zeros(1, output_dim))\n",
        "    def forward(self, x):\n",
        "        mu = self._forward(x)\n",
        "        if self.trainable_std == True:\n",
        "            std = torch.exp(self.logstd)\n",
        "        else:\n",
        "            logstd = torch.zeros_like(mu)\n",
        "            std = torch.exp(logstd)\n",
        "        return mu,std\n",
        "\n",
        "class Critic(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function, last_activation = None):\n",
        "        super(Critic, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "\n",
        "    def forward(self, *x):\n",
        "        x = torch.cat(x,-1)\n",
        "        return self._forward(x)\n",
        ""
      ],
      "metadata": {
        "id": "-IOjyENUdzbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Dict(dict):\n",
        "    def __init__(self,config,section_name,location = False):\n",
        "        super(Dict,self).__init__()\n",
        "        self.initialize(config, section_name,location)\n",
        "    def initialize(self, config, section_name,location):\n",
        "        for key,value in config.items(section_name):\n",
        "            if location :\n",
        "                self[key] = value\n",
        "            else:\n",
        "                self[key] = eval(value)\n",
        "    def __getattr__(self,val):\n",
        "        return self[val]\n",
        "\n",
        "def make_transition(state,action,reward,next_state,done,log_prob=None):\n",
        "    transition = {}\n",
        "    transition['state'] = state\n",
        "    transition['action'] = action\n",
        "    transition['reward'] = reward\n",
        "    transition['next_state'] = next_state\n",
        "    transition['log_prob'] = log_prob\n",
        "    transition['done'] = done\n",
        "    return transition\n",
        "\n",
        "def make_mini_batch(*value):\n",
        "    mini_batch_size = value[0]\n",
        "    full_batch_size = len(value[1])\n",
        "    full_indices = np.arange(full_batch_size)\n",
        "    np.random.shuffle(full_indices)\n",
        "    for i in range(full_batch_size // mini_batch_size):\n",
        "        indices = full_indices[mini_batch_size*i : mini_batch_size*(i+1)]\n",
        "        yield [x[indices] for x in value[1:]]\n",
        "\n",
        "def convert_to_tensor(*value):\n",
        "    device = value[0]\n",
        "    return [torch.tensor(x).float().to(device) for x in value[1:]]\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, action_prob_exist, max_size, state_dim, num_action):\n",
        "        self.max_size = max_size\n",
        "        self.data_idx = 0\n",
        "        self.action_prob_exist = action_prob_exist\n",
        "        self.data = {}\n",
        "\n",
        "        self.data['state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['action'] = np.zeros((self.max_size, num_action))\n",
        "        self.data['reward'] = np.zeros((self.max_size, 1))\n",
        "        self.data['next_state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['done'] = np.zeros((self.max_size, 1))\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'] = np.zeros((self.max_size, 1))\n",
        "    def put_data(self, transition):\n",
        "        idx = self.data_idx % self.max_size\n",
        "        self.data['state'][idx] = transition['state']\n",
        "        self.data['action'][idx] = transition['action']\n",
        "        self.data['reward'][idx] = transition['reward']\n",
        "        self.data['next_state'][idx] = transition['next_state']\n",
        "        self.data['done'][idx] = float(transition['done'])\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'][idx] = transition['log_prob']\n",
        "\n",
        "        self.data_idx += 1\n",
        "    def sample(self, shuffle, batch_size = None):\n",
        "        if shuffle :\n",
        "            sample_num = min(self.max_size, self.data_idx)\n",
        "            rand_idx = np.random.choice(sample_num, batch_size,replace=False)\n",
        "            sampled_data = {}\n",
        "            sampled_data['state'] = self.data['state'][rand_idx]\n",
        "            sampled_data['action'] = self.data['action'][rand_idx]\n",
        "            sampled_data['reward'] = self.data['reward'][rand_idx]\n",
        "            sampled_data['next_state'] = self.data['next_state'][rand_idx]\n",
        "            sampled_data['done'] = self.data['done'][rand_idx]\n",
        "            if self.action_prob_exist :\n",
        "                sampled_data['log_prob'] = self.data['log_prob'][rand_idx]\n",
        "            return sampled_data\n",
        "        else:\n",
        "            return self.data\n",
        "    def size(self):\n",
        "        return min(self.max_size, self.data_idx)\n",
        "class RunningMeanStd(object):\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count"
      ],
      "metadata": {
        "id": "nUhmYZbwd4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                actor_loss = (-torch.min(surr1, surr2) - entropy).mean()\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n"
      ],
      "metadata": {
        "id": "SYyu-Ijsdp_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO - training loop"
      ],
      "metadata": {
        "id": "xF8gG1nKe0nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 2048 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 3e-4\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 64 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name) # Removed episode_length=1000\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "# num_total_steps_pytorch = 1_000_000 # Total environment steps for PyTorch PPO\n",
        "current_total_steps = 0\n",
        "episode_count = 0\n",
        "rng = jax.random.PRNGKey(0) # JAX RNG for environment\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Change the while loop condition to run for 10 episodes\n",
        "while episode_count < 1:\n",
        "    episode_count += 1\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    episode_reward = 0\n",
        "    # Clear replay buffer for new trajectory collection (on-policy PPO)\n",
        "    ppo_torch_agent.data = ReplayBuffer(\n",
        "        action_prob_exist=True,\n",
        "        max_size=ppo_torch_args.traj_length,\n",
        "        state_dim=env.observation_size,\n",
        "        num_action=env.action_size\n",
        "    )\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length): # Collect for traj_length steps\n",
        "        # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "        obs_torch = torch.from_numpy(np.array(env_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "        # Get action from PyTorch actor\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            # --- DEBUG: Print action_torch shape ---\n",
        "            # print(f\"DEBUG: action_torch shape: {action_torch.shape}\")\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "        action_jax = jnp.asarray(action_torch.squeeze(0).cpu().numpy())\n",
        "        # --- DEBUG: Print action_jax shape ---\n",
        "        # print(f\"DEBUG: action_jax shape (after squeeze): {action_jax.shape}\")\n",
        "\n",
        "        # Step JAX environment\n",
        "        rng, step_rng = jax.random.split(rng) # Need a new rng for each step if needed by brax (jit_step doesn't take it)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (remove batch dimension where applicable)\n",
        "        obs_np = obs_torch.squeeze(0).cpu().numpy()\n",
        "        action_np = action_torch.squeeze(0).cpu().numpy()\n",
        "        reward_np = np.array(next_env_state.reward).reshape(1) # Ensure (1,) shape\n",
        "        next_obs_np = np.array(next_env_state.obs)\n",
        "        done_np = np.array(next_env_state.done).reshape(1)     # Ensure (1,) shape\n",
        "        log_prob_np = log_prob_torch.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Store transition in PyTorch ReplayBuffer\n",
        "        transition = make_transition(\n",
        "            obs_np,\n",
        "            action_np,\n",
        "            reward_np,\n",
        "            next_obs_np,\n",
        "            done_np,\n",
        "            log_prob_np\n",
        "        )\n",
        "        ppo_torch_agent.put_data(transition)\n",
        "\n",
        "        env_state = next_env_state\n",
        "        episode_reward += next_env_state.reward\n",
        "        current_total_steps += 1\n",
        "\n",
        "        if env_state.done:\n",
        "            print(f\"Episode {episode_count} finished early at step {t+1}. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "            break\n",
        "    else: # If loop completes without break\n",
        "      print(f\"Episode {episode_count} completed {ppo_torch_args.traj_length} steps. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length steps\n",
        "    if ppo_torch_agent.data.size() >= ppo_torch_args.traj_length:\n",
        "        ppo_torch_agent.train_net(episode_count)\n",
        "        print(f\"PPO agent trained for episode {episode_count}.\")\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "        # --- DEBUG: Print action_torch_eval shape ---\n",
        "        print(f\"DEBUG: action_torch_eval shape: {action_torch_eval.shape}\")\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "    # --- DEBUG: Print action_jax_eval shape ---\n",
        "    print(f\"DEBUG: action_jax_eval shape (after squeeze): {action_jax_eval.shape}\")\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)"
      ],
      "metadata": {
        "id": "uvqNqIbid8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "        # --- DEBUG: Print action_torch_eval shape ---\n",
        "        print(f\"DEBUG: action_torch_eval shape: {action_torch_eval.shape}\")\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "    # --- DEBUG: Print action_jax_eval shape ---\n",
        "    print(f\"DEBUG: action_jax_eval shape (after squeeze): {action_jax_eval.shape}\")\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)"
      ],
      "metadata": {
        "id": "SYnfhpblsmVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MqWRiY9jbgDI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}