{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damjam212/RL/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mujoco Cpider Notebook contains:\n",
        "  1. Instalation Section\n",
        "  2. Env definition with rollout\n",
        "  3. Training with rollout"
      ],
      "metadata": {
        "id": "ksFfWbeuQrGk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['MUJOCO_GL'] = 'egl' # Ensure EGL rendering is used\n",
        "\n",
        "from datetime import datetime\n",
        "from etils import epath\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "import os\n",
        "from ml_collections import config_dict\n",
        "\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from flax.training import orbax_utils\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv6WUVUm78k"
      },
      "source": [
        "# Simple ENV with spider\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XML spider defintion"
      ],
      "metadata": {
        "id": "WDfVKuffvTDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spider_xml = \"\"\"\n",
        "<mujoco model=\"ant\">\n",
        "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
        "  <option integrator=\"RK4\" timestep=\"0.01\"/>\n",
        "  <custom>\n",
        "    <numeric data=\"0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0\" name=\"init_qpos\"/>\n",
        "  </custom>\n",
        "  <default>\n",
        "    <joint armature=\"1\" damping=\"1\" limited=\"true\"/>\n",
        "    <geom conaffinity=\"0\" condim=\"3\" density=\"5.0\" friction=\"1 0.5 0.5\" margin=\"0.01\" rgba=\"0.8 0.6 0.4 1\"/>\n",
        "  </default>\n",
        "  <asset>\n",
        "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
        "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
        "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
        "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
        "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
        "  </asset>\n",
        "  <worldbody>\n",
        "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
        "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 0.75\">\n",
        "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
        "      <geom name=\"torso_geom\" pos=\"0 0 0\" size=\"0.25\" type=\"sphere\"/>\n",
        "      <joint armature=\"0\" damping=\"0\" limited=\"false\" margin=\"0.01\" name=\"root\" pos=\"0 0 0\" type=\"free\"/>\n",
        "      <body name=\"front_left_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"aux_1_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_1\" pos=\"0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_1\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"left_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_1\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 0.4 0.0\" name=\"left_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"front_right_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"aux_2_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_2\" pos=\"-0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_2\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"right_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_2\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 0.4 0.0\" name=\"right_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"aux_3_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_3\" pos=\"-0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_3\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"back_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 -0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_3\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 -0.4 0.0\" name=\"third_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"right_back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"aux_4_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_4\" pos=\"0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_4\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"rightback_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 -0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_4\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 -0.4 0.0\" name=\"fourth_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_3\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_3\" gear=\"150\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sj_Kkg0Uqx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spider Env"
      ],
      "metadata": {
        "id": "6ZbTRbgovaJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtGMYNLE3QJN"
      },
      "outputs": [],
      "source": [
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.5, # Zmniejszone (było 10.0)\n",
        "      north_reward_weight=6.0,   # Zmniejszone (było 10.0)\n",
        "      sideways_cost_weight=0.1,  # Zwiększone\n",
        "      ctrl_cost_weight=0.1,      # Zwiększone\n",
        "      healthy_reward=3.0,        # Zwiększone\n",
        "      terminate_when_unhealthy=True, # Zmienione na True (KLUCZOWE!)\n",
        "      orientation_cost_weight=10.0, # Kara za odchylenie od pionu (roll/pitch)\n",
        "      z_angular_velocity_cost_weight=0.1, # Kara za wirowanie tułowia (yaw rate)\n",
        "      healthy_z_range=(0.3, 0.8),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      episode_length: int = 1000,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "    mj_data = mujoco.MjData(mj_model)\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    self.episode_length = episode_length\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._north_reward_weight = north_reward_weight\n",
        "    self._sideways_cost_weight = sideways_cost_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._orientation_cost_weight = orientation_cost_weight\n",
        "    self._z_angular_velocity_cost_weight = z_angular_velocity_cost_weight\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "    self._torso_body_idx = mujoco.mj_name2id(\n",
        "        self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    self._mj_data = mj_data\n",
        "\n",
        "  # --- TUTAJ BYŁ PRAWDOPODOBNIE BŁĄD WCIĘCIA ---\n",
        "  # Metoda reset musi być na tym samym poziomie wcięcia co __init__\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = jp.asarray(self._mj_data.qpos) + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "        'north_reward': zero,\n",
        "        'sideways_cost': zero,\n",
        "        'orientation_cost': zero, # Dodane\n",
        "        'z_angular_cost': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "    orientation_cost = self._orientation_cost_weight * jp.sum(jp.square(data.q[4:6]))\n",
        "    # ^ Używamy q[4] i q[5], które odpowiadają za odchylenia 'x' i 'y' kwaternionu (roll i pitch)\n",
        "\n",
        "    # 2. KOSZT PRĘDKOŚCI KĄTOWEJ Z (kara za wirowanie tułowia)\n",
        "    # Prędkość kątowa tułowia (wokół osi Z) znajduje się w cvel[idx, 5].\n",
        "    z_angular_velocity = data.cvel[self._torso_body_idx, 5]\n",
        "    z_angular_cost = self._z_angular_velocity_cost_weight * jp.square(z_angular_velocity)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Calculate reward for moving in the 'north' direction (positive y-axis)\n",
        "    torso_y_velocity = data.cvel[self._torso_body_idx, 1]\n",
        "    north_reward = self._north_reward_weight * torso_y_velocity\n",
        "\n",
        "    # Calculate cost for sideways movement (x-axis)\n",
        "    torso_x_velocity = data.cvel[self._torso_body_idx, 0]\n",
        "    sideways_cost = self._sideways_cost_weight * jp.abs(torso_x_velocity)\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "\n",
        "    # Healthy reward logic\n",
        "    healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    # Done logic (CRITICAL FIX)\n",
        "    if self._terminate_when_unhealthy:\n",
        "        done = 1.0 - is_healthy\n",
        "    else:\n",
        "        done = 0.0\n",
        "\n",
        "    # Reward scaling (divided by 100.0 as discussed to stabilize PPO)\n",
        "    raw_reward = north_reward + healthy_reward - ctrl_cost - sideways_cost - orientation_cost - z_angular_cost # Nowy koszt\n",
        "    # Opcjonalnie: skalowanie tutaj, lub w configu PPO.\n",
        "    # Na razie zostawiamy surowe, bo zmieniłeś wagi na mniejsze (1.5 zamiast 10).\n",
        "    reward = raw_reward\n",
        "\n",
        "    obs = self._get_obs(data, action)\n",
        "\n",
        "    state.metrics.update(\n",
        "        forward_reward=north_reward,\n",
        "        reward_linvel=north_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=data.xpos[self._torso_body_idx, 0],\n",
        "        y_position=data.xpos[self._torso_body_idx, 1],\n",
        "        distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "        x_velocity=torso_x_velocity,\n",
        "        y_velocity=torso_y_velocity,\n",
        "        north_reward=north_reward,\n",
        "        sideways_cost=sideways_cost,\n",
        "        orientation_cost=orientation_cost, # Dodane\n",
        "        z_angular_cost=z_angular_cost,\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "    return jp.concatenate([\n",
        "        data.qpos[2:3],  # Torso z-position\n",
        "        data.cvel[self._torso_body_idx, 0:1], # Torso x-velocity\n",
        "        data.cvel[self._torso_body_idx, 1:2], # Torso y-velocity\n",
        "        data.qpos[7:], # Joint positions\n",
        "        data.qvel[6:], # Joint velocities\n",
        "    ])\n",
        "\n",
        "# Re-register environment\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "\n",
        "# Add code to get and print the observation shape\n",
        "env_test = Humanoid()\n",
        "dummy_data = mujoco.MjData(env_test.sys.mj_model) # Use mj_data\n",
        "dummy_obs = env_test._get_obs(env_test.pipeline_init(jp.asarray(dummy_data.qpos), jp.asarray(dummy_data.qvel)), jp.zeros(env_test.sys.nu))\n",
        "print(\"Observation shape:\", dummy_obs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.5,\n",
        "      north_reward_weight=6.0,\n",
        "      sideways_cost_weight=0.1,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=3.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      orientation_cost_weight=10.0,\n",
        "      z_angular_velocity_cost_weight=0.1,\n",
        "      healthy_z_range=(0.3, 0.8),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      use_contact_forces=True,  # <--- NOWY PARAMETR (Domyślnie włączone)\n",
        "      contact_force_range=(-1.0, 1.0), # <--- ZAKRES PRZYCINANIA (CLIP)\n",
        "      episode_length: int = 1000,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "    mj_data = mujoco.MjData(mj_model)\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    self.episode_length = episode_length\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._north_reward_weight = north_reward_weight\n",
        "    self._sideways_cost_weight = sideways_cost_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._orientation_cost_weight = orientation_cost_weight\n",
        "    self._z_angular_velocity_cost_weight = z_angular_velocity_cost_weight\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    # --- KONFIGURACJA CONTACT FORCES ---\n",
        "    self._use_contact_forces = use_contact_forces\n",
        "    self._contact_force_range = contact_force_range\n",
        "    # -----------------------------------\n",
        "\n",
        "    self._torso_body_idx = mujoco.mj_name2id(\n",
        "        self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    self._mj_data = mj_data\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = jp.asarray(self._mj_data.qpos) + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "        'north_reward': zero,\n",
        "        'sideways_cost': zero,\n",
        "        'orientation_cost': zero,\n",
        "        'z_angular_cost': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    # ... (Logika step pozostaje bez zmian, chyba że chcesz dodać contact_cost do nagrody) ...\n",
        "    # ... Kod step jest identyczny jak w Twojej wersji powyżej ...\n",
        "\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    # OBLICZENIA NAGRÓD (Skrótowo przepisane z Twojego kodu dla kontekstu)\n",
        "    orientation_cost = self._orientation_cost_weight * jp.sum(jp.square(data.q[4:6]))\n",
        "    z_angular_velocity = data.cvel[self._torso_body_idx, 5]\n",
        "    z_angular_cost = self._z_angular_velocity_cost_weight * jp.square(z_angular_velocity)\n",
        "    torso_y_velocity = data.cvel[self._torso_body_idx, 1]\n",
        "    north_reward = self._north_reward_weight * torso_y_velocity\n",
        "    torso_x_velocity = data.cvel[self._torso_body_idx, 0]\n",
        "    sideways_cost = self._sideways_cost_weight * jp.abs(torso_x_velocity)\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    healthy_reward = self._healthy_reward * is_healthy\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    if self._terminate_when_unhealthy:\n",
        "        done = 1.0 - is_healthy\n",
        "    else:\n",
        "        done = 0.0\n",
        "\n",
        "    raw_reward = north_reward + healthy_reward - ctrl_cost - sideways_cost - orientation_cost - z_angular_cost\n",
        "    reward = raw_reward\n",
        "\n",
        "    # ZMIANA: Obs w step teraz uwzględni contact forces\n",
        "    obs = self._get_obs(data, action)\n",
        "\n",
        "    state.metrics.update(\n",
        "        forward_reward=north_reward,\n",
        "        reward_linvel=north_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=data.xpos[self._torso_body_idx, 0],\n",
        "        y_position=data.xpos[self._torso_body_idx, 1],\n",
        "        distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "        x_velocity=torso_x_velocity,\n",
        "        y_velocity=torso_y_velocity,\n",
        "        north_reward=north_reward,\n",
        "        sideways_cost=sideways_cost,\n",
        "        orientation_cost=orientation_cost,\n",
        "        z_angular_cost=z_angular_cost,\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes body position, velocities, and contact forces.\"\"\"\n",
        "\n",
        "    # 1. Podstawowe obserwacje (to co miałeś)\n",
        "    base_obs = [\n",
        "        data.qpos[2:3],  # Torso z-position\n",
        "        data.cvel[self._torso_body_idx, 0:1], # Torso x-velocity\n",
        "        data.cvel[self._torso_body_idx, 1:2], # Torso y-velocity\n",
        "        data.qpos[7:],   # Joint positions\n",
        "        data.qvel[6:],   # Joint velocities\n",
        "    ]\n",
        "\n",
        "    # 2. Dodanie Contact Forces (jeśli włączone)\n",
        "    if self._use_contact_forces:\n",
        "        # data.cfrc_ext to macierz (n_bodies, 6) -> [Fx, Fy, Fz, Tx, Ty, Tz]\n",
        "        # Bierzemy siły dla wszystkich ciał.\n",
        "\n",
        "        # Przycinanie (Clipping) - ważne dla stabilności sieci neuronowej!\n",
        "        # Siły uderzenia mogą być ogromne i zdestabilizować trening.\n",
        "        min_val, max_val = self._contact_force_range\n",
        "        contact_forces = jp.clip(data.cfrc_ext, min_val, max_val)\n",
        "\n",
        "        # Spłaszczamy do wektora 1D\n",
        "        contact_forces_flat = contact_forces.flatten()\n",
        "\n",
        "        base_obs.append(contact_forces_flat)\n",
        "\n",
        "    return jp.concatenate(base_obs)"
      ],
      "metadata": {
        "id": "y6Ai_KfgXHkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplfied Spider Env"
      ],
      "metadata": {
        "id": "b6JnVi0imGZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.0, # Zmieniono wagę na standardową\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=1.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.35, 1.0), # ZMIANA: Szerszy zakres, żeby pająk nie \"umierał\" od razu\n",
        "      reset_noise_scale=0.1,       # ZMIANA: Większy szum na starcie pomaga w eksploracji\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      use_contact_forces=True,\n",
        "      contact_force_range=(-1.0, 1.0),\n",
        "      episode_length: int = 1000,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "    mj_data = mujoco.MjData(mj_model)\n",
        "\n",
        "    # --- ZMIANA 1: Ulepszona fizyka dla wielonoga ---\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 20      # Zwiększone z 6 (stabilność)\n",
        "    mj_model.opt.ls_iterations = 10   # Zwiększone z 6\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    self.episode_length = episode_length\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "    self._use_contact_forces = use_contact_forces\n",
        "    self._contact_force_range = contact_force_range\n",
        "\n",
        "    self._torso_body_idx = mujoco.mj_name2id(\n",
        "        self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    self._mj_data = mj_data\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = jp.asarray(self._mj_data.qpos) + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    # --- ZMIANA 2: Uproszczona logika nagrody ---\n",
        "    # Usunęliśmy orientation_cost, sideways_cost, north_reward.\n",
        "    # Teraz nagradzamy ruch w osi X (domyślny przód Anta).\n",
        "\n",
        "    x_velocity = data.cvel[self._torso_body_idx, 0] # Prędkość w osi X\n",
        "    forward_reward = self._forward_reward_weight * x_velocity\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "\n",
        "    healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    if self._terminate_when_unhealthy:\n",
        "        done = 1.0 - is_healthy\n",
        "    else:\n",
        "        done = 0.0\n",
        "\n",
        "    # Prosta suma: idź do przodu + żyj - energia\n",
        "    raw_reward = forward_reward + healthy_reward - ctrl_cost\n",
        "    reward = raw_reward\n",
        "\n",
        "    obs = self._get_obs(data, action)\n",
        "\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=data.xpos[self._torso_body_idx, 0],\n",
        "        y_position=data.xpos[self._torso_body_idx, 1],\n",
        "        distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "        x_velocity=x_velocity,\n",
        "        # y_velocity=... (usunięte, zbędne)\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes body position, velocities, and contact forces.\"\"\"\n",
        "\n",
        "    base_obs = [\n",
        "        data.qpos[2:3],  # Torso z-position\n",
        "\n",
        "        # --- ZMIANA 3: Dodanie orientacji ---\n",
        "        # Zamiast prędkości globalnych (cvel), dajemy orientację (Quaternion).\n",
        "        # To pozwala pająkowi wiedzieć, gdzie jest \"góra\", a gdzie \"dół\".\n",
        "        data.qpos[3:7],  # Orientacja (Quaternion: w, x, y, z)\n",
        "        # ------------------------------------\n",
        "\n",
        "        data.qpos[7:],   # Joint positions\n",
        "        data.qvel[6:],   # Joint velocities\n",
        "    ]\n",
        "\n",
        "    if self._use_contact_forces:\n",
        "        min_val, max_val = self._contact_force_range\n",
        "        contact_forces = jp.clip(data.cfrc_ext, min_val, max_val)\n",
        "        contact_forces_flat = contact_forces.flatten()\n",
        "        base_obs.append(contact_forces_flat)\n",
        "\n",
        "    return jp.concatenate(base_obs)\n",
        "\n",
        "# Re-register environment\n",
        "envs.register_environment('humanoid', Humanoid)"
      ],
      "metadata": {
        "id": "ctcQFfgKZOjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Humanoid(PipelineEnv):\n",
        "      def __init__(\n",
        "            self,\n",
        "            forward_reward_weight=1.0,\n",
        "            ctrl_cost_weight=0.1,\n",
        "            healthy_reward=1.0,\n",
        "            terminate_when_unhealthy=True,\n",
        "            healthy_z_range=(0.2, 1.0), # Nieco luźniej dla Anta\n",
        "            reset_noise_scale=0.1,\n",
        "            exclude_current_positions_from_observation=True,\n",
        "            use_contact_forces=False, # Wyłączone dla prostszego startu\n",
        "            contact_force_range=(-1.0, 1.0),\n",
        "            episode_length: int = 1000,\n",
        "            **kwargs,\n",
        "        ):\n",
        "            mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "\n",
        "            # Ulepszona fizyka dla stabilności\n",
        "            mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "            mj_model.opt.iterations = 20\n",
        "            mj_model.opt.ls_iterations = 10\n",
        "\n",
        "            sys = mjcf.load_model(mj_model)\n",
        "\n",
        "            physics_steps_per_control_step = 5\n",
        "            kwargs['n_frames'] = kwargs.get('n_frames', physics_steps_per_control_step)\n",
        "            kwargs['backend'] = 'mjx'\n",
        "\n",
        "            super().__init__(sys, **kwargs)\n",
        "\n",
        "            self._forward_reward_weight = forward_reward_weight\n",
        "            self._ctrl_cost_weight = ctrl_cost_weight\n",
        "            self._healthy_reward = healthy_reward\n",
        "            self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "            self._healthy_z_range = healthy_z_range\n",
        "            self._reset_noise_scale = reset_noise_scale\n",
        "            self._use_contact_forces = use_contact_forces\n",
        "            self._contact_force_range = contact_force_range\n",
        "\n",
        "            self._torso_body_idx = mujoco.mj_name2id(\n",
        "                self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "            )\n",
        "            self._mj_data = mujoco.MjData(mj_model)\n",
        "\n",
        "      def reset(self, rng: jp.ndarray) -> State:\n",
        "          rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "          low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "\n",
        "          qpos = jp.asarray(self._mj_data.qpos)\n",
        "          qpos_noise = jax.random.uniform(rng1, (self.sys.nq,), minval=low, maxval=hi)\n",
        "\n",
        "          # Ochrona kwaternionów (indeksy 3:7) przed prostym dodawaniem szumu\n",
        "          mask = jp.ones(self.sys.nq).at[3:7].set(0.0)\n",
        "          qpos = qpos + qpos_noise * mask\n",
        "\n",
        "          qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "              rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "          )\n",
        "\n",
        "          data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "          obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "          reward, done, zero = jp.zeros(3)\n",
        "          metrics = {\n",
        "              'forward_reward': zero,\n",
        "              'reward_linvel': zero,\n",
        "              'reward_quadctrl': zero,\n",
        "              'reward_alive': zero,\n",
        "              'reward_upright': zero,\n",
        "              'x_position': zero,\n",
        "              'y_position': zero,\n",
        "              'distance_from_origin': zero,\n",
        "              'x_velocity': zero,\n",
        "          }\n",
        "          return State(data, obs, reward, done, metrics)\n",
        "\n",
        "      def step(self, state: State, action: jp.ndarray) -> State:\n",
        "          data0 = state.pipeline_state\n",
        "          data = self.pipeline_step(data0, action)\n",
        "\n",
        "          x_velocity = data.cvel[self._torso_body_idx, 0]\n",
        "          forward_reward = self._forward_reward_weight * x_velocity\n",
        "\n",
        "          min_z, max_z = self._healthy_z_range\n",
        "          z_pos = data.qpos[2]\n",
        "          is_healthy = jp.where(z_pos < min_z, 0.0, 1.0)\n",
        "          is_healthy = jp.where(z_pos > max_z, 0.0, is_healthy)\n",
        "\n",
        "          healthy_reward = self._healthy_reward * is_healthy\n",
        "          ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "          # Upright Reward (zabezpieczenie przed turlaniem)\n",
        "          # data.xmat dla torso, element [2,2] (oś Z vs Z)\n",
        "          upright_factor = data.xmat[self._torso_body_idx, 2, 2]\n",
        "          upright_reward = 1.0 * upright_factor\n",
        "\n",
        "          if self._terminate_when_unhealthy:\n",
        "              done = 1.0 - is_healthy\n",
        "          else:\n",
        "              done = 0.0\n",
        "\n",
        "          raw_reward = forward_reward + healthy_reward + upright_reward - ctrl_cost\n",
        "          reward = raw_reward\n",
        "\n",
        "          obs = self._get_obs(data, action)\n",
        "\n",
        "          state.metrics.update(\n",
        "              forward_reward=forward_reward,\n",
        "              reward_linvel=forward_reward,\n",
        "              reward_quadctrl=-ctrl_cost,\n",
        "              reward_alive=healthy_reward,\n",
        "              reward_upright=upright_reward,\n",
        "              x_position=data.xpos[self._torso_body_idx, 0],\n",
        "              y_position=data.xpos[self._torso_body_idx, 1],\n",
        "              distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "              x_velocity=x_velocity,\n",
        "          )\n",
        "\n",
        "          return state.replace(\n",
        "              pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "          )\n",
        "\n",
        "      def _get_obs(self, data: mjx.Data, action: jp.ndarray) -> jp.ndarray:\n",
        "          base_obs = [\n",
        "              data.qpos[2:3],   # Z-position\n",
        "              data.qpos[3:7],   # Orientation (Quat)\n",
        "              data.qpos[7:],    # Joint positions\n",
        "              data.qvel[:6],    # Root velocities (Lin + Ang) - KLUCZOWE\n",
        "              data.qvel[6:],    # Joint velocities\n",
        "          ]\n",
        "          if self._use_contact_forces:\n",
        "              min_val, max_val = self._contact_force_range\n",
        "              contact_forces = jp.clip(data.cfrc_ext, min_val, max_val)\n",
        "              contact_forces_flat = contact_forces.flatten()\n",
        "              base_obs.append(contact_forces_flat)\n",
        "\n",
        "          return jp.concatenate(base_obs)\n",
        "\n",
        "# Rejestracja środowiska\n",
        "envs.register_environment('humanoid', Humanoid)"
      ],
      "metadata": {
        "id": "l3B0C7RUqw5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from jax import numpy as jp\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "from brax.envs.base import PipelineEnv, State\n",
        "from brax import envs\n",
        "from brax.io import mjcf\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "    def __init__(\n",
        "        self,\n",
        "        ctrl_cost_weight=0.5,\n",
        "        contact_cost_weight=5e-4,\n",
        "        healthy_reward=1.0,\n",
        "        terminate_when_unhealthy=True,\n",
        "        healthy_z_range=(0.2, 1.0),\n",
        "        contact_force_range=(-1.0, 1.0),\n",
        "        reset_noise_scale=0.1,\n",
        "        exclude_current_positions_from_observation=True,\n",
        "        # Brax/MJX specific:\n",
        "        physics_steps_per_control_step=5,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "\n",
        "        # Ustawienia solvera dla stabilności w MJX\n",
        "        mj_model.opt.solver = mujoco.mjtSolver.mjSOL_NEWTON\n",
        "        mj_model.opt.iterations = 1\n",
        "        mj_model.opt.ls_iterations = 4\n",
        "\n",
        "        sys = mjcf.load_model(mj_model)\n",
        "\n",
        "        kwargs['n_frames'] = kwargs.get('n_frames', physics_steps_per_control_step)\n",
        "        kwargs['backend'] = 'mjx'\n",
        "\n",
        "        super().__init__(sys, **kwargs)\n",
        "\n",
        "        self._ctrl_cost_weight = ctrl_cost_weight\n",
        "        self._contact_cost_weight = contact_cost_weight\n",
        "        self._healthy_reward = healthy_reward\n",
        "        self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "        self._healthy_z_range = healthy_z_range\n",
        "        self._contact_force_range = contact_force_range\n",
        "        self._reset_noise_scale = reset_noise_scale\n",
        "        self._exclude_current_positions_from_observation = exclude_current_positions_from_observation\n",
        "\n",
        "        self._mj_data = mujoco.MjData(mj_model)\n",
        "\n",
        "    def reset(self, rng: jp.ndarray) -> State:\n",
        "        rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "        low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "\n",
        "        # Dostęp do qpos i qvel z modelu referencyjnego\n",
        "        qpos = jp.asarray(self._mj_data.qpos)\n",
        "        qvel = jp.asarray(self._mj_data.qvel)\n",
        "\n",
        "        # Dodanie szumu do qpos (pozycja)\n",
        "        # Ant ma joint 'free' (root), więc pierwsze 7 elementów to pozycja (3) + rotacja (4)\n",
        "        qpos_noise = jax.random.uniform(rng1, (self.sys.nq,), minval=low, maxval=hi)\n",
        "        qpos = qpos + qpos_noise\n",
        "\n",
        "        # Normalizacja kwaternionów (indeksy 3,4,5,6) po dodaniu szumu\n",
        "        quat = qpos[3:7]\n",
        "        quat = quat / jp.linalg.norm(quat)\n",
        "        qpos = qpos.at[3:7].set(quat)\n",
        "\n",
        "        # Dodanie szumu do qvel (prędkość)\n",
        "        qvel_noise = jax.random.normal(rng2, (self.sys.nv,)) * self._reset_noise_scale\n",
        "        qvel = qvel + qvel_noise\n",
        "\n",
        "        data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "        obs = self._get_obs(data)\n",
        "        reward, done, zero = jp.zeros(3)\n",
        "\n",
        "        # Inicjalizacja metryk (zgodnie z konwencją Humanoida/Gymnasium)\n",
        "        metrics = {\n",
        "            'forward_reward': zero,\n",
        "            'reward_linvel': zero,\n",
        "            'reward_quadctrl': zero,\n",
        "            'reward_alive': zero,\n",
        "            'reward_contact': zero,\n",
        "            'x_position': zero,\n",
        "            'y_position': zero,\n",
        "            'distance_from_origin': zero,\n",
        "            'x_velocity': zero,\n",
        "            'y_velocity': zero,\n",
        "        }\n",
        "        return State(data, obs, reward, done, metrics)\n",
        "\n",
        "    def step(self, state: State, action: jp.ndarray) -> State:\n",
        "        data0 = state.pipeline_state\n",
        "        data = self.pipeline_step(data0, action)\n",
        "\n",
        "        # 1. Prędkość (Forward Reward)\n",
        "        # W MuJoCo qvel[0] to prędkość liniowa w osi X\n",
        "        x_velocity = data.qvel[0]\n",
        "        y_velocity = data.qvel[1]\n",
        "        forward_reward = x_velocity\n",
        "\n",
        "        # 2. Healthy Reward (Nagroda za przetrwanie)\n",
        "        min_z, max_z = self._healthy_z_range\n",
        "        z_pos = data.qpos[2]\n",
        "\n",
        "        # Sprawdzenie czy robot jest \"zdrowy\" (skończone wartości i odpowiednie Z)\n",
        "        is_finite = jp.all(jp.isfinite(data.qpos)) & jp.all(jp.isfinite(data.qvel))\n",
        "        is_healthy = is_finite & (z_pos >= min_z) & (z_pos <= max_z)\n",
        "\n",
        "        healthy_reward = self._healthy_reward * jp.where(is_healthy, 1.0, 0.0)\n",
        "\n",
        "        # Warunek zakończenia\n",
        "        if self._terminate_when_unhealthy:\n",
        "            done = 1.0 - jp.where(is_healthy, 1.0, 0.0)\n",
        "        else:\n",
        "            done = 0.0\n",
        "\n",
        "        # 3. Koszty (Costs)\n",
        "        # Control cost: waga * suma kwadratów akcji\n",
        "        ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "        # Contact cost: waga * suma kwadratów sił kontaktu (ograniczonych do -1, 1)\n",
        "        cfrc_ext_clipped = jp.clip(data.cfrc_ext, -1.0, 1.0)\n",
        "        contact_cost = self._contact_cost_weight * jp.sum(jp.square(cfrc_ext_clipped))\n",
        "\n",
        "        # Sumaryczna nagroda\n",
        "        reward = forward_reward + healthy_reward - ctrl_cost - contact_cost\n",
        "\n",
        "        obs = self._get_obs(data)\n",
        "\n",
        "        # Aktualizacja metryk\n",
        "        state.metrics.update(\n",
        "            forward_reward=forward_reward,\n",
        "            reward_linvel=forward_reward,\n",
        "            reward_quadctrl=-ctrl_cost,\n",
        "            reward_alive=healthy_reward,\n",
        "            reward_contact=-contact_cost,\n",
        "            x_position=data.qpos[0],\n",
        "            y_position=data.qpos[1],\n",
        "            distance_from_origin=jp.linalg.norm(data.qpos[0:2]),\n",
        "            x_velocity=x_velocity,\n",
        "            y_velocity=y_velocity,\n",
        "        )\n",
        "\n",
        "        return state.replace(\n",
        "            pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "        )\n",
        "\n",
        "    def _get_obs(self, data: mjx.Data) -> jp.ndarray:\n",
        "        # qpos:\n",
        "        # [0]: x (ignorowane domyślnie)\n",
        "        # [1]: y (ignorowane domyślnie)\n",
        "        # [2]: z\n",
        "        # [3-6]: quat (orientacja)\n",
        "        # [7-14]: joint angles (8 stawów)\n",
        "\n",
        "        # Standardowa logika Gymnasium z exclude_current_positions_from_observation=True\n",
        "        if self._exclude_current_positions_from_observation:\n",
        "            qpos_obs = data.qpos[2:]\n",
        "        else:\n",
        "            qpos_obs = data.qpos\n",
        "\n",
        "        qvel_obs = data.qvel\n",
        "\n",
        "        # Siły kontaktu (wymagane przez logikę AntEnv)\n",
        "        # cfrc_ext ma wymiar (nbody, 6) -> (torque, force)\n",
        "        # Clipujemy siły zgodnie z definicją środowiska\n",
        "        contact_force_obs = jp.clip(data.cfrc_ext, -1.0, 1.0).flatten()\n",
        "\n",
        "        return jp.concatenate([\n",
        "            qpos_obs,\n",
        "            qvel_obs,\n",
        "            contact_force_obs,\n",
        "        ])\n",
        "\n",
        "envs.register_environment('humanoid', Humanoid)"
      ],
      "metadata": {
        "id": "inq3EXiryrwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K6IznI2y83"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout.\n",
        "\n",
        "NOTE: Since episodes terminate early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKLFK54C1CH"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph8u-v2Q2xLS"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(100):\n",
        "  ctrl = jp.zeros(env.sys.nu) # Set control input to zero for standing still\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "media.show_video(env.render(rollout), fps=1.0 / env.dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O0zQEttGONfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation space:\", env.observation_size)"
      ],
      "metadata": {
        "id": "42pERvKYOOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO - implementation"
      ],
      "metadata": {
        "id": "2xIGm9hndmKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class NetworkBase(nn.Module, metaclass=ABCMeta):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        super(NetworkBase, self).__init__()\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Network(NetworkBase):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.relu,last_activation = None):\n",
        "        super(Network, self).__init__()\n",
        "        self.activation = activation_function\n",
        "        self.last_activation = last_activation\n",
        "        layers_unit = [input_dim]+ [hidden_dim]*(layer_num-1)\n",
        "        layers = ([nn.Linear(layers_unit[idx],layers_unit[idx+1]) for idx in range(len(layers_unit)-1)])\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.last_layer = nn.Linear(layers_unit[-1],output_dim)\n",
        "        self.network_init()\n",
        "    def forward(self, x):\n",
        "        return self._forward(x)\n",
        "    def _forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.last_layer(x)\n",
        "        if self.last_activation != None:\n",
        "            x = self.last_activation(x)\n",
        "        return x\n",
        "    def network_init(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.orthogonal_(layer.weight)\n",
        "                layer.bias.data.zero_()"
      ],
      "metadata": {
        "id": "SSTtLSBWdyj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Actor(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.tanh,last_activation = None, trainable_std = False):\n",
        "        super(Actor, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "        self.trainable_std = trainable_std\n",
        "        if self.trainable_std == True:\n",
        "            self.logstd = nn.Parameter(torch.zeros(1, output_dim))\n",
        "    def forward(self, x):\n",
        "        mu = self._forward(x)\n",
        "        if self.trainable_std == True:\n",
        "            std = torch.exp(self.logstd)\n",
        "        else:\n",
        "            logstd = torch.zeros_like(mu)\n",
        "            std = torch.exp(logstd)\n",
        "        return mu,std\n",
        "\n",
        "class Critic(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function, last_activation = None):\n",
        "        super(Critic, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "\n",
        "    def forward(self, *x):\n",
        "        x = torch.cat(x,-1)\n",
        "        return self._forward(x)\n"
      ],
      "metadata": {
        "id": "-IOjyENUdzbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Dict(dict):\n",
        "    def __init__(self,config,section_name,location = False):\n",
        "        super(Dict,self).__init__()\n",
        "        self.initialize(config, section_name,location)\n",
        "    def initialize(self, config, section_name,location):\n",
        "        for key,value in config.items(section_name):\n",
        "            if location :\n",
        "                self[key] = value\n",
        "            else:\n",
        "                self[key] = eval(value)\n",
        "    def __getattr__(self,val):\n",
        "        return self[val]\n",
        "\n",
        "def make_transition(state,action,reward,next_state,done,log_prob=None):\n",
        "    transition = {}\n",
        "    transition['state'] = state\n",
        "    transition['action'] = action\n",
        "    transition['reward'] = reward\n",
        "    transition['next_state'] = next_state\n",
        "    transition['log_prob'] = log_prob\n",
        "    transition['done'] = done\n",
        "    return transition\n",
        "\n",
        "def make_mini_batch(*value):\n",
        "    mini_batch_size = value[0]\n",
        "    full_batch_size = len(value[1])\n",
        "    full_indices = np.arange(full_batch_size)\n",
        "    np.random.shuffle(full_indices)\n",
        "    for i in range(full_batch_size // mini_batch_size):\n",
        "        indices = full_indices[mini_batch_size*i : mini_batch_size*(i+1)]\n",
        "        yield [x[indices] for x in value[1:]]\n",
        "\n",
        "def convert_to_tensor(*value):\n",
        "    device = value[0]\n",
        "    return [torch.tensor(x).float().to(device) for x in value[1:]]\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, action_prob_exist, max_size, state_dim, num_action):\n",
        "        self.max_size = max_size\n",
        "        self.data_idx = 0\n",
        "        self.action_prob_exist = action_prob_exist\n",
        "        self.data = {}\n",
        "\n",
        "        self.data['state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['action'] = np.zeros((self.max_size, num_action))\n",
        "        self.data['reward'] = np.zeros((self.max_size, 1))\n",
        "        self.data['next_state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['done'] = np.zeros((self.max_size, 1))\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'] = np.zeros((self.max_size, 1))\n",
        "    def put_data(self, transition):\n",
        "        idx = self.data_idx % self.max_size\n",
        "        self.data['state'][idx] = transition['state']\n",
        "        self.data['action'][idx] = transition['action']\n",
        "        self.data['reward'][idx] = transition['reward']\n",
        "        self.data['next_state'][idx] = transition['next_state']\n",
        "        self.data['done'][idx] = float(transition['done'])\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'][idx] = transition['log_prob']\n",
        "\n",
        "        self.data_idx += 1\n",
        "    def sample(self, shuffle, batch_size = None):\n",
        "        if shuffle :\n",
        "            sample_num = min(self.max_size, self.data_idx)\n",
        "            rand_idx = np.random.choice(sample_num, batch_size,replace=False)\n",
        "            sampled_data = {}\n",
        "            sampled_data['state'] = self.data['state'][rand_idx]\n",
        "            sampled_data['action'] = self.data['action'][rand_idx]\n",
        "            sampled_data['reward'] = self.data['reward'][rand_idx]\n",
        "            sampled_data['next_state'] = self.data['next_state'][rand_idx]\n",
        "            sampled_data['done'] = self.data['done'][rand_idx]\n",
        "            if self.action_prob_exist :\n",
        "                sampled_data['log_prob'] = self.data['log_prob'][rand_idx]\n",
        "            return sampled_data\n",
        "        else:\n",
        "            return self.data\n",
        "    def size(self):\n",
        "        return min(self.max_size, self.data_idx)\n",
        "class RunningMeanStd(object):\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count"
      ],
      "metadata": {
        "id": "nUhmYZbwd4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                actor_loss = (-torch.min(surr1, surr2) - entropy).mean()\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n"
      ],
      "metadata": {
        "id": "SYyu-Ijsdp_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO - training loop"
      ],
      "metadata": {
        "id": "xF8gG1nKe0nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 2048 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 3e-4\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 64 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name) # Removed episode_length=1000\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "# num_total_steps_pytorch = 1_000_000 # Total environment steps for PyTorch PPO\n",
        "current_total_steps = 0\n",
        "episode_count = 0\n",
        "rng = jax.random.PRNGKey(0) # JAX RNG for environment\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Change the while loop condition to run for 10 episodes\n",
        "while episode_count < 300:\n",
        "    episode_count += 1\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    episode_reward = 0\n",
        "    episode_north_reward = 0\n",
        "    episode_healthy_reward = 0\n",
        "    episode_ctrl_cost = 0\n",
        "    episode_sideways_cost = 0\n",
        "\n",
        "    # Clear replay buffer for new trajectory collection (on-policy PPO)\n",
        "    ppo_torch_agent.data = ReplayBuffer(\n",
        "        action_prob_exist=True,\n",
        "        max_size=ppo_torch_args.traj_length,\n",
        "        state_dim=env.observation_size,\n",
        "        num_action=env.action_size\n",
        "    )\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length): # Collect for traj_length steps\n",
        "        # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "        obs_torch = torch.from_numpy(np.array(env_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "        # Get action from PyTorch actor\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "        action_jax = jnp.asarray(action_torch.squeeze(0).cpu().numpy())\n",
        "\n",
        "        # Step JAX environment\n",
        "        rng, step_rng = jax.random.split(rng) # Need a new rng for each step if needed by brax (jit_step doesn't take it)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (remove batch dimension where applicable)\n",
        "        obs_np = obs_torch.squeeze(0).cpu().numpy()\n",
        "        action_np = action_torch.squeeze(0).cpu().numpy()\n",
        "        reward_np = np.array(next_env_state.reward).reshape(1) # Ensure (1,) shape\n",
        "        next_obs_np = np.array(next_env_state.obs)\n",
        "        done_np = np.array(next_env_state.done).reshape(1)     # Ensure (1,) shape\n",
        "        log_prob_np = log_prob_torch.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Store transition in PyTorch ReplayBuffer\n",
        "        transition = make_transition(\n",
        "            obs_np,\n",
        "            action_np,\n",
        "            reward_np,\n",
        "            next_obs_np,\n",
        "            done_np,\n",
        "            log_prob_np\n",
        "        )\n",
        "        ppo_torch_agent.put_data(transition)\n",
        "\n",
        "        episode_reward += next_env_state.reward\n",
        "        episode_north_reward += next_env_state.metrics['north_reward']\n",
        "        episode_healthy_reward += next_env_state.metrics['reward_alive']\n",
        "        episode_ctrl_cost += -next_env_state.metrics['reward_quadctrl'] # ctrl_cost is stored as negative in metrics\n",
        "        episode_sideways_cost += next_env_state.metrics['sideways_cost']\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += 1\n",
        "\n",
        "        if env_state.done:\n",
        "            print(f\"Episode {episode_count} finished early at step {t+1}. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "            print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "            break\n",
        "    else: # If loop completes without break\n",
        "      print(f\"Episode {episode_count} completed {ppo_torch_args.traj_length} steps. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "      print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length steps\n",
        "    if ppo_torch_agent.data.size() >= ppo_torch_args.traj_length:\n",
        "        ppo_torch_agent.train_net(episode_count)\n",
        "        print(f\"PPO agent trained for episode {episode_count}.\")\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "metadata": {
        "id": "uvqNqIbid8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "        # --- DEBUG: Print action_torch_eval shape ---\n",
        "        print(f\"DEBUG: action_torch_eval shape: {action_torch_eval.shape}\")\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "    # --- DEBUG: Print action_jax_eval shape ---\n",
        "    print(f\"DEBUG: action_jax_eval shape (after squeeze): {action_jax_eval.shape}\")\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)"
      ],
      "metadata": {
        "id": "SYnfhpblsmVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MqWRiY9jbgDI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ba071c1"
      },
      "source": [
        "# Task\n",
        "I will proceed with the following steps:\n",
        "\n",
        "1.  **Modify `PPO.train_net` (cell `SYyu-Ijsdp_q`)**:\n",
        "    *   Separate the `actor_loss` into `policy_loss_raw` (policy gradient part) and `entropy_loss_term` (entropy regularization part).\n",
        "    *   Ensure `critic_loss` (value loss part) is correctly captured.\n",
        "    *   Modify the `train_net` method to return these three average loss values: `policy_loss_raw.item()`, `entropy_loss_term.item()`, and `critic_loss.item()`.\n",
        "\n",
        "2.  **Modify the training loop (cell `uvqNqIbid8KS`)**:\n",
        "    *   Add `import csv` and `from datetime import datetime`.\n",
        "    *   Generate a unique CSV filename using the current timestamp.\n",
        "    *   Open the CSV file and write the header row, including: `episode`, `total_reward`, `north_reward`, `healthy_reward`, `ctrl_cost`, `sideways_cost`, `policy_gradient_loss`, `entropy_loss`, `value_loss`, and `total_steps`.\n",
        "    *   In the training loop, after `ppo_torch_agent.train_net` is called, capture the returned `policy_loss`, `entropy_loss`, and `value_loss`.\n",
        "    *   Append a new row to the CSV file with the `episode_count`, `episode_reward`, component rewards, the captured detailed loss values, and `current_total_steps`.\n",
        "    *   Make sure to handle the `ctrl_cost` correctly, as it's stored as negative in metrics.\n",
        "\n",
        "This will ensure the detailed loss components are captured and logged for each training episode.\n",
        "\n",
        "```python\n",
        "# cell_id: SYyu-Ijsdp_q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                \n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean()\n",
        "                entropy_loss_term = (-entropy).mean()\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Collect losses for averaging\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "        \n",
        "        # Calculate average losses over all mini-batches and epochs\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch) if policy_losses_raw_epoch else 0\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch) if entropy_losses_term_epoch else 0\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch) if critic_losses_epoch else 0\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n",
        "\n",
        "```\n",
        "\n",
        "```python\n",
        "# cell_id: uvqNqIbid8KS\n",
        "\n",
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 2048 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 3e-4\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 64 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name) # Removed episode_length=1000\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- CSV Logging Setup ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Define header for CSV\n",
        "header = [\n",
        "    'episode', 'total_reward', 'north_reward', 'healthy_reward',\n",
        "    'ctrl_cost', 'sideways_cost', 'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging training data to {csv_filename}\")\n",
        "# --- End CSV Logging Setup ---\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "# num_total_steps_pytorch = 1_000_000 # Total environment steps for PyTorch PPO\n",
        "current_total_steps = 0\n",
        "episode_count = 0\n",
        "rng = jax.random.PRNGKey(0) # JAX RNG for environment\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Change the while loop condition to run for 10 episodes\n",
        "while episode_count < 300:\n",
        "    episode_count += 1\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    episode_reward = 0\n",
        "    episode_north_reward = 0\n",
        "    episode_healthy_reward = 0\n",
        "    episode_ctrl_cost = 0\n",
        "    episode_sideways_cost = 0\n",
        "\n",
        "    # Clear replay buffer for new trajectory collection (on-policy PPO)\n",
        "    ppo_torch_agent.data = ReplayBuffer(\n",
        "        action_prob_exist=True,\n",
        "        max_size=ppo_torch_args.traj_length,\n",
        "        state_dim=env.observation_size,\n",
        "        num_action=env.action_size\n",
        "    )\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length): # Collect for traj_length steps\n",
        "        # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "        obs_torch = torch.from_numpy(np.array(env_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "        # Get action from PyTorch actor\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "        action_jax = jnp.asarray(action_torch.squeeze(0).cpu().numpy())\n",
        "\n",
        "        # Step JAX environment\n",
        "        rng, step_rng = jax.random.split(rng) # Need a new rng for each step if needed by brax (jit_step doesn't take it)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (remove batch dimension where applicable)\n",
        "        obs_np = obs_torch.squeeze(0).cpu().numpy()\n",
        "        action_np = action_torch.squeeze(0).cpu().numpy()\n",
        "        reward_np = np.array(next_env_state.reward).reshape(1) # Ensure (1,) shape\n",
        "        next_obs_np = np.array(next_env_state.obs)\n",
        "        done_np = np.array(next_env_state.done).reshape(1)     # Ensure (1,) shape\n",
        "        log_prob_np = log_prob_torch.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Store transition in PyTorch ReplayBuffer\n",
        "        transition = make_transition(\n",
        "            obs_np,\n",
        "            action_np,\n",
        "            reward_np,\n",
        "            next_obs_np,\n",
        "            done_np,\n",
        "            log_prob_np\n",
        "        )\n",
        "        ppo_torch_agent.put_data(transition)\n",
        "\n",
        "        episode_reward += next_env_state.reward\n",
        "        episode_north_reward += next_env_state.metrics['north_reward']\n",
        "        episode_healthy_reward += next_env_state.metrics['reward_alive']\n",
        "        episode_ctrl_cost += -next_env_state.metrics['reward_quadctrl'] # ctrl_cost is stored as negative in metrics\n",
        "        episode_sideways_cost += next_env_state.metrics['sideways_cost']\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += 1\n",
        "\n",
        "        if env_state.done:\n",
        "            print(f\"Episode {episode_count} finished early at step {t+1}. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "            print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "            break\n",
        "    else: # If loop completes without break\n",
        "      print(f\"Episode {episode_count} completed {ppo_torch_args.traj_length} steps. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "      print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length steps\n",
        "    if ppo_torch_agent.data.size() >= ppo_torch_args.traj_length:\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(episode_count)\n",
        "        print(f\"PPO agent trained for episode {episode_count}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        # --- Log to CSV ---\n",
        "        csv_writer.writerow([\n",
        "            episode_count,\n",
        "            float(episode_reward),\n",
        "            float(episode_north_reward),\n",
        "            float(episode_healthy_reward),\n",
        "            float(episode_ctrl_cost),\n",
        "            float(episode_sideways_cost),\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps\n",
        "        ])\n",
        "        csv_file.flush() # Ensure data is written to disk immediately\n",
        "        # --- End Log to CSV ---\n",
        "\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "# --- Close CSV file after training ---\n",
        "csv_file.close()\n",
        "print(f\"Training data saved to {csv_filename}\")\n",
        "# --- End Close CSV file ---\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68d6af6c"
      },
      "source": [
        "## Modify PPO.train_net to return detailed losses\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `train_net` method in the `PPO` class (cell `SYyu-Ijsdp_q`) to calculate and return average `policy_loss_raw` (policy gradient part), `entropy_loss_term` (entropy regularization part), and the final `critic_loss` (value loss part, including clipping and scaling) over the training epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19fa5739"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `train_net` method in the `PPO` class to calculate and return average detailed loss components. I will update the code in cell `SYyu-Ijsdp_q` to implement this functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ac02d4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean() # Policy gradient part\n",
        "                entropy_loss_term = (-entropy).mean() # Entropy regularization part\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term # Total actor loss\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Append losses to lists\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "\n",
        "        # Calculate average losses over the epoch\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch)\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch)\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch)\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean() # Policy gradient part\n",
        "                entropy_loss_term = (-entropy).mean() # Entropy regularization part\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term # Total actor loss\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Append losses to lists\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "\n",
        "        # Calculate average losses over the epoch\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch)\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch)\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch)\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n"
      ],
      "metadata": {
        "id": "LFgz5qIsYe4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train parallel envs"
      ],
      "metadata": {
        "id": "2g_s_1C3IpAl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "766ab396"
      },
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "from brax import envs\n",
        "\n",
        "class ScaleRewardWrapper(envs.Wrapper):\n",
        "    def __init__(self, env, scale=0.1):\n",
        "        super().__init__(env)\n",
        "        self.scale = scale\n",
        "\n",
        "    def reset(self, rng):\n",
        "        state = super().reset(rng)\n",
        "        return state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        state = super().step(state, action)\n",
        "        # Skalujemy nagrodę (zmniejszamy ją)\n",
        "        return state.replace(reward=state.reward * self.scale)# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 64 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 1e-3\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 1024 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.2\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 6144 # Added: Number of parallel environments\n",
        "        self.num_updates = 100 # Added: Number of PPO updates (replaces episode_count limit)\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "# Modified: Use envs.create for batched environments\n",
        "env = envs.create(env_name=env_name, episode_length=2000, batch_size=ppo_torch_args.num_envs)\n",
        "env = ScaleRewardWrapper(env, scale=0.1)\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "# Added: Debug print for number of environments\n",
        "print(f\"DEBUG: Number of environments (batch_size): {env.batch_size}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- CSV Logging Setup ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Define header for CSV\n",
        "# Modified: Use 'update_idx' and 'avg_' prefix for averaged metrics\n",
        "header = [\n",
        "    'update_idx', 'avg_total_reward', 'avg_north_reward', 'avg_healthy_reward',\n",
        "    'avg_ctrl_cost', 'avg_sideways_cost', 'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging training data to {csv_filename}\")\n",
        "# --- End CSV Logging Setup ---\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "current_total_steps = 0\n",
        "# Initialize rng for the loop outside of it\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Initialise replay buffer with correct size\n",
        "ppo_torch_agent.data = ReplayBuffer(\n",
        "    action_prob_exist=True,\n",
        "    max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs, # Corrected max_size for batched collection\n",
        "    state_dim=env.observation_size,\n",
        "    num_action=env.action_size\n",
        ")\n",
        "\n",
        "# Outer loop for PPO updates\n",
        "for update_idx in range(ppo_torch_args.num_updates):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    # Initialize accumulators for metrics over the entire collected batch\n",
        "    total_sum_rewards_collected = 0.0\n",
        "    total_sum_north_rewards_collected = 0.0\n",
        "    total_sum_healthy_rewards_collected = 0.0\n",
        "    total_sum_ctrl_costs_collected = 0.0\n",
        "    total_sum_sideways_costs_collected = 0.0\n",
        "\n",
        "    # Collect for traj_length steps from num_envs parallel environments\n",
        "    for t in range(ppo_torch_args.traj_length):\n",
        "        # Modified: Use DLPack for zero-copy JAX to PyTorch transfer (fixed deprecated API)\n",
        "        obs_torch = torch.utils.dlpack.from_dlpack(env_state.obs).to(device)\n",
        "\n",
        "        # Get action from PyTorch actor (now takes a batch of observations)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Modified: Use DLPack for zero-copy PyTorch to JAX transfer (fixed deprecated API)\n",
        "        action_jax = jax.dlpack.from_dlpack(action_torch)\n",
        "\n",
        "        # Step JAX environment (now processes a batch of actions)\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (for num_envs parallel transitions)\n",
        "        # and accumulate metrics for logging\n",
        "        current_obs_batch_np = obs_torch.cpu().numpy()\n",
        "        current_action_batch_np = action_torch.cpu().numpy()\n",
        "        current_log_prob_batch_np = log_prob_torch.cpu().numpy()\n",
        "\n",
        "        next_obs_batch_np = np.array(next_env_state.obs)\n",
        "        reward_batch_np = np.array(next_env_state.reward)\n",
        "        done_batch_np = np.array(next_env_state.done)\n",
        "\n",
        "        north_reward_batch_np = np.array(next_env_state.metrics['north_reward'])\n",
        "        healthy_reward_batch_np = np.array(next_env_state.metrics['reward_alive'])\n",
        "        ctrl_cost_batch_np = -np.array(next_env_state.metrics['reward_quadctrl']) # ctrl_cost is stored as negative in metrics\n",
        "        sideways_cost_batch_np = np.array(next_env_state.metrics['sideways_cost'])\n",
        "\n",
        "        # Store transitions in PyTorch ReplayBuffer and accumulate metrics\n",
        "        for env_idx in range(ppo_torch_args.num_envs):\n",
        "            # The `if env_idx == 0:` condition was removed, as all envs' metrics contribute to the sum.\n",
        "            transition = make_transition(\n",
        "                current_obs_batch_np[env_idx],\n",
        "                current_action_batch_np[env_idx],\n",
        "                reward_batch_np[env_idx].reshape(1),\n",
        "                next_obs_batch_np[env_idx],\n",
        "                done_batch_np[env_idx].reshape(1),\n",
        "                current_log_prob_batch_np[env_idx]\n",
        "            )\n",
        "            ppo_torch_agent.put_data(transition)\n",
        "\n",
        "            # Accumulate sum of rewards and costs across all environments\n",
        "            total_sum_rewards_collected += reward_batch_np[env_idx]\n",
        "            total_sum_north_rewards_collected += north_reward_batch_np[env_idx]\n",
        "            total_sum_healthy_rewards_collected += healthy_reward_batch_np[env_idx]\n",
        "            total_sum_ctrl_costs_collected += ctrl_cost_batch_np[env_idx]\n",
        "            total_sum_sideways_costs_collected += sideways_cost_batch_np[env_idx]\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += ppo_torch_args.num_envs # Increment total steps by the batch size\n",
        "\n",
        "    # Removed: Early exit and print logic for single environment, now fixed length collection\n",
        "    # The print statement now reflects completion of traj_length steps for all environments\n",
        "\n",
        "    # After collecting traj_length steps for all num_envs:\n",
        "    # Calculate average metrics over all collected transitions in this update\n",
        "    num_collected_transitions = ppo_torch_args.traj_length * ppo_torch_args.num_envs\n",
        "    avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "    avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "    avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "    avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "    avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "\n",
        "    # Modified: Print statements use averaged metrics\n",
        "    print(f\"Update {update_idx + 1} completed. Avg Total Reward: {avg_total_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "    print(f\"  Avg Reward Components: North Reward: {avg_north_reward:.2f}, Healthy Reward: {avg_healthy_reward:.2f}, Ctrl Cost: {avg_ctrl_cost:.2f}, Sideways Cost: {avg_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length * num_envs steps\n",
        "    # The buffer size `max_size` is now correctly `traj_length * num_envs`\n",
        "    if ppo_torch_agent.data.size() >= num_collected_transitions:\n",
        "        # Modified: Pass update_idx to train_net\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(update_idx)\n",
        "        print(f\"PPO agent trained for update {update_idx + 1}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        # --- Log to CSV ---\n",
        "        # Modified: Log averaged metrics\n",
        "        csv_writer.writerow([\n",
        "            update_idx + 1, # Log update_idx\n",
        "            float(avg_total_reward),\n",
        "            float(avg_north_reward),\n",
        "            float(avg_healthy_reward),\n",
        "            float(avg_ctrl_cost),\n",
        "            float(avg_sideways_cost),\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps\n",
        "        ])\n",
        "        csv_file.flush() # Ensure data is written to disk immediately\n",
        "        # --- End Log to CSV ---\n",
        "\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "# --- Close CSV file after training ---\n",
        "csv_file.close()\n",
        "print(f\"Training data saved to {csv_filename}\")\n",
        "# --- End Close CSV file ---\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "# For evaluation, we still use a single environment for visualization consistency.\n",
        "eval_env = envs.create(env_name=env_name, episode_length=2000, batch_size=1) # Create a single env for evaluation\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 1000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Modified: Use DLPack for zero-copy JAX to PyTorch transfer in evaluation (fixed deprecated API)\n",
        "    obs_torch_eval = torch.utils.dlpack.from_dlpack(eval_state.obs).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Modified: Use DLPack for zero-copy PyTorch to JAX transfer in evaluation (fixed deprecated API)\n",
        "    action_jax_eval = jax.dlpack.from_dlpack(action_torch_eval.squeeze(0))\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done[0]: # Check done for the first (and only) environment in eval batch\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model to path"
      ],
      "metadata": {
        "id": "GDUxLOQjcfPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a filename for the saved model\n",
        "model_save_path = \"trained_ppo_model.pth\"\n",
        "\n",
        "# Save the state dictionary of the PPO agent\n",
        "torch.save(ppo_torch_agent.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Trained PPO model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "zfO1dzXAFB-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (assuming they are already imported in previous cells)\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Re-instantiate PPOConfig if the kernel state might have been reset\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 8 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 1e-3\n",
        "        self.train_epoch = 4 # Number of PPO epochs\n",
        "        self.batch_size = 1024 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.2\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 4096 # Number of parallel environments\n",
        "        self.num_updates = 100 # Default number of new PPO updates\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "# Reuse the ScaleRewardWrapper if it was part of the original environment setup\n",
        "class ScaleRewardWrapper(envs.Wrapper):\n",
        "    def __init__(self, env, scale=0.1):\n",
        "        super().__init__(env)\n",
        "        self.scale = scale\n",
        "\n",
        "    def reset(self, rng):\n",
        "        state = super().reset(rng)\n",
        "        return state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        state = super().step(state, action)\n",
        "        return state.replace(reward=state.reward * self.scale)\n",
        "\n",
        "env = envs.create(env_name=env_name, episode_length=500, batch_size=ppo_torch_args.num_envs)\n",
        "env = ScaleRewardWrapper(env, scale=0.1) # Applying a reward scale during new training\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "print(f\"DEBUG: Number of environments (batch_size): {env.batch_size}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model_save_path = \"trained_ppo_model.pth\"\n",
        "try:\n",
        "    ppo_torch_agent.load_state_dict(torch.load(model_save_path))\n",
        "    print(f\"Successfully loaded model from {model_save_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Model file not found at {model_save_path}. Starting training from scratch.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}. Starting training from scratch.\")\n",
        "\n",
        "print(\"PyTorch PPO agent initialized (or loaded).\")\n",
        "\n",
        "# --- CSV Logging Setup for new training session ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_resumed_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Modified: Added 'avg_episode_length' to the header\n",
        "header = [\n",
        "    'update_idx', 'avg_total_reward', 'avg_north_reward', 'avg_healthy_reward',\n",
        "    'avg_ctrl_cost', 'avg_sideways_cost', 'avg_orientation_cost', 'avg_z_angular_cost', # Added new costs\n",
        "    'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps', 'avg_episode_length'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging new training data to {csv_filename}\")\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "current_total_steps = 0\n",
        "rng = jax.random.PRNGKey(0) # Re-initialize RNG for this training session\n",
        "\n",
        "# Added: Initialize episode_step_counts for each parallel environment\n",
        "episode_step_counts = jnp.zeros(ppo_torch_args.num_envs, dtype=jnp.int32)\n",
        "\n",
        "print(\"Starting new PyTorch PPO training loop...\")\n",
        "\n",
        "ppo_torch_agent.data = ReplayBuffer(\n",
        "    action_prob_exist=True,\n",
        "    max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "    state_dim=env.observation_size,\n",
        "    num_action=env.action_size\n",
        ")\n",
        "\n",
        "for update_idx in range(ppo_torch_args.num_updates):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    total_sum_rewards_collected = 0.0\n",
        "    total_sum_north_rewards_collected = 0.0\n",
        "    total_sum_healthy_rewards_collected = 0.0\n",
        "    total_sum_ctrl_costs_collected = 0.0\n",
        "    total_sum_sideways_costs_collected = 0.0\n",
        "    total_sum_orientation_costs_collected = 0.0 # Added\n",
        "    total_sum_z_angular_costs_collected = 0.0 # Added\n",
        "    # Added: Accumulators for average episode length\n",
        "    total_episode_lengths_collected = 0\n",
        "    terminated_episodes_count = 0\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length):\n",
        "        obs_torch = torch.utils.dlpack.from_dlpack(env_state.obs).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        action_jax = jax.dlpack.from_dlpack(action_torch)\n",
        "\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Added: Update episode_step_counts and accumulate terminated episode lengths\n",
        "        # Increment step counts for all active environments\n",
        "        episode_step_counts = episode_step_counts + 1\n",
        "\n",
        "        # Identify environments that terminated in this step\n",
        "        terminated_mask = next_env_state.done # This is a JAX array of booleans\n",
        "\n",
        "        # Add lengths of newly terminated episodes to total_episode_lengths_collected\n",
        "        # Only sum episode_step_counts for environments where terminated_mask is True\n",
        "        terminated_lengths_this_step = episode_step_counts * terminated_mask\n",
        "        total_episode_lengths_collected += jnp.sum(terminated_lengths_this_step).item() # .item() to convert JAX scalar to Python scalar\n",
        "        terminated_episodes_count += jnp.sum(terminated_mask).item() # .item()\n",
        "\n",
        "        # Reset step counts for environments that terminated\n",
        "        episode_step_counts = episode_step_counts * (1 - terminated_mask)\n",
        "\n",
        "        current_obs_batch_np = obs_torch.cpu().numpy()\n",
        "        current_action_batch_np = action_torch.cpu().numpy()\n",
        "        current_log_prob_batch_np = log_prob_torch.cpu().numpy()\n",
        "\n",
        "        next_obs_batch_np = np.array(next_env_state.obs)\n",
        "        reward_batch_np = np.array(next_env_state.reward)\n",
        "        done_batch_np = np.array(next_env_state.done)\n",
        "\n",
        "        north_reward_batch_np = np.array(next_env_state.metrics['north_reward'])\n",
        "        healthy_reward_batch_np = np.array(next_env_state.metrics['reward_alive'])\n",
        "        ctrl_cost_batch_np = -np.array(next_env_state.metrics['reward_quadctrl'])\n",
        "        sideways_cost_batch_np = np.array(next_env_state.metrics['sideways_cost'])\n",
        "        orientation_cost_batch_np = np.array(next_env_state.metrics['orientation_cost']) # Added\n",
        "        z_angular_cost_batch_np = np.array(next_env_state.metrics['z_angular_cost']) # Added\n",
        "\n",
        "        for env_idx in range(ppo_torch_args.num_envs):\n",
        "            transition = make_transition(\n",
        "                current_obs_batch_np[env_idx],\n",
        "                current_action_batch_np[env_idx],\n",
        "                reward_batch_np[env_idx].reshape(1),\n",
        "                next_obs_batch_np[env_idx],\n",
        "                done_batch_np[env_idx].reshape(1),\n",
        "                current_log_prob_batch_np[env_idx]\n",
        "            )\n",
        "            ppo_torch_agent.put_data(transition)\n",
        "\n",
        "            total_sum_rewards_collected += reward_batch_np[env_idx]\n",
        "            total_sum_north_rewards_collected += north_reward_batch_np[env_idx]\n",
        "            total_sum_healthy_rewards_collected += healthy_reward_batch_np[env_idx]\n",
        "            total_sum_ctrl_costs_collected += ctrl_cost_batch_np[env_idx]\n",
        "            total_sum_sideways_costs_collected += sideways_cost_batch_np[env_idx]\n",
        "            total_sum_orientation_costs_collected += orientation_cost_batch_np[env_idx] # Added\n",
        "            total_sum_z_angular_costs_collected += z_angular_cost_batch_np[env_idx] # Added\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += ppo_torch_args.num_envs\n",
        "\n",
        "    num_collected_transitions = ppo_torch_args.traj_length * ppo_torch_args.num_envs\n",
        "    avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "    avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "    avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "    avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "    avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "    avg_orientation_cost = total_sum_orientation_costs_collected / num_collected_transitions # Added\n",
        "    avg_z_angular_cost = total_sum_z_angular_costs_collected / num_collected_transitions # Added\n",
        "\n",
        "    # Added: Calculate average episode length\n",
        "    avg_episode_length = total_episode_lengths_collected / terminated_episodes_count if terminated_episodes_count > 0 else 0.0\n",
        "\n",
        "    print(f\"Update {update_idx + 1} completed. Avg Total Reward: {avg_total_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "    print(f\"  Avg Reward Components: North Reward: {avg_north_reward:.2f}, Healthy Reward: {avg_healthy_reward:.2f}, Ctrl Cost: {avg_ctrl_cost:.2f}, Sideways Cost: {avg_sideways_cost:.2f}\")\n",
        "    print(f\"  Added Costs: Orientation Cost: {avg_orientation_cost:.2f}, Z Angular Cost: {avg_z_angular_cost:.2f}\") # Added\n",
        "    # Added: Print average episode length\n",
        "    print(f\"  Avg Episode Length: {avg_episode_length:.2f} (from {terminated_episodes_count} terminations)\")\n",
        "\n",
        "    if ppo_torch_agent.data.size() >= num_collected_transitions:\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(update_idx)\n",
        "        print(f\"PPO agent trained for update {update_idx + 1}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        csv_writer.writerow([\n",
        "            update_idx + 1,\n",
        "            float(avg_total_reward),\n",
        "            float(avg_north_reward),\n",
        "            float(avg_healthy_reward),\n",
        "            float(avg_ctrl_cost),\n",
        "            float(avg_sideways_cost),\n",
        "            float(avg_orientation_cost), # Added\n",
        "            float(avg_z_angular_cost), # Added\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps,\n",
        "            float(avg_episode_length) # Added: Log average episode length\n",
        "        ])\n",
        "        csv_file.flush()\n",
        "\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "csv_file.close()\n",
        "print(f\"New training data saved to {csv_filename}\")\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with newly trained PyTorch PPO policy...\")\n",
        "eval_env = envs.create(env_name=env_name, episode_length=2000, batch_size=1)\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 1000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    obs_torch_eval = torch.utils.dlpack.from_dlpack(eval_state.obs).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval\n",
        "\n",
        "    action_jax_eval = jax.dlpack.from_dlpack(action_torch_eval.squeeze(0))\n",
        "\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done[0]:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from new PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "metadata": {
        "id": "FBBQ73FRcW_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with newly trained PyTorch PPO policy...\")\n",
        "eval_env = envs.create(env_name=env_name, episode_length=500, batch_size=1)\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 1000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    obs_torch_eval = torch.utils.dlpack.from_dlpack(eval_state.obs).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval\n",
        "\n",
        "    action_jax_eval = jax.dlpack.from_dlpack(action_torch_eval.squeeze(0))\n",
        "\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done[0]:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from new PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "metadata": {
        "id": "_ZVX-ZYwiBtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectrized PPO"
      ],
      "metadata": {
        "id": "Xvzi1Wz5Is_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Nowa, Poprawna Klasa PPO (Bez ReplayBuffer) ---\n",
        "class PPO_Vectorized(nn.Module):\n",
        "    def __init__(self, device, state_dim, action_dim, args):\n",
        "        super(PPO_Vectorized, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "\n",
        "        # Twoje sieci (zakładam, że klasy Actor/Critic masz zdefiniowane tak samo)\n",
        "        self.actor = Actor(args.layer_num, state_dim, action_dim, args.hidden_dim,\n",
        "                           args.activation_function, args.last_activation, args.trainable_std)\n",
        "        self.critic = Critic(args.layer_num, state_dim, 1,\n",
        "                             args.hidden_dim, args.activation_function, args.last_activation)\n",
        "\n",
        "        self.actor.to(device)\n",
        "        self.critic.to(device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
        "\n",
        "    def get_action(self, x):\n",
        "        return self.actor(x)\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    # --- KLUCZOWA ZMIANA: GAE na tensorach 2D (Time, Batch) ---\n",
        "    def compute_gae(self, rewards, values, next_values, dones):\n",
        "        \"\"\"\n",
        "        rewards: (Steps, Envs)\n",
        "        values: (Steps, Envs)\n",
        "        next_values: (Steps, Envs)\n",
        "        dones: (Steps, Envs)\n",
        "        \"\"\"\n",
        "        # Obliczamy TD error dla całej macierzy naraz\n",
        "        delta = rewards + self.args.gamma * next_values * (1 - dones) - values\n",
        "\n",
        "        advantage = torch.zeros_like(rewards).to(self.device)\n",
        "        last_gae_lam = 0\n",
        "\n",
        "        # Iterujemy TYLKO po czasie (wstecz).\n",
        "        # Operacje wewnątrz pętli są wektoryzowane po wymiarze 'Envs'.\n",
        "        # Dzięki temu Env0 bierze dane z przyszłości Env0, a Env1 z Env1.\n",
        "        step_len = rewards.shape[0]\n",
        "\n",
        "        for t in reversed(range(step_len)):\n",
        "            advantage[t] = delta[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantage[t]\n",
        "\n",
        "        return advantage\n",
        "\n",
        "    def update(self, states, actions, log_probs, rewards, next_states, dones):\n",
        "        # 1. Oblicz wartości (V) dla wszystkich kroków\n",
        "        # states shape: (Steps, Envs, ObsDim)\n",
        "\n",
        "        # Spłaszczamy na chwilę do (Steps*Envs, ObsDim) żeby przepuścić przez sieć\n",
        "        T, B, D = states.shape\n",
        "        flat_states = states.view(-1, D)\n",
        "        flat_next_states = next_states.view(-1, D)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            values = self.critic(flat_states).view(T, B)\n",
        "            next_values = self.critic(flat_next_states).view(T, B)\n",
        "\n",
        "        # 2. Oblicz GAE (poprawnie, zachowując separację środowisk!)\n",
        "        advantages = self.compute_gae(rewards, values, next_values, dones)\n",
        "        returns = advantages + values\n",
        "\n",
        "        # 3. Spłaszczanie danych do treningu (Mieszanie)\n",
        "        # Dopiero TERAZ mieszamy dane, gdy GAE jest już policzone poprawnie.\n",
        "        b_states = states.view(-1, D)\n",
        "        b_actions = actions.view(-1, self.args.action_dim) if self.args.action_dim > 1 else actions.view(-1)\n",
        "        b_log_probs = log_probs.view(-1, 1)\n",
        "        b_advantages = advantages.view(-1, 1)\n",
        "        b_returns = returns.view(-1, 1)\n",
        "        b_values = values.view(-1, 1)\n",
        "\n",
        "        # Normalizacja Advantages (ważne dla stabilności)\n",
        "        b_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\n",
        "\n",
        "        # 4. Pętla optymalizacji (Mini-batche)\n",
        "        total_idxs = T * B\n",
        "        idxs = np.arange(total_idxs)\n",
        "\n",
        "        avg_pg_loss = 0\n",
        "        avg_v_loss = 0\n",
        "        avg_ent_loss = 0\n",
        "        updates = 0\n",
        "\n",
        "        for epoch in range(self.args.train_epoch):\n",
        "            np.random.shuffle(idxs)\n",
        "            for start in range(0, total_idxs, self.args.batch_size):\n",
        "                end = start + self.args.batch_size\n",
        "                mb_idx = idxs[start:end]\n",
        "\n",
        "                # Pobieramy mini-batch\n",
        "                mb_states = b_states[mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_old_log_probs = b_log_probs[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_old_values = b_values[mb_idx]\n",
        "\n",
        "                # ... (Tutaj wstawiasz standardowy kod PPO loss z Twojego poprzedniego snippetu) ...\n",
        "                # Aktor\n",
        "                mu, sigma = self.actor(mb_states)\n",
        "                dist = torch.distributions.Normal(mu, sigma)\n",
        "                new_log_probs = dist.log_prob(mb_actions).sum(dim=-1, keepdim=True)\n",
        "                entropy = dist.entropy().sum(dim=-1).mean()\n",
        "                ratio = torch.exp(new_log_probs - mb_old_log_probs)\n",
        "                surr1 = ratio * mb_advantages\n",
        "                surr2 = torch.clamp(ratio, 1.0 - self.args.max_clip, 1.0 + self.args.max_clip) * mb_advantages\n",
        "                pg_loss = -torch.min(surr1, surr2).mean() - self.args.entropy_coef * entropy\n",
        "\n",
        "                # Krytyk\n",
        "                new_values = self.critic(mb_states)\n",
        "                # Value clipping (opcjonalne, ale dobre)\n",
        "                v_loss_unclipped = (new_values - mb_returns).pow(2)\n",
        "                v_clipped = mb_old_values + torch.clamp(new_values - mb_old_values, -self.args.max_clip, self.args.max_clip)\n",
        "                v_loss_clipped = (v_clipped - mb_returns).pow(2)\n",
        "                v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
        "\n",
        "                # Update\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                pg_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                v_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                avg_pg_loss += pg_loss.item()\n",
        "                avg_v_loss += v_loss.item()\n",
        "                avg_ent_loss += entropy.item()\n",
        "                updates += 1\n",
        "\n",
        "        return avg_pg_loss/updates, avg_ent_loss/updates, avg_v_loss/updates\n"
      ],
      "metadata": {
        "id": "7TyMYjHIITG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Zakładamy, że klasa PPO_Vectorized oraz definicje Actor/Critic są w pliku fixed_ppo_gae.py\n",
        "# from fixed_ppo_gae import PPO_Vectorized\n",
        "\n",
        "# --- 1. Klasa pomocnicza do normalizacji (Running Mean Std) ---\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = self.update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - mean\n",
        "        tot_count = count + batch_count\n",
        "        new_mean = mean + delta * batch_count / tot_count\n",
        "        m_a = var * count\n",
        "        m_b = batch_var * batch_count\n",
        "        M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "        new_var = M2 / tot_count\n",
        "        new_count = tot_count\n",
        "        return new_mean, new_var, new_count\n",
        "\n",
        "    def normalize(self, x):\n",
        "        return (x - self.mean) / np.sqrt(self.var + 1e-8)\n",
        "\n",
        "# --- 2. Konfiguracja ---\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 16      # Długość zbieranej trajektorii\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 1e-3\n",
        "        self.critic_lr = 5e-3\n",
        "        self.train_epoch = 4\n",
        "        self.batch_size = 512\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95        # GAE Lambda\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 2048       # Liczba środowisk równoległych\n",
        "        self.num_updates = 2000     # Liczba pętli treningowych\n",
        "        self.action_dim = None # Will be set dynamically\n",
        "\n",
        "args = PPOConfig()\n",
        "\n",
        "# --- 3. Inicjalizacja Środowiska ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.create(env_name=env_name, episode_length=500, batch_size=args.num_envs)\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# Set action_dim after environment is created\n",
        "args.action_dim = env.action_size\n",
        "\n",
        "# --- 4. Inicjalizacja Agenta i Urządzenia ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Urządzenie treningowe: {device}\")\n",
        "\n",
        "# Inicjalizacja agenta (używamy klasy z Twojego pliku fixed_ppo_gae)\n",
        "# Upewnij się, że masz zaimportowaną klasę PPO_Vectorized!\n",
        "# agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "\n",
        "# # Inicjalizacja normalizera\n",
        "# obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "\n",
        "# # --- 5. Przygotowanie buforów pamięci (Tensorów) ---\n",
        "# # To jest kluczowe dla szybkości i poprawności GAE.\n",
        "# # Alokujemy pamięć raz na GPU.\n",
        "# steps = args.traj_length\n",
        "# num_envs = args.num_envs\n",
        "# obs_dim = env.observation_size\n",
        "# act_dim = env.action_size\n",
        "\n",
        "# storage_obs = torch.zeros((steps, num_envs, obs_dim), device=device)\n",
        "# storage_next_obs = torch.zeros((steps, num_envs, obs_dim), device=device)\n",
        "# storage_actions = torch.zeros((steps, num_envs, act_dim), device=device)\n",
        "# storage_rewards = torch.zeros((steps, num_envs), device=device)\n",
        "# storage_dones = torch.zeros((steps, num_envs), device=device)\n",
        "# storage_log_probs = torch.zeros((steps, num_envs), device=device)\n",
        "\n",
        "# # --- 6. Główna Pętla Treningowa ---\n",
        "# rng = jax.random.PRNGKey(0)\n",
        "# rng, reset_rng = jax.random.split(rng)\n",
        "# state = jit_reset(reset_rng)\n",
        "\n",
        "# # Logowanie\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "# log_file = open(f\"training_log_{timestamp}.csv\", \"w\", newline=\"\")\n",
        "# writer = csv.writer(log_file)\n",
        "# writer.writerow([\"Update\", \"Avg_Total_Reward\", \"Avg_North_Reward\", \"Avg_Healthy_Reward\", \"Avg_Ctrl_Cost\", \"Avg_Sideways_Cost\", \"Avg_Orientation_Cost\", \"Avg_Z_Angular_Cost\", \"Avg_Episode_Length\", \"PG_Loss\", \"Val_Loss\", \"Ent_Loss\"])\n",
        "\n",
        "# print(\"Rozpoczynam trening...\")\n",
        "\n",
        "# # Added: Initialize episode_step_counts for each parallel environment\n",
        "# episode_step_counts = jnp.zeros(args.num_envs, dtype=jnp.int32)\n",
        "\n",
        "# for update in range(args.num_updates):\n",
        "\n",
        "#     # === FAZA 1: ZBIERANIE DANYCH (ROLLOUT) ===\n",
        "#     total_sum_rewards_collected = 0.0\n",
        "#     total_sum_north_rewards_collected = 0.0\n",
        "#     total_sum_healthy_rewards_collected = 0.0\n",
        "#     total_sum_ctrl_costs_collected = 0.0\n",
        "#     total_sum_sideways_costs_collected = 0.0\n",
        "#     total_sum_orientation_costs_collected = 0.0\n",
        "#     total_sum_z_angular_costs_collected = 0.0\n",
        "#     # Added: Accumulators for average episode length\n",
        "#     total_episode_lengths_collected = 0\n",
        "#     terminated_episodes_count = 0\n",
        "\n",
        "#     for t in range(steps):\n",
        "#         # 1. Pobranie i normalizacja obserwacji (JAX -> Numpy)\n",
        "#         obs_np = np.array(state.obs)\n",
        "#         obs_rms.update(obs_np) # Aktualizacja statystyk\n",
        "#         obs_norm = obs_rms.normalize(obs_np) # Normalizacja\n",
        "\n",
        "#         # 2. Konwersja do Torch\n",
        "#         obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "#         # 3. Wybór akcji przez Agenta\n",
        "#         with torch.no_grad():\n",
        "#             mu, sigma = agent.get_action(obs_torch)\n",
        "#             dist = torch.distributions.Normal(mu, sigma)\n",
        "#             action = dist.sample()\n",
        "#             log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "#         # 4. Krok środowiska (Torch -> JAX)\n",
        "#         action_jax = jax.dlpack.from_dlpack(action)\n",
        "#         rng, step_rng = jax.random.split(rng)\n",
        "#         next_state = jit_step(state, action_jax)\n",
        "\n",
        "#         # 5. Normalizacja 'next_obs' (potrzebne do Value Function target)\n",
        "#         next_obs_np = np.array(next_state.obs)\n",
        "#         next_obs_norm = obs_rms.normalize(next_obs_np)\n",
        "\n",
        "#         # 6. ZAPIS DO BUFORA (Wektoryzowany)\n",
        "#         storage_obs[t] = obs_torch\n",
        "#         storage_next_obs[t] = torch.tensor(next_obs_norm, dtype=torch.float32, device=device)\n",
        "#         storage_actions[t] = action\n",
        "#         storage_rewards[t] = torch.tensor(np.array(next_state.reward), dtype=torch.float32, device=device)\n",
        "#         storage_dones[t] = torch.tensor(np.array(next_state.done), dtype=torch.float32, device=device)\n",
        "#         storage_log_probs[t] = log_prob\n",
        "\n",
        "#         # Added: Update episode_step_counts and accumulate terminated episode lengths\n",
        "#         episode_step_counts = episode_step_counts + 1\n",
        "#         terminated_mask = np.array(next_state.done) # Convert JAX array to numpy for boolean indexing\n",
        "\n",
        "#         terminated_lengths_this_step = episode_step_counts * terminated_mask\n",
        "#         total_episode_lengths_collected += np.sum(terminated_lengths_this_step) # Sum numpy array\n",
        "#         terminated_episodes_count += np.sum(terminated_mask) # Sum numpy array\n",
        "\n",
        "#         episode_step_counts = episode_step_counts * (1 - terminated_mask)\n",
        "\n",
        "#         # Metryki\n",
        "#         total_sum_rewards_collected += np.sum(next_state.reward)\n",
        "#         total_sum_north_rewards_collected += np.sum(next_state.metrics['north_reward'])\n",
        "#         total_sum_healthy_rewards_collected += np.sum(next_state.metrics['reward_alive'])\n",
        "#         total_sum_ctrl_costs_collected += np.sum(-next_state.metrics['reward_quadctrl'])\n",
        "#         total_sum_sideways_costs_collected += np.sum(next_state.metrics['sideways_cost'])\n",
        "#         total_sum_orientation_costs_collected += np.sum(next_state.metrics['orientation_cost'])\n",
        "#         total_sum_z_angular_costs_collected += np.sum(next_state.metrics['z_angular_cost'])\n",
        "\n",
        "#         # Przejście do nowego stanu\n",
        "#         state = next_state\n",
        "\n",
        "#     # === FAZA 2: AKTUALIZACJA SIECI (PPO UPDATE) ===\n",
        "#     # Wywołujemy nową metodę update, która obsługuje tensory (Steps, Envs)\n",
        "#     pg_loss, ent_loss, val_loss = agent.update(\n",
        "#         storage_obs,\n",
        "#         storage_actions,\n",
        "#         storage_log_probs,\n",
        "#         storage_rewards,\n",
        "#         storage_next_obs,\n",
        "#         storage_dones\n",
        "#     )\n",
        "\n",
        "#     # === FAZA 3: LOGOWANIE I ZAPIS ===\n",
        "#     num_collected_transitions = steps * num_envs\n",
        "\n",
        "#     avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "#     avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "#     avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "#     avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "#     avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "#     avg_orientation_cost = total_sum_orientation_costs_collected / num_collected_transitions\n",
        "#     avg_z_angular_cost = total_sum_z_angular_costs_collected / num_collected_transitions\n",
        "\n",
        "#     avg_episode_length = total_episode_lengths_collected / terminated_episodes_count if terminated_episodes_count > 0 else 0.0\n",
        "\n",
        "#     print(f\"Update {update+1}: Avg Total Reward={avg_total_reward:.3f} | PG={pg_loss:.4f} Val={val_loss:.4f} Ent={ent_loss:.4f}\")\n",
        "#     print(f\"  Avg Rewards: North={avg_north_reward:.3f}, Healthy={avg_healthy_reward:.3f}, Ctrl Cost={avg_ctrl_cost:.3f}, Sideways Cost={avg_sideways_cost:.3f}\")\n",
        "#     print(f\"  Avg Added Costs: Orientation={avg_orientation_cost:.3f}, Z Angular={avg_z_angular_cost:.3f}\")\n",
        "#     print(f\"  Avg Episode Length: {avg_episode_length:.2f} (from {terminated_episodes_count} terminations)\")\n",
        "\n",
        "#     writer.writerow([update+1, avg_total_reward, avg_north_reward, avg_healthy_reward, avg_ctrl_cost, avg_sideways_cost, avg_orientation_cost, avg_z_angular_cost, avg_episode_length, pg_loss, val_loss, ent_loss])\n",
        "#     log_file.flush()\n",
        "\n",
        "#     # Checkpointing co 10 update'ów\n",
        "#     if (update + 1) % 10 == 0:\n",
        "#         checkpoint = {\n",
        "#             'model_state_dict': agent.state_dict(),\n",
        "#             'actor_opt': agent.actor_optimizer.state_dict(),\n",
        "#             'critic_opt': agent.critic_optimizer.state_dict(),\n",
        "#             'obs_rms_mean': obs_rms.mean,\n",
        "#             'obs_rms_var': obs_rms.var,\n",
        "#             'obs_rms_count': obs_rms.count\n",
        "#         }\n",
        "#         torch.save(checkpoint, \"ppo_vectorized_checkpoint.pth\")\n",
        "\n",
        "# print(\"Trening zakończony.\")\n",
        "# log_file.close()\n",
        "\n",
        "agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "# Bufory pamięci na GPU\n",
        "steps = args.traj_length\n",
        "num_envs = args.num_envs\n",
        "storage_obs = torch.zeros((steps, num_envs, env.observation_size), device=device)\n",
        "storage_next_obs = torch.zeros((steps, num_envs, env.observation_size), device=device)\n",
        "storage_actions = torch.zeros((steps, num_envs, env.action_size), device=device)\n",
        "storage_rewards = torch.zeros((steps, num_envs), device=device)\n",
        "storage_dones = torch.zeros((steps, num_envs), device=device)\n",
        "storage_log_probs = torch.zeros((steps, num_envs), device=device)\n",
        "\n",
        "# Start środowiska\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, reset_rng = jax.random.split(rng)\n",
        "state = jit_reset(reset_rng)\n",
        "\n",
        "# Logowanie\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = open(f\"spider_training_{timestamp}.csv\", \"w\", newline=\"\")\n",
        "writer = csv.writer(log_file)\n",
        "# Zaktualizowane nagłówki CSV pasujące do nowego środowiska\n",
        "writer.writerow([\"Update\", \"Avg_Total_Reward\", \"Avg_Forward_Reward\", \"Avg_Healthy_Reward\", \"Avg_Ctrl_Cost\", \"Avg_X_Velocity\", \"Avg_Episode_Length\", \"PG_Loss\", \"Val_Loss\"])\n",
        "\n",
        "episode_step_counts = jnp.zeros(args.num_envs, dtype=jnp.int32)\n",
        "print(\"Rozpoczynam trening Pająka...\")\n",
        "\n",
        "for update in range(args.num_updates):\n",
        "    # --- Zbieranie metryk ---\n",
        "    total_rewards = 0.0\n",
        "    total_forward_reward = 0.0\n",
        "    total_healthy_reward = 0.0\n",
        "    total_ctrl_cost = 0.0\n",
        "    total_x_vel = 0.0\n",
        "\n",
        "    collected_len = 0\n",
        "    terminated_count = 0\n",
        "\n",
        "    # === ROLLOUT ===\n",
        "    for t in range(steps):\n",
        "        # 1. Normalizacja obserwacji (Update + Normalize)\n",
        "        obs_np = np.array(state.obs)\n",
        "        obs_rms.update(obs_np) # Aktualizujemy wiedzę o świecie\n",
        "        obs_norm = obs_rms.normalize(obs_np)\n",
        "\n",
        "        obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = agent.get_action(obs_torch)\n",
        "            dist = torch.distributions.Normal(mu, sigma)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "        action_jax = jax.dlpack.from_dlpack(action)\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_state = jit_step(state, action_jax)\n",
        "\n",
        "        # 2. Normalizacja next_obs (do bufora)\n",
        "        next_obs_norm = obs_rms.normalize(np.array(next_state.obs))\n",
        "\n",
        "        # 3. Zapis do bufora\n",
        "        storage_obs[t] = obs_torch\n",
        "        storage_next_obs[t] = torch.tensor(next_obs_norm, dtype=torch.float32, device=device)\n",
        "        storage_actions[t] = action\n",
        "        storage_rewards[t] = torch.tensor(np.array(next_state.reward), dtype=torch.float32, device=device)\n",
        "        storage_dones[t] = torch.tensor(np.array(next_state.done), dtype=torch.float32, device=device)\n",
        "        storage_log_probs[t] = log_prob\n",
        "\n",
        "        # Agregacja metryk\n",
        "        episode_step_counts += 1\n",
        "        done_mask_np = np.array(next_state.done)\n",
        "        collected_len += np.sum(episode_step_counts * done_mask_np)\n",
        "        terminated_count += np.sum(done_mask_np)\n",
        "        episode_step_counts *= (1 - done_mask_np)\n",
        "\n",
        "        total_rewards += np.sum(next_state.reward)\n",
        "        total_forward_reward += np.sum(next_state.metrics['forward_reward'])\n",
        "        total_healthy_reward += np.sum(next_state.metrics['reward_alive'])\n",
        "        total_ctrl_cost += np.sum(-next_state.metrics['reward_quadctrl'])\n",
        "        total_x_vel += np.sum(next_state.metrics['x_velocity'])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # === UPDATE ===\n",
        "    pg_loss, ent_loss, val_loss = agent.update(\n",
        "        storage_obs, storage_actions, storage_log_probs, storage_rewards, storage_next_obs, storage_dones\n",
        "    )\n",
        "\n",
        "    # === LOGOWANIE ===\n",
        "    denom = steps * num_envs\n",
        "    avg_ep_len = collected_len / terminated_count if terminated_count > 0 else 0.0\n",
        "\n",
        "    print(f\"Update {update+1}/{args.num_updates}: Reward={total_rewards/denom:.3f} | Len={avg_ep_len:.1f} | X-Vel={total_x_vel/denom:.2f} | Losses: PG={pg_loss:.3f} V={val_loss:.3f}\")\n",
        "\n",
        "    writer.writerow([\n",
        "        update+1,\n",
        "        total_rewards/denom,\n",
        "        total_forward_reward/denom,\n",
        "        total_healthy_reward/denom,\n",
        "        total_ctrl_cost/denom,\n",
        "        total_x_vel/denom,\n",
        "        avg_ep_len,\n",
        "        pg_loss,\n",
        "        val_loss\n",
        "    ])\n",
        "    log_file.flush()\n",
        "\n",
        "    # === PEŁNY ZAPIS CHECKPOINTU (NAPRAWIONE) ===\n",
        "    if (update + 1) % 50 == 0:\n",
        "        filename = f\"ppo_spider_full_checkpoint_{update+1}.pth\"\n",
        "\n",
        "        full_checkpoint = {\n",
        "            'update_idx': update + 1,\n",
        "            'model_state_dict': agent.state_dict(),\n",
        "            'optimizer_actor_state_dict': agent.actor_optimizer.state_dict(),\n",
        "            'optimizer_critic_state_dict': agent.critic_optimizer.state_dict(),\n",
        "            # ZAPISUJEMY STATYSTYKI NORMALIZACJI - TO JEST KLUCZOWE\n",
        "            'obs_rms_mean': obs_rms.mean,\n",
        "            'obs_rms_var': obs_rms.var,\n",
        "            'obs_rms_count': obs_rms.count\n",
        "        }\n",
        "\n",
        "        torch.save(full_checkpoint, filename)\n",
        "        print(f\"💾 Zapisano pełny checkpoint: {filename}\")\n",
        "\n",
        "log_file.close()\n",
        "print(\"Koniec treningu.\")"
      ],
      "metadata": {
        "id": "WuxW28viIKgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. WIZUALIZACJA\n",
        "# ==========================================\n",
        "\n",
        "# Ustawienia\n",
        "checkpoint_file = \"ppo_spider_full_checkpoint_2000.pth\" # <-- Zmień na swój plik!\n",
        "eval_length = 500\n",
        "\n",
        "print(\"Tworzenie środowiska...\")\n",
        "# batch_size=1 jest konieczne do renderowania wideo\n",
        "env = envs.create(env_name='humanoid', episode_length=eval_length, batch_size=1)\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "args = PPOConfig()\n",
        "obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "\n",
        "print(f\"Inicjalizacja agenta (Obs: {env.observation_size}, Act: {env.action_size})...\")\n",
        "agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "\n",
        "if os.path.exists(checkpoint_file):\n",
        "    print(f\"Wczytywanie modelu: {checkpoint_file}\")\n",
        "    try:\n",
        "        # weights_only=False pozwala wczytać RunningMeanStd (numpy)\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device, weights_only=False)\n",
        "\n",
        "        # 1. Wagi sieci\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            agent.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            # Fallback jeśli zapisałeś same wagi bez słownika\n",
        "            agent.load_state_dict(checkpoint)\n",
        "\n",
        "        # 2. Statystyki normalizacji (jeśli są)\n",
        "        # Twój kod treningowy nie zapisywał ich w 'spider_model_X.pth',\n",
        "        # ale w 'ppo_vectorized_checkpoint.pth'.\n",
        "        # Jeśli używasz spider_model_X, statystyki będą domyślne (mean=0, var=1).\n",
        "        # To może być problemem (robot może upaść).\n",
        "        # Najlepiej używać pełnego checkpointu, jeśli go masz.\n",
        "        if 'obs_rms_mean' in checkpoint:\n",
        "            obs_rms.mean = checkpoint['obs_rms_mean']\n",
        "            obs_rms.var = checkpoint['obs_rms_var']\n",
        "            print(\"✅ Statystyki normalizacji wczytane.\")\n",
        "        else:\n",
        "            print(\"⚠️ UWAGA: Brak statystyk normalizacji w pliku modelu.\")\n",
        "            print(\"   Agent może zachowywać się niestabilnie, bo widzi inne wartości niż podczas treningu.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Błąd wczytywania: {e}\")\n",
        "        exit(1)\n",
        "else:\n",
        "    print(f\"❌ Nie znaleziono pliku: {checkpoint_file}\")\n",
        "    exit(1)\n",
        "\n",
        "# --- Generowanie ---\n",
        "print(\"Generowanie symulacji...\")\n",
        "rng = jax.random.PRNGKey(42)\n",
        "state = jit_reset(rng)\n",
        "rollout = []\n",
        "\n",
        "for _ in range(eval_length):\n",
        "    # Normalizacja\n",
        "    obs_np = np.array(state.obs)\n",
        "    obs_norm = obs_rms.normalize(obs_np)\n",
        "    obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Używamy średniej (mu) dla płynnego ruchu bez losowości\n",
        "        mu, _ = agent.get_action(obs_torch)\n",
        "        action = mu\n",
        "\n",
        "    action_jax = jax.dlpack.from_dlpack(action)\n",
        "    state = jit_step(state, action_jax)\n",
        "    rollout.append(state.pipeline_state)\n",
        "\n",
        "    if state.done[0]:\n",
        "        # W Brax done=1 nie przerywa symulacji (auto-reset),\n",
        "        # ale warto wiedzieć, że upadł.\n",
        "        pass\n",
        "\n",
        "print(\"Renderowanie wideo (może chwilę potrwać)...\")\n",
        "# Renderujemy z kamerą śledzącą ('track')\n",
        "video = env.render(rollout[::2], height=480, width=640, camera='track')\n",
        "\n",
        "output_name = f\"viz_{checkpoint_file.replace('.pth', '.mp4')}\"\n",
        "media.write_video(output_name, video, fps=1.0/env.dt/2)\n",
        "print(f\"✅ Zapisano wideo: {output_name}\")\n",
        "\n",
        "# # --- 2. Konfiguracja ---\n",
        "# env_name = 'humanoid'\n",
        "# checkpoint_path = \"spider_model_300.pth\"\n",
        "# eval_episode_length = 500\n",
        "\n",
        "# # --- 3. Inicjalizacja ---\n",
        "# print(\"Inicjalizacja środowiska i agenta...\")\n",
        "# env = envs.create(env_name=env_name, episode_length=eval_episode_length, batch_size=1)\n",
        "# jit_reset = jax.jit(env.reset)\n",
        "# jit_step = jax.jit(env.step)\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# args = PPOConfig()\n",
        "# agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "# obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "\n",
        "# # --- 4. Wczytanie Checkpointu (POPRAWIONE) ---\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     print(f\"Wczytywanie modelu z: {checkpoint_path}\")\n",
        "#     try:\n",
        "#         # --- ZMIANA TUTAJ: weights_only=False ---\n",
        "#         # Ponieważ zapisujemy też numpy arrays (obs_rms) i stan optymalizatora,\n",
        "#         # musimy pozwolić na ładowanie obiektów innych niż czyste wagi.\n",
        "#         checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "#         agent.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#         obs_rms.mean = checkpoint['obs_rms_mean']\n",
        "#         obs_rms.var = checkpoint['obs_rms_var']\n",
        "#         obs_rms.count = checkpoint['obs_rms_count']\n",
        "#         print(\"✅ Model i statystyki wczytane poprawnie.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Błąd wczytywania: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "#         exit(1) # Odkomentuj w skrypcie, w notebooku lepiej widzieć błąd\n",
        "# else:\n",
        "#     print(f\"❌ Nie znaleziono pliku: {checkpoint_path}\")\n",
        "#     # exit(1)\n",
        "\n",
        "# # --- 5. Generowanie Wideo ---\n",
        "# print(f\"Generowanie rolloutu ({eval_episode_length} kroków)...\")\n",
        "# rng = jax.random.PRNGKey(42)\n",
        "# state = jit_reset(rng)\n",
        "# rollout = []\n",
        "\n",
        "# for _ in range(eval_episode_length):\n",
        "#     obs_np = np.array(state.obs)\n",
        "#     obs_norm = obs_rms.normalize(obs_np)\n",
        "#     obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         mu, _ = agent.get_action(obs_torch)\n",
        "#         action = mu\n",
        "\n",
        "#     action_jax = jax.dlpack.from_dlpack(action)\n",
        "#     state = jit_step(state, action_jax)\n",
        "#     rollout.append(state.pipeline_state)\n",
        "\n",
        "#     if state.done[0]:\n",
        "#         print(\"Agent upadł lub ukończył zadanie przed czasem.\")\n",
        "\n",
        "# print(\"Renderowanie wideo...\")\n",
        "# video = env.render(rollout[::2])\n",
        "\n",
        "# output_filename = \"humanoid_policy.mp4\"\n",
        "# media.write_video(output_filename, video, fps=1.0/env.dt/2)\n",
        "# print(f\"✅ Wideo zapisane jako: {output_filename}\")"
      ],
      "metadata": {
        "id": "tdq2hNCjq9rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media.show_video(video, fps=1.0/env.dt/2)\n"
      ],
      "metadata": {
        "id": "xtNgUGslKNO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train from checkpoint - vectorized ppo"
      ],
      "metadata": {
        "id": "lTlhQI3eczM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Zakładamy, że klasa PPO_Vectorized oraz definicje Actor/Critic są w pliku fixed_ppo_gae.py\n",
        "# from fixed_ppo_gae import PPO_Vectorized\n",
        "\n",
        "# --- 1. Klasa pomocnicza do normalizacji (Running Mean Std) ---\n",
        "class RunningMeanStd:\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = self.update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_mean_var_count_from_moments(self, mean, var, count, batch_mean, batch_var, batch_count):\n",
        "        delta = batch_mean - mean\n",
        "        tot_count = count + batch_count\n",
        "        new_mean = mean + delta * batch_count / tot_count\n",
        "        m_a = var * count\n",
        "        m_b = batch_var * batch_count\n",
        "        M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "        new_var = M2 / tot_count\n",
        "        new_count = tot_count\n",
        "        return new_mean, new_var, new_count\n",
        "\n",
        "    def normalize(self, x):\n",
        "        return (x - self.mean) / np.sqrt(self.var + 1e-8)\n",
        "\n",
        "# --- 2. Konfiguracja ---\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 16      # Długość zbieranej trajektorii\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 1e-4\n",
        "        self.critic_lr = 5e-4\n",
        "        self.train_epoch = 4\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95        # GAE Lambda\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 512       # Liczba środowisk równoległych\n",
        "        self.num_updates = 2000     # Liczba pętli treningowych\n",
        "        self.action_dim = None # Will be set dynamically\n",
        "\n",
        "args = PPOConfig()\n",
        "\n",
        "# --- 3. Inicjalizacja Środowiska ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.create(env_name=env_name, episode_length=500, batch_size=args.num_envs)\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# Set action_dim after environment is created\n",
        "args.action_dim = env.action_size\n",
        "\n",
        "# --- 4. Inicjalizacja Agenta i Urządzenia ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Urządzenie treningowe: {device}\")\n",
        "\n",
        "# Inicjalizacja agenta (używamy klasy z Twojego pliku fixed_ppo_gae)\n",
        "# Upewnij się, że masz zaimportowaną klasę PPO_Vectorized!\n",
        "# agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "\n",
        "# # Inicjalizacja normalizera\n",
        "# obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "\n",
        "# # --- 5. Przygotowanie buforów pamięci (Tensorów) ---\n",
        "# # To jest kluczowe dla szybkości i poprawności GAE.\n",
        "# # Alokujemy pamięć raz na GPU.\n",
        "# steps = args.traj_length\n",
        "# num_envs = args.num_envs\n",
        "# obs_dim = env.observation_size\n",
        "# act_dim = env.action_size\n",
        "\n",
        "# storage_obs = torch.zeros((steps, num_envs, obs_dim), device=device)\n",
        "# storage_next_obs = torch.zeros((steps, num_envs, obs_dim), device=device)\n",
        "# storage_actions = torch.zeros((steps, num_envs, act_dim), device=device)\n",
        "# storage_rewards = torch.zeros((steps, num_envs), device=device)\n",
        "# storage_dones = torch.zeros((steps, num_envs), device=device)\n",
        "# storage_log_probs = torch.zeros((steps, num_envs), device=device)\n",
        "\n",
        "# # --- 6. Główna Pętla Treningowa ---\n",
        "# rng = jax.random.PRNGKey(0)\n",
        "# rng, reset_rng = jax.random.split(rng)\n",
        "# state = jit_reset(reset_rng)\n",
        "\n",
        "# # Logowanie\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "# log_file = open(f\"training_log_{timestamp}.csv\", \"w\", newline=\"\")\n",
        "# writer = csv.writer(log_file)\n",
        "# writer.writerow([\"Update\", \"Avg_Total_Reward\", \"Avg_North_Reward\", \"Avg_Healthy_Reward\", \"Avg_Ctrl_Cost\", \"Avg_Sideways_Cost\", \"Avg_Orientation_Cost\", \"Avg_Z_Angular_Cost\", \"Avg_Episode_Length\", \"PG_Loss\", \"Val_Loss\", \"Ent_Loss\"])\n",
        "\n",
        "# print(\"Rozpoczynam trening...\")\n",
        "\n",
        "# # Added: Initialize episode_step_counts for each parallel environment\n",
        "# episode_step_counts = jnp.zeros(args.num_envs, dtype=jnp.int32)\n",
        "\n",
        "# for update in range(args.num_updates):\n",
        "\n",
        "#     # === FAZA 1: ZBIERANIE DANYCH (ROLLOUT) ===\n",
        "#     total_sum_rewards_collected = 0.0\n",
        "#     total_sum_north_rewards_collected = 0.0\n",
        "#     total_sum_healthy_rewards_collected = 0.0\n",
        "#     total_sum_ctrl_costs_collected = 0.0\n",
        "#     total_sum_sideways_costs_collected = 0.0\n",
        "#     total_sum_orientation_costs_collected = 0.0\n",
        "#     total_sum_z_angular_costs_collected = 0.0\n",
        "#     # Added: Accumulators for average episode length\n",
        "#     total_episode_lengths_collected = 0\n",
        "#     terminated_episodes_count = 0\n",
        "\n",
        "#     for t in range(steps):\n",
        "#         # 1. Pobranie i normalizacja obserwacji (JAX -> Numpy)\n",
        "#         obs_np = np.array(state.obs)\n",
        "#         obs_rms.update(obs_np) # Aktualizacja statystyk\n",
        "#         obs_norm = obs_rms.normalize(obs_np) # Normalizacja\n",
        "\n",
        "#         # 2. Konwersja do Torch\n",
        "#         obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "#         # 3. Wybór akcji przez Agenta\n",
        "#         with torch.no_grad():\n",
        "#             mu, sigma = agent.get_action(obs_torch)\n",
        "#             dist = torch.distributions.Normal(mu, sigma)\n",
        "#             action = dist.sample()\n",
        "#             log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "#         # 4. Krok środowiska (Torch -> JAX)\n",
        "#         action_jax = jax.dlpack.from_dlpack(action)\n",
        "#         rng, step_rng = jax.random.split(rng)\n",
        "#         next_state = jit_step(state, action_jax)\n",
        "\n",
        "#         # 5. Normalizacja 'next_obs' (potrzebne do Value Function target)\n",
        "#         next_obs_np = np.array(next_state.obs)\n",
        "#         next_obs_norm = obs_rms.normalize(next_obs_np)\n",
        "\n",
        "#         # 6. ZAPIS DO BUFORA (Wektoryzowany)\n",
        "#         storage_obs[t] = obs_torch\n",
        "#         storage_next_obs[t] = torch.tensor(next_obs_norm, dtype=torch.float32, device=device)\n",
        "#         storage_actions[t] = action\n",
        "#         storage_rewards[t] = torch.tensor(np.array(next_state.reward), dtype=torch.float32, device=device)\n",
        "#         storage_dones[t] = torch.tensor(np.array(next_state.done), dtype=torch.float32, device=device)\n",
        "#         storage_log_probs[t] = log_prob\n",
        "\n",
        "#         # Added: Update episode_step_counts and accumulate terminated episode lengths\n",
        "#         episode_step_counts = episode_step_counts + 1\n",
        "#         terminated_mask = np.array(next_state.done) # Convert JAX array to numpy for boolean indexing\n",
        "\n",
        "#         terminated_lengths_this_step = episode_step_counts * terminated_mask\n",
        "#         total_episode_lengths_collected += np.sum(terminated_lengths_this_step) # Sum numpy array\n",
        "#         terminated_episodes_count += np.sum(terminated_mask) # Sum numpy array\n",
        "\n",
        "#         episode_step_counts = episode_step_counts * (1 - terminated_mask)\n",
        "\n",
        "#         # Metryki\n",
        "#         total_sum_rewards_collected += np.sum(next_state.reward)\n",
        "#         total_sum_north_rewards_collected += np.sum(next_state.metrics['north_reward'])\n",
        "#         total_sum_healthy_rewards_collected += np.sum(next_state.metrics['reward_alive'])\n",
        "#         total_sum_ctrl_costs_collected += np.sum(-next_state.metrics['reward_quadctrl'])\n",
        "#         total_sum_sideways_costs_collected += np.sum(next_state.metrics['sideways_cost'])\n",
        "#         total_sum_orientation_costs_collected += np.sum(next_state.metrics['orientation_cost'])\n",
        "#         total_sum_z_angular_costs_collected += np.sum(next_state.metrics['z_angular_cost'])\n",
        "\n",
        "#         # Przejście do nowego stanu\n",
        "#         state = next_state\n",
        "\n",
        "#     # === FAZA 2: AKTUALIZACJA SIECI (PPO UPDATE) ===\n",
        "#     # Wywołujemy nową metodę update, która obsługuje tensory (Steps, Envs)\n",
        "#     pg_loss, ent_loss, val_loss = agent.update(\n",
        "#         storage_obs,\n",
        "#         storage_actions,\n",
        "#         storage_log_probs,\n",
        "#         storage_rewards,\n",
        "#         storage_next_obs,\n",
        "#         storage_dones\n",
        "#     )\n",
        "\n",
        "#     # === FAZA 3: LOGOWANIE I ZAPIS ===\n",
        "#     num_collected_transitions = steps * num_envs\n",
        "\n",
        "#     avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "#     avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "#     avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "#     avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "#     avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "#     avg_orientation_cost = total_sum_orientation_costs_collected / num_collected_transitions\n",
        "#     avg_z_angular_cost = total_sum_z_angular_costs_collected / num_collected_transitions\n",
        "\n",
        "#     avg_episode_length = total_episode_lengths_collected / terminated_episodes_count if terminated_episodes_count > 0 else 0.0\n",
        "\n",
        "#     print(f\"Update {update+1}: Avg Total Reward={avg_total_reward:.3f} | PG={pg_loss:.4f} Val={val_loss:.4f} Ent={ent_loss:.4f}\")\n",
        "#     print(f\"  Avg Rewards: North={avg_north_reward:.3f}, Healthy={avg_healthy_reward:.3f}, Ctrl Cost={avg_ctrl_cost:.3f}, Sideways Cost={avg_sideways_cost:.3f}\")\n",
        "#     print(f\"  Avg Added Costs: Orientation={avg_orientation_cost:.3f}, Z Angular={avg_z_angular_cost:.3f}\")\n",
        "#     print(f\"  Avg Episode Length: {avg_episode_length:.2f} (from {terminated_episodes_count} terminations)\")\n",
        "\n",
        "#     writer.writerow([update+1, avg_total_reward, avg_north_reward, avg_healthy_reward, avg_ctrl_cost, avg_sideways_cost, avg_orientation_cost, avg_z_angular_cost, avg_episode_length, pg_loss, val_loss, ent_loss])\n",
        "#     log_file.flush()\n",
        "\n",
        "#     # Checkpointing co 10 update'ów\n",
        "#     if (update + 1) % 10 == 0:\n",
        "#         checkpoint = {\n",
        "#             'model_state_dict': agent.state_dict(),\n",
        "#             'actor_opt': agent.actor_optimizer.state_dict(),\n",
        "#             'critic_opt': agent.critic_optimizer.state_dict(),\n",
        "#             'obs_rms_mean': obs_rms.mean,\n",
        "#             'obs_rms_var': obs_rms.var,\n",
        "#             'obs_rms_count': obs_rms.count\n",
        "#         }\n",
        "#         torch.save(checkpoint, \"ppo_vectorized_checkpoint.pth\")\n",
        "\n",
        "# print(\"Trening zakończony.\")\n",
        "# log_file.close()\n",
        "\n",
        "agent = PPO_Vectorized(device, env.observation_size, env.action_size, args)\n",
        "obs_rms = RunningMeanStd(shape=(env.observation_size,))\n",
        "# Bufory pamięci na GPU\n",
        "steps = args.traj_length\n",
        "num_envs = args.num_envs\n",
        "storage_obs = torch.zeros((steps, num_envs, env.observation_size), device=device)\n",
        "storage_next_obs = torch.zeros((steps, num_envs, env.observation_size), device=device)\n",
        "storage_actions = torch.zeros((steps, num_envs, env.action_size), device=device)\n",
        "storage_rewards = torch.zeros((steps, num_envs), device=device)\n",
        "storage_dones = torch.zeros((steps, num_envs), device=device)\n",
        "storage_log_probs = torch.zeros((steps, num_envs), device=device)\n",
        "\n",
        "# Start środowiska\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rng, reset_rng = jax.random.split(rng)\n",
        "state = jit_reset(reset_rng)\n",
        "\n",
        "# Logowanie\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_file = open(f\"spider_training_{timestamp}.csv\", \"w\", newline=\"\")\n",
        "writer = csv.writer(log_file)\n",
        "# Zaktualizowane nagłówki CSV pasujące do nowego środowiska\n",
        "writer.writerow([\"Update\", \"Avg_Total_Reward\", \"Avg_Forward_Reward\", \"Avg_Healthy_Reward\", \"Avg_Ctrl_Cost\", \"Avg_X_Velocity\", \"Avg_Episode_Length\", \"PG_Loss\", \"Val_Loss\"])\n",
        "\n",
        "episode_step_counts = jnp.zeros(args.num_envs, dtype=jnp.int32)\n",
        "print(\"Rozpoczynam trening Pająka...\")\n",
        "\n",
        "checkpoint_path = \"ppo_spider_full_checkpoint_1900.pth\"  # Podaj nazwę swojego pliku\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"--- Wczytywanie checkpointu: {checkpoint_path} ---\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # 1. Wczytanie wag modelu\n",
        "    agent.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # 2. Wczytanie stanu optymalizatorów\n",
        "    agent.actor_optimizer.load_state_dict(checkpoint['optimizer_actor_state_dict'])\n",
        "    agent.critic_optimizer.load_state_dict(checkpoint['optimizer_critic_state_dict'])\n",
        "\n",
        "    # 3. Wczytanie statystyk normalizacji (KLUCZOWE)\n",
        "    obs_rms.mean = checkpoint['obs_rms_mean']\n",
        "    obs_rms.var = checkpoint['obs_rms_var']\n",
        "    obs_rms.count = checkpoint['obs_rms_count']\n",
        "\n",
        "    # 4. Ustawienie punktu startowego pętli\n",
        "    start_update = checkpoint['update_idx']\n",
        "    print(f\"Wznowienie treningu od aktualizacji numer: {start_update + 1}\")\n",
        "else:\n",
        "    print(\"Nie znaleziono checkpointu. Rozpoczynanie nowego treningu.\")\n",
        "    start_update = 0\n",
        "\n",
        "for update in range(args.num_updates):\n",
        "    # --- Zbieranie metryk ---\n",
        "    total_rewards = 0.0\n",
        "    total_forward_reward = 0.0\n",
        "    total_healthy_reward = 0.0\n",
        "    total_ctrl_cost = 0.0\n",
        "    total_x_vel = 0.0\n",
        "\n",
        "    collected_len = 0\n",
        "    terminated_count = 0\n",
        "\n",
        "    # === ROLLOUT ===\n",
        "    for t in range(steps):\n",
        "        # 1. Normalizacja obserwacji (Update + Normalize)\n",
        "        obs_np = np.array(state.obs)\n",
        "        obs_rms.update(obs_np) # Aktualizujemy wiedzę o świecie\n",
        "        obs_norm = obs_rms.normalize(obs_np)\n",
        "\n",
        "        obs_torch = torch.tensor(obs_norm, dtype=torch.float32, device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = agent.get_action(obs_torch)\n",
        "            dist = torch.distributions.Normal(mu, sigma)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "        action_jax = jax.dlpack.from_dlpack(action)\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_state = jit_step(state, action_jax)\n",
        "\n",
        "        # 2. Normalizacja next_obs (do bufora)\n",
        "        next_obs_norm = obs_rms.normalize(np.array(next_state.obs))\n",
        "\n",
        "        # 3. Zapis do bufora\n",
        "        storage_obs[t] = obs_torch\n",
        "        storage_next_obs[t] = torch.tensor(next_obs_norm, dtype=torch.float32, device=device)\n",
        "        storage_actions[t] = action\n",
        "        storage_rewards[t] = torch.tensor(np.array(next_state.reward), dtype=torch.float32, device=device)\n",
        "        storage_dones[t] = torch.tensor(np.array(next_state.done), dtype=torch.float32, device=device)\n",
        "        storage_log_probs[t] = log_prob\n",
        "\n",
        "        # Agregacja metryk\n",
        "        episode_step_counts += 1\n",
        "        done_mask_np = np.array(next_state.done)\n",
        "        collected_len += np.sum(episode_step_counts * done_mask_np)\n",
        "        terminated_count += np.sum(done_mask_np)\n",
        "        episode_step_counts *= (1 - done_mask_np)\n",
        "\n",
        "        total_rewards += np.sum(next_state.reward)\n",
        "        total_forward_reward += np.sum(next_state.metrics['forward_reward'])\n",
        "        total_healthy_reward += np.sum(next_state.metrics['reward_alive'])\n",
        "        total_ctrl_cost += np.sum(-next_state.metrics['reward_quadctrl'])\n",
        "        total_x_vel += np.sum(next_state.metrics['x_velocity'])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # === UPDATE ===\n",
        "    pg_loss, ent_loss, val_loss = agent.update(\n",
        "        storage_obs, storage_actions, storage_log_probs, storage_rewards, storage_next_obs, storage_dones\n",
        "    )\n",
        "\n",
        "    # === LOGOWANIE ===\n",
        "    denom = steps * num_envs\n",
        "    avg_ep_len = collected_len / terminated_count if terminated_count > 0 else 0.0\n",
        "\n",
        "    print(f\"Update {update+1}/{args.num_updates}: Reward={total_rewards/denom:.3f} | Len={avg_ep_len:.1f} | X-Vel={total_x_vel/denom:.2f} | Losses: PG={pg_loss:.3f} V={val_loss:.3f}\")\n",
        "\n",
        "    writer.writerow([\n",
        "        update+1,\n",
        "        total_rewards/denom,\n",
        "        total_forward_reward/denom,\n",
        "        total_healthy_reward/denom,\n",
        "        total_ctrl_cost/denom,\n",
        "        total_x_vel/denom,\n",
        "        avg_ep_len,\n",
        "        pg_loss,\n",
        "        val_loss\n",
        "    ])\n",
        "    log_file.flush()\n",
        "\n",
        "    # === PEŁNY ZAPIS CHECKPOINTU (NAPRAWIONE) ===\n",
        "    if (update + 1) % 50 == 0:\n",
        "        filename = f\"ppo_spider_full_checkpoint_{update+1}.pth\"\n",
        "\n",
        "        full_checkpoint = {\n",
        "            'update_idx': update + 1,\n",
        "            'model_state_dict': agent.state_dict(),\n",
        "            'optimizer_actor_state_dict': agent.actor_optimizer.state_dict(),\n",
        "            'optimizer_critic_state_dict': agent.critic_optimizer.state_dict(),\n",
        "            # ZAPISUJEMY STATYSTYKI NORMALIZACJI - TO JEST KLUCZOWE\n",
        "            'obs_rms_mean': obs_rms.mean,\n",
        "            'obs_rms_var': obs_rms.var,\n",
        "            'obs_rms_count': obs_rms.count\n",
        "        }\n",
        "\n",
        "        torch.save(full_checkpoint, filename)\n",
        "        print(f\"💾 Zapisano pełny checkpoint: {filename}\")\n",
        "\n",
        "log_file.close()\n",
        "print(\"Koniec treningu.\")"
      ],
      "metadata": {
        "id": "uoQxu2nlKs0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fr-64HbYjTji"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}