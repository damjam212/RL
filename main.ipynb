{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mujoco Cpider Notebook contains:\n",
        "  1. Instalation Section\n",
        "  2. Env definition with rollout\n",
        "  3. Training with rollout"
      ],
      "metadata": {
        "id": "ksFfWbeuQrGk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['MUJOCO_GL'] = 'egl' # Ensure EGL rendering is used\n",
        "\n",
        "from datetime import datetime\n",
        "from etils import epath\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "import os\n",
        "from ml_collections import config_dict\n",
        "\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from flax.training import orbax_utils\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from orbax import checkpoint as ocp\n",
        "\n",
        "import mujoco\n",
        "from mujoco import mjx\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.base import State as PipelineState\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv6WUVUm78k"
      },
      "source": [
        "# Simple ENV with spider\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XML spider defintion"
      ],
      "metadata": {
        "id": "WDfVKuffvTDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spider_xml = \"\"\"\n",
        "<mujoco model=\"ant\">\n",
        "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
        "  <option integrator=\"RK4\" timestep=\"0.01\"/>\n",
        "  <custom>\n",
        "    <numeric data=\"0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0\" name=\"init_qpos\"/>\n",
        "  </custom>\n",
        "  <default>\n",
        "    <joint armature=\"1\" damping=\"1\" limited=\"true\"/>\n",
        "    <geom conaffinity=\"0\" condim=\"3\" density=\"5.0\" friction=\"1 0.5 0.5\" margin=\"0.01\" rgba=\"0.8 0.6 0.4 1\"/>\n",
        "  </default>\n",
        "  <asset>\n",
        "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
        "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
        "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
        "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
        "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
        "  </asset>\n",
        "  <worldbody>\n",
        "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
        "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 0.75\">\n",
        "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
        "      <geom name=\"torso_geom\" pos=\"0 0 0\" size=\"0.25\" type=\"sphere\"/>\n",
        "      <joint armature=\"0\" damping=\"0\" limited=\"false\" margin=\"0.01\" name=\"root\" pos=\"0 0 0\" type=\"free\"/>\n",
        "      <body name=\"front_left_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"aux_1_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_1\" pos=\"0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_1\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 0.2 0.0\" name=\"left_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_1\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 0.4 0.0\" name=\"left_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"front_right_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"aux_2_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_2\" pos=\"-0.2 0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_2\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 0.2 0.0\" name=\"right_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_2\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 0.4 0.0\" name=\"right_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"aux_3_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_3\" pos=\"-0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_3\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 -0.2 -0.2 0.0\" name=\"back_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"-0.2 -0.2 0\">\n",
        "            <joint axis=\"-1 1 0\" name=\"ankle_3\" pos=\"0.0 0.0 0.0\" range=\"-70 -30\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 -0.4 -0.4 0.0\" name=\"third_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"right_back_leg\" pos=\"0 0 0\">\n",
        "        <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"aux_4_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "        <body name=\"aux_4\" pos=\"0.2 -0.2 0\">\n",
        "          <joint axis=\"0 0 1\" name=\"hip_4\" pos=\"0.0 0.0 0.0\" range=\"-30 30\" type=\"hinge\"/>\n",
        "          <geom fromto=\"0.0 0.0 0.0 0.2 -0.2 0.0\" name=\"rightback_leg_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          <body pos=\"0.2 -0.2 0\">\n",
        "            <joint axis=\"1 1 0\" name=\"ankle_4\" pos=\"0.0 0.0 0.0\" range=\"30 70\" type=\"hinge\"/>\n",
        "            <geom fromto=\"0.0 0.0 0.0 0.4 -0.4 0.0\" name=\"fourth_ankle_geom\" size=\"0.08\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_4\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_1\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_2\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"hip_3\" gear=\"150\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\" joint=\"ankle_3\" gear=\"150\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sj_Kkg0Uqx9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spider Env"
      ],
      "metadata": {
        "id": "6ZbTRbgovaJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtGMYNLE3QJN"
      },
      "outputs": [],
      "source": [
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.5, # Zmniejszone (było 10.0)\n",
        "      north_reward_weight=10.0,   # Zmniejszone (było 10.0)\n",
        "      sideways_cost_weight=0.1,  # Zwiększone\n",
        "      ctrl_cost_weight=0.001,      # Zwiększone\n",
        "      healthy_reward=5.0,        # Zwiększone\n",
        "      terminate_when_unhealthy=True, # Zmienione na True (KLUCZOWE!)\n",
        "      orientation_cost_weight=1.0, # Kara za odchylenie od pionu (roll/pitch)\n",
        "      z_angular_velocity_cost_weight=0.1, # Kara za wirowanie tułowia (yaw rate)\n",
        "      healthy_z_range=(0.3, 1.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      episode_length: int = 1000,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    mj_model = mujoco.MjModel.from_xml_string(spider_xml)\n",
        "    mj_data = mujoco.MjData(mj_model)\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    self.episode_length = episode_length\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._north_reward_weight = north_reward_weight\n",
        "    self._sideways_cost_weight = sideways_cost_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._orientation_cost_weight = orientation_cost_weight\n",
        "    self._z_angular_velocity_cost_weight = z_angular_velocity_cost_weight\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "    self._torso_body_idx = mujoco.mj_name2id(\n",
        "        self.sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    self._mj_data = mj_data\n",
        "\n",
        "  # --- TUTAJ BYŁ PRAWDOPODOBNIE BŁĄD WCIĘCIA ---\n",
        "  # Metoda reset musi być na tym samym poziomie wcięcia co __init__\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = jp.asarray(self._mj_data.qpos) + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jp.asarray(self._mj_data.qvel) + jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "        'north_reward': zero,\n",
        "        'sideways_cost': zero,\n",
        "        'orientation_cost': zero, # Dodane\n",
        "        'z_angular_cost': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "    orientation_cost = self._orientation_cost_weight * jp.sum(jp.square(data.q[4:6]))\n",
        "    # ^ Używamy q[4] i q[5], które odpowiadają za odchylenia 'x' i 'y' kwaternionu (roll i pitch)\n",
        "\n",
        "    # 2. KOSZT PRĘDKOŚCI KĄTOWEJ Z (kara za wirowanie tułowia)\n",
        "    # Prędkość kątowa tułowia (wokół osi Z) znajduje się w cvel[idx, 5].\n",
        "    z_angular_velocity = data.cvel[self._torso_body_idx, 5]\n",
        "    z_angular_cost = self._z_angular_velocity_cost_weight * jp.square(z_angular_velocity)\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    # Calculate reward for moving in the 'north' direction (positive y-axis)\n",
        "    torso_y_velocity = data.cvel[self._torso_body_idx, 1]\n",
        "    north_reward = self._north_reward_weight * torso_y_velocity\n",
        "\n",
        "    # Calculate cost for sideways movement (x-axis)\n",
        "    torso_x_velocity = data.cvel[self._torso_body_idx, 0]\n",
        "    sideways_cost = self._sideways_cost_weight * jp.abs(torso_x_velocity)\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "\n",
        "    # Healthy reward logic\n",
        "    healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    # Done logic (CRITICAL FIX)\n",
        "    if self._terminate_when_unhealthy:\n",
        "        done = 1.0 - is_healthy\n",
        "    else:\n",
        "        done = 0.0\n",
        "\n",
        "    # Reward scaling (divided by 100.0 as discussed to stabilize PPO)\n",
        "    raw_reward = north_reward + healthy_reward - ctrl_cost - sideways_cost - orientation_cost - z_angular_cost # Nowy koszt\n",
        "    # Opcjonalnie: skalowanie tutaj, lub w configu PPO.\n",
        "    # Na razie zostawiamy surowe, bo zmieniłeś wagi na mniejsze (1.5 zamiast 10).\n",
        "    reward = raw_reward\n",
        "\n",
        "    obs = self._get_obs(data, action)\n",
        "\n",
        "    state.metrics.update(\n",
        "        forward_reward=north_reward,\n",
        "        reward_linvel=north_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=data.xpos[self._torso_body_idx, 0],\n",
        "        y_position=data.xpos[self._torso_body_idx, 1],\n",
        "        distance_from_origin=jp.linalg.norm(data.xpos[self._torso_body_idx, :2]),\n",
        "        x_velocity=torso_x_velocity,\n",
        "        y_velocity=torso_y_velocity,\n",
        "        north_reward=north_reward,\n",
        "        sideways_cost=sideways_cost,\n",
        "        orientation_cost=orientation_cost, # Dodane\n",
        "        z_angular_cost=z_angular_cost,\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "    return jp.concatenate([\n",
        "        data.qpos[2:3],  # Torso z-position\n",
        "        data.cvel[self._torso_body_idx, 0:1], # Torso x-velocity\n",
        "        data.cvel[self._torso_body_idx, 1:2], # Torso y-velocity\n",
        "        data.qpos[7:], # Joint positions\n",
        "        data.qvel[6:], # Joint velocities\n",
        "    ])\n",
        "\n",
        "# Re-register environment\n",
        "envs.register_environment('humanoid', Humanoid)\n",
        "\n",
        "\n",
        "# Add code to get and print the observation shape\n",
        "env_test = Humanoid()\n",
        "dummy_data = mujoco.MjData(env_test.sys.mj_model) # Use mj_data\n",
        "dummy_obs = env_test._get_obs(env_test.pipeline_init(jp.asarray(dummy_data.qpos), jp.asarray(dummy_data.qvel)), jp.zeros(env_test.sys.nu))\n",
        "print(\"Observation shape:\", dummy_obs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(Optional) Show Data class for inspection"
      ],
      "metadata": {
        "id": "zd-wW1ANxPcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import jax\n",
        "# import mujoco\n",
        "# from brax import envs\n",
        "# import numpy as np\n",
        "\n",
        "# # Instantiate the environment\n",
        "# env_name = 'humanoid'\n",
        "# env = envs.get_environment(env_name)\n",
        "\n",
        "# # Define the jit reset function\n",
        "# jit_reset = jax.jit(env.reset)\n",
        "\n",
        "# # Reset the environment to get an initial state\n",
        "# rng = jax.random.PRNGKey(0)\n",
        "# state = jit_reset(rng)\n",
        "\n",
        "# # The mjx.Data object is stored in state.pipeline_state\n",
        "# data = state.pipeline_state\n",
        "\n",
        "# print(\"--- Contents of mjx.Data object ---\")\n",
        "# print(f\"Type of data: {type(data)}\\n\")\n",
        "\n",
        "# print(\"--- All attributes of mjx.Data object ---\\n\")\n",
        "# for attr_name in sorted(dir(data)):\n",
        "#     if not attr_name.startswith('_'): # Exclude private attributes\n",
        "#         attr_value = getattr(data, attr_name)\n",
        "#         attr_type = type(attr_value)\n",
        "#         if isinstance(attr_value, (np.ndarray, jax.Array)): # Check if it's a JAX or NumPy array\n",
        "#             print(f\"  Attribute: {attr_name}, Type: {attr_type}, Shape: {attr_value.shape}\")\n",
        "#         else:\n",
        "#             print(f\"  Attribute: {attr_name}, Type: {attr_type}\")\n",
        "\n",
        "# print(\"\\n--- Specific attributes shown before ---\")\n",
        "# print(\"1. Generalized positions (qpos):\")\n",
        "# print(f\"  Shape: {data.qpos.shape}\")\n",
        "# print(f\"  Value (first 10 elements): {data.qpos[:10]}\\n\")\n",
        "\n",
        "# print(\"2. Generalized velocities (qvel):\")\n",
        "# print(f\"  Shape: {data.qvel.shape}\")\n",
        "# print(f\"  Value (first 10 elements): {data.qvel[:10]}\\n\")\n",
        "\n",
        "# print(\"3. Cartesian position of bodies (xpos):\")\n",
        "# print(f\"  Shape: {data.xpos.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.xpos[:5]}\\n\")\n",
        "\n",
        "# print(\"4. Cartesian orientation of bodies (xquat):\")\n",
        "# print(f\"  Shape: {data.xquat.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.xquat[:5]}\\n\")\n",
        "\n",
        "# print(\"5. Center of mass velocity (cvel):\")\n",
        "# print(f\"  Shape: {data.cvel.shape}\")\n",
        "# print(f\"  Value (first 5 bodies):\\n{data.cvel[:5]}\\n\")\n",
        "\n",
        "# print(\"6. Body indices (for reference, not part of data object directly but useful):\")\n",
        "# print(f\"  Torso body index: {env._torso_body_idx}\")"
      ],
      "metadata": {
        "id": "ikGX1ZRMv9t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Inspect action"
      ],
      "metadata": {
        "id": "D3icJ4W30vrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import jax\n",
        "# from brax import envs\n",
        "# import jax.numpy as jp\n",
        "# from brax.training.agents.ppo import networks as ppo_networks\n",
        "# from brax.training.agents.ppo import train as ppo\n",
        "# from brax.io import model\n",
        "\n",
        "# # Re-instantiate the environment if not already available in this scope\n",
        "# env_name = 'humanoid'\n",
        "# env = envs.get_environment(env_name)\n",
        "\n",
        "# # Define and train a minimal policy if jit_inference_fn is not defined\n",
        "# try:\n",
        "#     # Attempt to use jit_inference_fn if already defined (e.g., from a previous run of training cells)\n",
        "#     _ = jit_inference_fn\n",
        "# except NameError:\n",
        "#     print(\"jit_inference_fn not found, training a minimal policy...\")\n",
        "#     make_inference_fn, params, _ = ppo.train(\n",
        "#         environment=env, num_timesteps=5_000, num_evals=1, episode_length=env.episode_length # Minimal training just for this demo\n",
        "#     )\n",
        "#     model_path = '/tmp/mjx_brax_policy'\n",
        "#     model.save_params(model_path, params)\n",
        "#     params = model.load_params(model_path)\n",
        "#     inference_fn = make_inference_fn(params)\n",
        "#     jit_inference_fn = jax.jit(inference_fn)\n",
        "#     print(\"Minimal policy trained and jit_inference_fn defined.\")\n",
        "\n",
        "# # Initialize the state\n",
        "# current_rng = jax.random.PRNGKey(1) # Use a new RNG key\n",
        "# state = jax.jit(env.reset)(current_rng)\n",
        "\n",
        "# # Generate an action using the inference function\n",
        "# act_rng, current_rng = jax.random.split(current_rng)\n",
        "# ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "\n",
        "# print(\"--- Content of the 'action' variable (ctrl from policy) ---\")\n",
        "# print(f\"Type of action: {type(ctrl)}\")\n",
        "# print(f\"Shape of action: {ctrl.shape}\")\n",
        "# print(f\"Value of action: {ctrl}\")\n"
      ],
      "metadata": {
        "id": "wx_QtlDdzlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K6IznI2y83"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout.\n",
        "\n",
        "NOTE: Since episodes terminate early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKLFK54C1CH"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph8u-v2Q2xLS"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(100):\n",
        "  ctrl = jp.zeros(env.sys.nu) # Set control input to zero for standing still\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "media.show_video(env.render(rollout), fps=1.0 / env.dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O0zQEttGONfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observation space:\", env.observation_size)"
      ],
      "metadata": {
        "id": "42pERvKYOOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO - github implementation"
      ],
      "metadata": {
        "id": "2xIGm9hndmKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class NetworkBase(nn.Module, metaclass=ABCMeta):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        super(NetworkBase, self).__init__()\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class Network(NetworkBase):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.relu,last_activation = None):\n",
        "        super(Network, self).__init__()\n",
        "        self.activation = activation_function\n",
        "        self.last_activation = last_activation\n",
        "        layers_unit = [input_dim]+ [hidden_dim]*(layer_num-1)\n",
        "        layers = ([nn.Linear(layers_unit[idx],layers_unit[idx+1]) for idx in range(len(layers_unit)-1)])\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.last_layer = nn.Linear(layers_unit[-1],output_dim)\n",
        "        self.network_init()\n",
        "    def forward(self, x):\n",
        "        return self._forward(x)\n",
        "    def _forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.last_layer(x)\n",
        "        if self.last_activation != None:\n",
        "            x = self.last_activation(x)\n",
        "        return x\n",
        "    def network_init(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.orthogonal_(layer.weight)\n",
        "                layer.bias.data.zero_()"
      ],
      "metadata": {
        "id": "SSTtLSBWdyj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Actor(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function = torch.tanh,last_activation = None, trainable_std = False):\n",
        "        super(Actor, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "        self.trainable_std = trainable_std\n",
        "        if self.trainable_std == True:\n",
        "            self.logstd = nn.Parameter(torch.zeros(1, output_dim))\n",
        "    def forward(self, x):\n",
        "        mu = self._forward(x)\n",
        "        if self.trainable_std == True:\n",
        "            std = torch.exp(self.logstd)\n",
        "        else:\n",
        "            logstd = torch.zeros_like(mu)\n",
        "            std = torch.exp(logstd)\n",
        "        return mu,std\n",
        "\n",
        "class Critic(Network):\n",
        "    def __init__(self, layer_num, input_dim, output_dim, hidden_dim, activation_function, last_activation = None):\n",
        "        super(Critic, self).__init__(layer_num, input_dim, output_dim, hidden_dim, activation_function ,last_activation)\n",
        "\n",
        "    def forward(self, *x):\n",
        "        x = torch.cat(x,-1)\n",
        "        return self._forward(x)\n"
      ],
      "metadata": {
        "id": "-IOjyENUdzbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Dict(dict):\n",
        "    def __init__(self,config,section_name,location = False):\n",
        "        super(Dict,self).__init__()\n",
        "        self.initialize(config, section_name,location)\n",
        "    def initialize(self, config, section_name,location):\n",
        "        for key,value in config.items(section_name):\n",
        "            if location :\n",
        "                self[key] = value\n",
        "            else:\n",
        "                self[key] = eval(value)\n",
        "    def __getattr__(self,val):\n",
        "        return self[val]\n",
        "\n",
        "def make_transition(state,action,reward,next_state,done,log_prob=None):\n",
        "    transition = {}\n",
        "    transition['state'] = state\n",
        "    transition['action'] = action\n",
        "    transition['reward'] = reward\n",
        "    transition['next_state'] = next_state\n",
        "    transition['log_prob'] = log_prob\n",
        "    transition['done'] = done\n",
        "    return transition\n",
        "\n",
        "def make_mini_batch(*value):\n",
        "    mini_batch_size = value[0]\n",
        "    full_batch_size = len(value[1])\n",
        "    full_indices = np.arange(full_batch_size)\n",
        "    np.random.shuffle(full_indices)\n",
        "    for i in range(full_batch_size // mini_batch_size):\n",
        "        indices = full_indices[mini_batch_size*i : mini_batch_size*(i+1)]\n",
        "        yield [x[indices] for x in value[1:]]\n",
        "\n",
        "def convert_to_tensor(*value):\n",
        "    device = value[0]\n",
        "    return [torch.tensor(x).float().to(device) for x in value[1:]]\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, action_prob_exist, max_size, state_dim, num_action):\n",
        "        self.max_size = max_size\n",
        "        self.data_idx = 0\n",
        "        self.action_prob_exist = action_prob_exist\n",
        "        self.data = {}\n",
        "\n",
        "        self.data['state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['action'] = np.zeros((self.max_size, num_action))\n",
        "        self.data['reward'] = np.zeros((self.max_size, 1))\n",
        "        self.data['next_state'] = np.zeros((self.max_size, state_dim))\n",
        "        self.data['done'] = np.zeros((self.max_size, 1))\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'] = np.zeros((self.max_size, 1))\n",
        "    def put_data(self, transition):\n",
        "        idx = self.data_idx % self.max_size\n",
        "        self.data['state'][idx] = transition['state']\n",
        "        self.data['action'][idx] = transition['action']\n",
        "        self.data['reward'][idx] = transition['reward']\n",
        "        self.data['next_state'][idx] = transition['next_state']\n",
        "        self.data['done'][idx] = float(transition['done'])\n",
        "        if self.action_prob_exist :\n",
        "            self.data['log_prob'][idx] = transition['log_prob']\n",
        "\n",
        "        self.data_idx += 1\n",
        "    def sample(self, shuffle, batch_size = None):\n",
        "        if shuffle :\n",
        "            sample_num = min(self.max_size, self.data_idx)\n",
        "            rand_idx = np.random.choice(sample_num, batch_size,replace=False)\n",
        "            sampled_data = {}\n",
        "            sampled_data['state'] = self.data['state'][rand_idx]\n",
        "            sampled_data['action'] = self.data['action'][rand_idx]\n",
        "            sampled_data['reward'] = self.data['reward'][rand_idx]\n",
        "            sampled_data['next_state'] = self.data['next_state'][rand_idx]\n",
        "            sampled_data['done'] = self.data['done'][rand_idx]\n",
        "            if self.action_prob_exist :\n",
        "                sampled_data['log_prob'] = self.data['log_prob'][rand_idx]\n",
        "            return sampled_data\n",
        "        else:\n",
        "            return self.data\n",
        "    def size(self):\n",
        "        return min(self.max_size, self.data_idx)\n",
        "class RunningMeanStd(object):\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count"
      ],
      "metadata": {
        "id": "nUhmYZbwd4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                actor_loss = (-torch.min(surr1, surr2) - entropy).mean()\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n"
      ],
      "metadata": {
        "id": "SYyu-Ijsdp_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO - training loop"
      ],
      "metadata": {
        "id": "xF8gG1nKe0nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 2048 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 3e-4\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 64 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name) # Removed episode_length=1000\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "# num_total_steps_pytorch = 1_000_000 # Total environment steps for PyTorch PPO\n",
        "current_total_steps = 0\n",
        "episode_count = 0\n",
        "rng = jax.random.PRNGKey(0) # JAX RNG for environment\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Change the while loop condition to run for 10 episodes\n",
        "while episode_count < 300:\n",
        "    episode_count += 1\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    episode_reward = 0\n",
        "    episode_north_reward = 0\n",
        "    episode_healthy_reward = 0\n",
        "    episode_ctrl_cost = 0\n",
        "    episode_sideways_cost = 0\n",
        "\n",
        "    # Clear replay buffer for new trajectory collection (on-policy PPO)\n",
        "    ppo_torch_agent.data = ReplayBuffer(\n",
        "        action_prob_exist=True,\n",
        "        max_size=ppo_torch_args.traj_length,\n",
        "        state_dim=env.observation_size,\n",
        "        num_action=env.action_size\n",
        "    )\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length): # Collect for traj_length steps\n",
        "        # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "        obs_torch = torch.from_numpy(np.array(env_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "        # Get action from PyTorch actor\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "        action_jax = jnp.asarray(action_torch.squeeze(0).cpu().numpy())\n",
        "\n",
        "        # Step JAX environment\n",
        "        rng, step_rng = jax.random.split(rng) # Need a new rng for each step if needed by brax (jit_step doesn't take it)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (remove batch dimension where applicable)\n",
        "        obs_np = obs_torch.squeeze(0).cpu().numpy()\n",
        "        action_np = action_torch.squeeze(0).cpu().numpy()\n",
        "        reward_np = np.array(next_env_state.reward).reshape(1) # Ensure (1,) shape\n",
        "        next_obs_np = np.array(next_env_state.obs)\n",
        "        done_np = np.array(next_env_state.done).reshape(1)     # Ensure (1,) shape\n",
        "        log_prob_np = log_prob_torch.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Store transition in PyTorch ReplayBuffer\n",
        "        transition = make_transition(\n",
        "            obs_np,\n",
        "            action_np,\n",
        "            reward_np,\n",
        "            next_obs_np,\n",
        "            done_np,\n",
        "            log_prob_np\n",
        "        )\n",
        "        ppo_torch_agent.put_data(transition)\n",
        "\n",
        "        episode_reward += next_env_state.reward\n",
        "        episode_north_reward += next_env_state.metrics['north_reward']\n",
        "        episode_healthy_reward += next_env_state.metrics['reward_alive']\n",
        "        episode_ctrl_cost += -next_env_state.metrics['reward_quadctrl'] # ctrl_cost is stored as negative in metrics\n",
        "        episode_sideways_cost += next_env_state.metrics['sideways_cost']\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += 1\n",
        "\n",
        "        if env_state.done:\n",
        "            print(f\"Episode {episode_count} finished early at step {t+1}. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "            print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "            break\n",
        "    else: # If loop completes without break\n",
        "      print(f\"Episode {episode_count} completed {ppo_torch_args.traj_length} steps. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "      print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length steps\n",
        "    if ppo_torch_agent.data.size() >= ppo_torch_args.traj_length:\n",
        "        ppo_torch_agent.train_net(episode_count)\n",
        "        print(f\"PPO agent trained for episode {episode_count}.\")\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "metadata": {
        "id": "uvqNqIbid8KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "        # --- DEBUG: Print action_torch_eval shape ---\n",
        "        print(f\"DEBUG: action_torch_eval shape: {action_torch_eval.shape}\")\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "    # --- DEBUG: Print action_jax_eval shape ---\n",
        "    print(f\"DEBUG: action_jax_eval shape (after squeeze): {action_jax_eval.shape}\")\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)"
      ],
      "metadata": {
        "id": "SYnfhpblsmVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MqWRiY9jbgDI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ba071c1"
      },
      "source": [
        "# Task\n",
        "I will proceed with the following steps:\n",
        "\n",
        "1.  **Modify `PPO.train_net` (cell `SYyu-Ijsdp_q`)**:\n",
        "    *   Separate the `actor_loss` into `policy_loss_raw` (policy gradient part) and `entropy_loss_term` (entropy regularization part).\n",
        "    *   Ensure `critic_loss` (value loss part) is correctly captured.\n",
        "    *   Modify the `train_net` method to return these three average loss values: `policy_loss_raw.item()`, `entropy_loss_term.item()`, and `critic_loss.item()`.\n",
        "\n",
        "2.  **Modify the training loop (cell `uvqNqIbid8KS`)**:\n",
        "    *   Add `import csv` and `from datetime import datetime`.\n",
        "    *   Generate a unique CSV filename using the current timestamp.\n",
        "    *   Open the CSV file and write the header row, including: `episode`, `total_reward`, `north_reward`, `healthy_reward`, `ctrl_cost`, `sideways_cost`, `policy_gradient_loss`, `entropy_loss`, `value_loss`, and `total_steps`.\n",
        "    *   In the training loop, after `ppo_torch_agent.train_net` is called, capture the returned `policy_loss`, `entropy_loss`, and `value_loss`.\n",
        "    *   Append a new row to the CSV file with the `episode_count`, `episode_reward`, component rewards, the captured detailed loss values, and `current_total_steps`.\n",
        "    *   Make sure to handle the `ctrl_cost` correctly, as it's stored as negative in metrics.\n",
        "\n",
        "This will ensure the detailed loss components are captured and logged for each training episode.\n",
        "\n",
        "```python\n",
        "# cell_id: SYyu-Ijsdp_q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                \n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean()\n",
        "                entropy_loss_term = (-entropy).mean()\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Collect losses for averaging\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "        \n",
        "        # Calculate average losses over all mini-batches and epochs\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch) if policy_losses_raw_epoch else 0\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch) if entropy_losses_term_epoch else 0\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch) if critic_losses_epoch else 0\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n",
        "\n",
        "```\n",
        "\n",
        "```python\n",
        "# cell_id: uvqNqIbid8KS\n",
        "\n",
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 2048 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 3e-4\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 64 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.max_grad_norm = 0.5\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name) # Removed episode_length=1000\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- CSV Logging Setup ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Define header for CSV\n",
        "header = [\n",
        "    'episode', 'total_reward', 'north_reward', 'healthy_reward',\n",
        "    'ctrl_cost', 'sideways_cost', 'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging training data to {csv_filename}\")\n",
        "# --- End CSV Logging Setup ---\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "# num_total_steps_pytorch = 1_000_000 # Total environment steps for PyTorch PPO\n",
        "current_total_steps = 0\n",
        "episode_count = 0\n",
        "rng = jax.random.PRNGKey(0) # JAX RNG for environment\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Change the while loop condition to run for 10 episodes\n",
        "while episode_count < 300:\n",
        "    episode_count += 1\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    episode_reward = 0\n",
        "    episode_north_reward = 0\n",
        "    episode_healthy_reward = 0\n",
        "    episode_ctrl_cost = 0\n",
        "    episode_sideways_cost = 0\n",
        "\n",
        "    # Clear replay buffer for new trajectory collection (on-policy PPO)\n",
        "    ppo_torch_agent.data = ReplayBuffer(\n",
        "        action_prob_exist=True,\n",
        "        max_size=ppo_torch_args.traj_length,\n",
        "        state_dim=env.observation_size,\n",
        "        num_action=env.action_size\n",
        "    )\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length): # Collect for traj_length steps\n",
        "        # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "        obs_torch = torch.from_numpy(np.array(env_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "        # Get action from PyTorch actor\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "        action_jax = jnp.asarray(action_torch.squeeze(0).cpu().numpy())\n",
        "\n",
        "        # Step JAX environment\n",
        "        rng, step_rng = jax.random.split(rng) # Need a new rng for each step if needed by brax (jit_step doesn't take it)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (remove batch dimension where applicable)\n",
        "        obs_np = obs_torch.squeeze(0).cpu().numpy()\n",
        "        action_np = action_torch.squeeze(0).cpu().numpy()\n",
        "        reward_np = np.array(next_env_state.reward).reshape(1) # Ensure (1,) shape\n",
        "        next_obs_np = np.array(next_env_state.obs)\n",
        "        done_np = np.array(next_env_state.done).reshape(1)     # Ensure (1,) shape\n",
        "        log_prob_np = log_prob_torch.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Store transition in PyTorch ReplayBuffer\n",
        "        transition = make_transition(\n",
        "            obs_np,\n",
        "            action_np,\n",
        "            reward_np,\n",
        "            next_obs_np,\n",
        "            done_np,\n",
        "            log_prob_np\n",
        "        )\n",
        "        ppo_torch_agent.put_data(transition)\n",
        "\n",
        "        episode_reward += next_env_state.reward\n",
        "        episode_north_reward += next_env_state.metrics['north_reward']\n",
        "        episode_healthy_reward += next_env_state.metrics['reward_alive']\n",
        "        episode_ctrl_cost += -next_env_state.metrics['reward_quadctrl'] # ctrl_cost is stored as negative in metrics\n",
        "        episode_sideways_cost += next_env_state.metrics['sideways_cost']\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += 1\n",
        "\n",
        "        if env_state.done:\n",
        "            print(f\"Episode {episode_count} finished early at step {t+1}. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "            print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "            break\n",
        "    else: # If loop completes without break\n",
        "      print(f\"Episode {episode_count} completed {ppo_torch_args.traj_length} steps. Total Reward: {episode_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "      print(f\"  Reward Components: North Reward: {episode_north_reward:.2f}, Healthy Reward: {episode_healthy_reward:.2f}, Ctrl Cost: {episode_ctrl_cost:.2f}, Sideways Cost: {episode_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length steps\n",
        "    if ppo_torch_agent.data.size() >= ppo_torch_args.traj_length:\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(episode_count)\n",
        "        print(f\"PPO agent trained for episode {episode_count}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        # --- Log to CSV ---\n",
        "        csv_writer.writerow([\n",
        "            episode_count,\n",
        "            float(episode_reward),\n",
        "            float(episode_north_reward),\n",
        "            float(episode_healthy_reward),\n",
        "            float(episode_ctrl_cost),\n",
        "            float(episode_sideways_cost),\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps\n",
        "        ])\n",
        "        csv_file.flush() # Ensure data is written to disk immediately\n",
        "        # --- End Log to CSV ---\n",
        "\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "# --- Close CSV file after training ---\n",
        "csv_file.close()\n",
        "print(f\"Training data saved to {csv_filename}\")\n",
        "# --- End Close CSV file ---\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "eval_env = envs.get_environment(env_name) # Create a new env for evaluation if needed\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Convert JAX observation to PyTorch tensor and add batch dimension\n",
        "    obs_torch_eval = torch.from_numpy(np.array(eval_state.obs)).float().to(device).unsqueeze(0)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Convert PyTorch action to JAX array and remove batch dimension for environment step\n",
        "    action_jax_eval = jnp.asarray(action_torch_eval.squeeze(0).cpu().numpy())\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68d6af6c"
      },
      "source": [
        "## Modify PPO.train_net to return detailed losses\n",
        "\n",
        "### Subtask:\n",
        "Adjust the `train_net` method in the `PPO` class (cell `SYyu-Ijsdp_q`) to calculate and return average `policy_loss_raw` (policy gradient part), `entropy_loss_term` (entropy regularization part), and the final `critic_loss` (value loss part, including clipping and scaling) over the training epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19fa5739"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `train_net` method in the `PPO` class to calculate and return average detailed loss components. I will update the code in cell `SYyu-Ijsdp_q` to implement this functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ac02d4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean() # Policy gradient part\n",
        "                entropy_loss_term = (-entropy).mean() # Entropy regularization part\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term # Total actor loss\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Append losses to lists\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "\n",
        "        # Calculate average losses over the epoch\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch)\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch)\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch)\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, writer, device, state_dim, action_dim, args):\n",
        "        super(PPO,self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.data = ReplayBuffer(action_prob_exist = True, max_size = self.args.traj_length, state_dim = state_dim, num_action = action_dim)\n",
        "        self.actor = Actor(self.args.layer_num, state_dim, action_dim, self.args.hidden_dim, \\\n",
        "                           self.args.activation_function,self.args.last_activation,self.args.trainable_std)\n",
        "        self.critic = Critic(self.args.layer_num, state_dim, 1, \\\n",
        "                             self.args.hidden_dim, self.args.activation_function,self.args.last_activation)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.args.actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.args.critic_lr)\n",
        "\n",
        "        self.writer = writer\n",
        "        self.device = device\n",
        "\n",
        "    def get_action(self,x):\n",
        "        mu,sigma = self.actor(x)\n",
        "        return mu,sigma\n",
        "\n",
        "    def v(self,x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def put_data(self,transition):\n",
        "        self.data.put_data(transition)\n",
        "\n",
        "    def get_gae(self, states, rewards, next_states, dones):\n",
        "        values = self.v(states).detach()  # (T, 1)\n",
        "        next_values = self.v(next_states).detach()  # (T, 1)\n",
        "\n",
        "        # TD errors: delta_t = r_t + gamma * V(s_{t+1}) * (1 - d_t) - V(s_t)\n",
        "        td_errors = rewards + self.args.gamma * next_values * (1 - dones) - values  # (T, 1)\n",
        "\n",
        "        advantages = torch.zeros_like(rewards).to(self.device)  # (T, 1)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        # Iterate backward to compute GAE: A_t = delta_t + gamma * lambda * A_{t+1} * (1 - d_t)\n",
        "        for t in reversed(range(len(td_errors))):\n",
        "            # (1 - dones[t]) acts as a mask, setting future advantage to 0 if terminal\n",
        "            advantages[t] = td_errors[t] + self.args.gamma * self.args.lambda_ * (1 - dones[t]) * last_gae_lam\n",
        "            last_gae_lam = advantages[t] # A_t becomes A_{t+1} for the next step\n",
        "\n",
        "        return values, advantages\n",
        "\n",
        "    def train_net(self,n_epi):\n",
        "        data = self.data.sample(shuffle = False)\n",
        "        states, actions, rewards, next_states, dones, old_log_probs = convert_to_tensor(self.device, data['state'], data['action'], data['reward'], data['next_state'], data['done'], data['log_prob'])\n",
        "\n",
        "        old_values, advantages = self.get_gae(states, rewards, next_states, dones)\n",
        "        returns = advantages + old_values\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-3)\n",
        "\n",
        "        # Initialize lists to store losses for averaging\n",
        "        policy_losses_raw_epoch = []\n",
        "        entropy_losses_term_epoch = []\n",
        "        critic_losses_epoch = []\n",
        "\n",
        "        for i in range(self.args.train_epoch):\n",
        "            for state,action,old_log_prob,advantage,return_,old_value \\\n",
        "            in make_mini_batch(self.args.batch_size, states, actions, \\\n",
        "                                           old_log_probs,advantages,returns,old_values):\n",
        "                curr_mu,curr_sigma = self.get_action(state)\n",
        "                value = self.v(state).float()\n",
        "                curr_dist = torch.distributions.Normal(curr_mu,curr_sigma)\n",
        "                entropy = curr_dist.entropy() * self.args.entropy_coef\n",
        "                curr_log_prob = curr_dist.log_prob(action).sum(1,keepdim = True)\n",
        "\n",
        "                #policy clipping\n",
        "                ratio = torch.exp(curr_log_prob - old_log_prob.detach())\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1-self.args.max_clip, 1+self.args.max_clip) * advantage\n",
        "                policy_loss_raw = (-torch.min(surr1, surr2)).mean() # Policy gradient part\n",
        "                entropy_loss_term = (-entropy).mean() # Entropy regularization part\n",
        "                actor_loss = policy_loss_raw + entropy_loss_term # Total actor loss\n",
        "\n",
        "                #value clipping (PPO2 technic)\n",
        "                old_value_clipped = old_value + (value - old_value).clamp(-self.args.max_clip,self.args.max_clip)\n",
        "                value_loss = (value - return_.detach().float()).pow(2)\n",
        "                value_loss_clipped = (old_value_clipped - return_.detach().float()).pow(2)\n",
        "                critic_loss = 0.5 * self.args.critic_coef * torch.max(value_loss,value_loss_clipped).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                self.critic_optimizer.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)\n",
        "                self.critic_optimizer.step()\n",
        "\n",
        "                # Append losses to lists\n",
        "                policy_losses_raw_epoch.append(policy_loss_raw.item())\n",
        "                entropy_losses_term_epoch.append(entropy_loss_term.item())\n",
        "                critic_losses_epoch.append(critic_loss.item())\n",
        "\n",
        "                if self.writer != None:\n",
        "                    self.writer.add_scalar(\"loss/actor_loss\", actor_loss.item(), n_epi)\n",
        "                    self.writer.add_scalar(\"loss/critic_loss\", critic_loss.item(), n_epi)\n",
        "\n",
        "        # Calculate average losses over the epoch\n",
        "        avg_policy_loss_raw = sum(policy_losses_raw_epoch) / len(policy_losses_raw_epoch)\n",
        "        avg_entropy_loss_term = sum(entropy_losses_term_epoch) / len(entropy_losses_term_epoch)\n",
        "        avg_critic_loss = sum(critic_losses_epoch) / len(critic_losses_epoch)\n",
        "\n",
        "        return avg_policy_loss_raw, avg_entropy_loss_term, avg_critic_loss\n"
      ],
      "metadata": {
        "id": "LFgz5qIsYe4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train parallel envs"
      ],
      "metadata": {
        "id": "2g_s_1C3IpAl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "766ab396"
      },
      "source": [
        "# Import necessary libraries\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "from brax import envs\n",
        "\n",
        "class ScaleRewardWrapper(envs.Wrapper):\n",
        "    def __init__(self, env, scale=0.1):\n",
        "        super().__init__(env)\n",
        "        self.scale = scale\n",
        "\n",
        "    def reset(self, rng):\n",
        "        state = super().reset(rng)\n",
        "        return state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        state = super().step(state, action)\n",
        "        # Skalujemy nagrodę (zmniejszamy ją)\n",
        "        return state.replace(reward=state.reward * self.scale)# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Create a simple config class to hold hyperparameters\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 64 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 1e-3\n",
        "        self.train_epoch = 1 # Number of PPO epochs\n",
        "        self.batch_size = 1024 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.2\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 6144 # Added: Number of parallel environments\n",
        "        self.num_updates = 100 # Added: Number of PPO updates (replaces episode_count limit)\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "# Modified: Use envs.create for batched environments\n",
        "env = envs.create(env_name=env_name, episode_length=2000, batch_size=ppo_torch_args.num_envs)\n",
        "env = ScaleRewardWrapper(env, scale=0.1)\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "# --- DEBUG: Print environment action size ---\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "# Added: Debug print for number of environments\n",
        "print(f\"DEBUG: Number of environments (batch_size): {env.batch_size}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now, as it's not provided in the notebook context\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "print(\"PyTorch PPO agent initialized.\")\n",
        "\n",
        "# --- CSV Logging Setup ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Define header for CSV\n",
        "# Modified: Use 'update_idx' and 'avg_' prefix for averaged metrics\n",
        "header = [\n",
        "    'update_idx', 'avg_total_reward', 'avg_north_reward', 'avg_healthy_reward',\n",
        "    'avg_ctrl_cost', 'avg_sideways_cost', 'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging training data to {csv_filename}\")\n",
        "# --- End CSV Logging Setup ---\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "current_total_steps = 0\n",
        "# Initialize rng for the loop outside of it\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "print(\"Starting PyTorch PPO training loop...\")\n",
        "\n",
        "# Initialise replay buffer with correct size\n",
        "ppo_torch_agent.data = ReplayBuffer(\n",
        "    action_prob_exist=True,\n",
        "    max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs, # Corrected max_size for batched collection\n",
        "    state_dim=env.observation_size,\n",
        "    num_action=env.action_size\n",
        ")\n",
        "\n",
        "# Outer loop for PPO updates\n",
        "for update_idx in range(ppo_torch_args.num_updates):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    # Initialize accumulators for metrics over the entire collected batch\n",
        "    total_sum_rewards_collected = 0.0\n",
        "    total_sum_north_rewards_collected = 0.0\n",
        "    total_sum_healthy_rewards_collected = 0.0\n",
        "    total_sum_ctrl_costs_collected = 0.0\n",
        "    total_sum_sideways_costs_collected = 0.0\n",
        "\n",
        "    # Collect for traj_length steps from num_envs parallel environments\n",
        "    for t in range(ppo_torch_args.traj_length):\n",
        "        # Modified: Use DLPack for zero-copy JAX to PyTorch transfer (fixed deprecated API)\n",
        "        obs_torch = torch.utils.dlpack.from_dlpack(env_state.obs).to(device)\n",
        "\n",
        "        # Get action from PyTorch actor (now takes a batch of observations)\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # Modified: Use DLPack for zero-copy PyTorch to JAX transfer (fixed deprecated API)\n",
        "        action_jax = jax.dlpack.from_dlpack(action_torch)\n",
        "\n",
        "        # Step JAX environment (now processes a batch of actions)\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Prepare numpy arrays for ReplayBuffer (for num_envs parallel transitions)\n",
        "        # and accumulate metrics for logging\n",
        "        current_obs_batch_np = obs_torch.cpu().numpy()\n",
        "        current_action_batch_np = action_torch.cpu().numpy()\n",
        "        current_log_prob_batch_np = log_prob_torch.cpu().numpy()\n",
        "\n",
        "        next_obs_batch_np = np.array(next_env_state.obs)\n",
        "        reward_batch_np = np.array(next_env_state.reward)\n",
        "        done_batch_np = np.array(next_env_state.done)\n",
        "\n",
        "        north_reward_batch_np = np.array(next_env_state.metrics['north_reward'])\n",
        "        healthy_reward_batch_np = np.array(next_env_state.metrics['reward_alive'])\n",
        "        ctrl_cost_batch_np = -np.array(next_env_state.metrics['reward_quadctrl']) # ctrl_cost is stored as negative in metrics\n",
        "        sideways_cost_batch_np = np.array(next_env_state.metrics['sideways_cost'])\n",
        "\n",
        "        # Store transitions in PyTorch ReplayBuffer and accumulate metrics\n",
        "        for env_idx in range(ppo_torch_args.num_envs):\n",
        "            # The `if env_idx == 0:` condition was removed, as all envs' metrics contribute to the sum.\n",
        "            transition = make_transition(\n",
        "                current_obs_batch_np[env_idx],\n",
        "                current_action_batch_np[env_idx],\n",
        "                reward_batch_np[env_idx].reshape(1),\n",
        "                next_obs_batch_np[env_idx],\n",
        "                done_batch_np[env_idx].reshape(1),\n",
        "                current_log_prob_batch_np[env_idx]\n",
        "            )\n",
        "            ppo_torch_agent.put_data(transition)\n",
        "\n",
        "            # Accumulate sum of rewards and costs across all environments\n",
        "            total_sum_rewards_collected += reward_batch_np[env_idx]\n",
        "            total_sum_north_rewards_collected += north_reward_batch_np[env_idx]\n",
        "            total_sum_healthy_rewards_collected += healthy_reward_batch_np[env_idx]\n",
        "            total_sum_ctrl_costs_collected += ctrl_cost_batch_np[env_idx]\n",
        "            total_sum_sideways_costs_collected += sideways_cost_batch_np[env_idx]\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += ppo_torch_args.num_envs # Increment total steps by the batch size\n",
        "\n",
        "    # Removed: Early exit and print logic for single environment, now fixed length collection\n",
        "    # The print statement now reflects completion of traj_length steps for all environments\n",
        "\n",
        "    # After collecting traj_length steps for all num_envs:\n",
        "    # Calculate average metrics over all collected transitions in this update\n",
        "    num_collected_transitions = ppo_torch_args.traj_length * ppo_torch_args.num_envs\n",
        "    avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "    avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "    avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "    avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "    avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "\n",
        "    # Modified: Print statements use averaged metrics\n",
        "    print(f\"Update {update_idx + 1} completed. Avg Total Reward: {avg_total_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "    print(f\"  Avg Reward Components: North Reward: {avg_north_reward:.2f}, Healthy Reward: {avg_healthy_reward:.2f}, Ctrl Cost: {avg_ctrl_cost:.2f}, Sideways Cost: {avg_sideways_cost:.2f}\")\n",
        "\n",
        "    # Train PPO agent after collecting traj_length * num_envs steps\n",
        "    # The buffer size `max_size` is now correctly `traj_length * num_envs`\n",
        "    if ppo_torch_agent.data.size() >= num_collected_transitions:\n",
        "        # Modified: Pass update_idx to train_net\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(update_idx)\n",
        "        print(f\"PPO agent trained for update {update_idx + 1}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        # --- Log to CSV ---\n",
        "        # Modified: Log averaged metrics\n",
        "        csv_writer.writerow([\n",
        "            update_idx + 1, # Log update_idx\n",
        "            float(avg_total_reward),\n",
        "            float(avg_north_reward),\n",
        "            float(avg_healthy_reward),\n",
        "            float(avg_ctrl_cost),\n",
        "            float(avg_sideways_cost),\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps\n",
        "        ])\n",
        "        csv_file.flush() # Ensure data is written to disk immediately\n",
        "        # --- End Log to CSV ---\n",
        "\n",
        "        # Clear the buffer for next rollout (on-policy)\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "# --- Close CSV file after training ---\n",
        "csv_file.close()\n",
        "print(f\"Training data saved to {csv_filename}\")\n",
        "# --- End Close CSV file ---\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with trained PyTorch PPO policy...\")\n",
        "# For evaluation, we still use a single environment for visualization consistency.\n",
        "eval_env = envs.create(env_name=env_name, episode_length=2000, batch_size=1) # Create a single env for evaluation\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 1000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    # Modified: Use DLPack for zero-copy JAX to PyTorch transfer in evaluation (fixed deprecated API)\n",
        "    obs_torch_eval = torch.utils.dlpack.from_dlpack(eval_state.obs).unsqueeze(0).to(device)\n",
        "\n",
        "    # Get action from PyTorch actor (deterministic for evaluation)\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval # Use mean for deterministic action\n",
        "\n",
        "    # Modified: Use DLPack for zero-copy PyTorch to JAX transfer in evaluation (fixed deprecated API)\n",
        "    action_jax_eval = jax.dlpack.from_dlpack(action_torch_eval.squeeze(0))\n",
        "\n",
        "    # Step JAX environment\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done[0]: # Check done for the first (and only) environment in eval batch\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model to path"
      ],
      "metadata": {
        "id": "GDUxLOQjcfPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a filename for the saved model\n",
        "model_save_path = \"trained_ppo_model.pth\"\n",
        "\n",
        "# Save the state dictionary of the PPO agent\n",
        "torch.save(ppo_torch_agent.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Trained PPO model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "zfO1dzXAFB-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (assuming they are already imported in previous cells)\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from brax import envs\n",
        "import torch\n",
        "import numpy as np\n",
        "import csv # Added for CSV logging\n",
        "from datetime import datetime # Added for unique filename\n",
        "\n",
        "# --- 1. Define Hyperparameters for the PyTorch PPO Agent ---\n",
        "# Re-instantiate PPOConfig if the kernel state might have been reset\n",
        "class PPOConfig:\n",
        "    def __init__(self):\n",
        "        self.traj_length = 8 # Number of steps to collect before update\n",
        "        self.layer_num = 2\n",
        "        self.hidden_dim = 256\n",
        "        self.activation_function = torch.tanh\n",
        "        self.last_activation = None\n",
        "        self.trainable_std = True\n",
        "        self.actor_lr = 3e-4\n",
        "        self.critic_lr = 1e-3\n",
        "        self.train_epoch = 4 # Number of PPO epochs\n",
        "        self.batch_size = 1024 # Minibatch size for update\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_ = 0.95\n",
        "        self.max_clip = 0.2\n",
        "        self.critic_coef = 0.5\n",
        "        self.entropy_coef = 0.2\n",
        "        self.max_grad_norm = 0.5\n",
        "        self.num_envs = 4096 # Number of parallel environments\n",
        "        self.num_updates = 100 # Default number of new PPO updates\n",
        "\n",
        "ppo_torch_args = PPOConfig()\n",
        "\n",
        "# --- 2. Initialize Brax Environment (JAX) ---\n",
        "env_name = 'humanoid'\n",
        "# Reuse the ScaleRewardWrapper if it was part of the original environment setup\n",
        "class ScaleRewardWrapper(envs.Wrapper):\n",
        "    def __init__(self, env, scale=0.1):\n",
        "        super().__init__(env)\n",
        "        self.scale = scale\n",
        "\n",
        "    def reset(self, rng):\n",
        "        state = super().reset(rng)\n",
        "        return state\n",
        "\n",
        "    def step(self, state, action):\n",
        "        state = super().step(state, action)\n",
        "        return state.replace(reward=state.reward * self.scale)\n",
        "\n",
        "env = envs.create(env_name=env_name, episode_length=500, batch_size=ppo_torch_args.num_envs)\n",
        "env = ScaleRewardWrapper(env, scale=0.1) # Applying a reward scale during new training\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n",
        "\n",
        "print(f\"DEBUG: env.observation_size: {env.observation_size}\")\n",
        "print(f\"DEBUG: env.action_size: {env.action_size}\")\n",
        "print(f\"DEBUG: env.sys.nu (num actuators): {env.sys.nu}\")\n",
        "print(f\"DEBUG: Number of environments (batch_size): {env.batch_size}\")\n",
        "\n",
        "# --- 3. Initialize PyTorch PPO Agent ---\n",
        "# Set device for PyTorch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using PyTorch device: {device}\")\n",
        "\n",
        "# Dummy writer for now\n",
        "class DummyWriter:\n",
        "    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n",
        "        pass\n",
        "writer = DummyWriter()\n",
        "\n",
        "ppo_torch_agent = PPO(\n",
        "    writer=writer,\n",
        "    device=device,\n",
        "    state_dim=env.observation_size,\n",
        "    action_dim=env.action_size,\n",
        "    args=ppo_torch_args\n",
        ")\n",
        "ppo_torch_agent.to(device)\n",
        "\n",
        "# Load the saved model state dictionary\n",
        "model_save_path = \"trained_ppo_model.pth\"\n",
        "try:\n",
        "    ppo_torch_agent.load_state_dict(torch.load(model_save_path))\n",
        "    print(f\"Successfully loaded model from {model_save_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Warning: Model file not found at {model_save_path}. Starting training from scratch.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}. Starting training from scratch.\")\n",
        "\n",
        "print(\"PyTorch PPO agent initialized (or loaded).\")\n",
        "\n",
        "# --- CSV Logging Setup for new training session ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "csv_filename = f\"ppo_training_log_resumed_{timestamp}.csv\"\n",
        "csv_file = open(csv_filename, 'w', newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "\n",
        "# Modified: Added 'avg_episode_length' to the header\n",
        "header = [\n",
        "    'update_idx', 'avg_total_reward', 'avg_north_reward', 'avg_healthy_reward',\n",
        "    'avg_ctrl_cost', 'avg_sideways_cost', 'avg_orientation_cost', 'avg_z_angular_cost', # Added new costs\n",
        "    'policy_gradient_loss',\n",
        "    'entropy_loss', 'value_loss', 'total_steps', 'avg_episode_length'\n",
        "]\n",
        "csv_writer.writerow(header)\n",
        "print(f\"Logging new training data to {csv_filename}\")\n",
        "\n",
        "# --- 4. Main Training Loop ---\n",
        "current_total_steps = 0\n",
        "rng = jax.random.PRNGKey(0) # Re-initialize RNG for this training session\n",
        "\n",
        "# Added: Initialize episode_step_counts for each parallel environment\n",
        "episode_step_counts = jnp.zeros(ppo_torch_args.num_envs, dtype=jnp.int32)\n",
        "\n",
        "print(\"Starting new PyTorch PPO training loop...\")\n",
        "\n",
        "ppo_torch_agent.data = ReplayBuffer(\n",
        "    action_prob_exist=True,\n",
        "    max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "    state_dim=env.observation_size,\n",
        "    num_action=env.action_size\n",
        ")\n",
        "\n",
        "for update_idx in range(ppo_torch_args.num_updates):\n",
        "    rng, reset_rng = jax.random.split(rng)\n",
        "    env_state = jit_reset(reset_rng)\n",
        "\n",
        "    total_sum_rewards_collected = 0.0\n",
        "    total_sum_north_rewards_collected = 0.0\n",
        "    total_sum_healthy_rewards_collected = 0.0\n",
        "    total_sum_ctrl_costs_collected = 0.0\n",
        "    total_sum_sideways_costs_collected = 0.0\n",
        "    total_sum_orientation_costs_collected = 0.0 # Added\n",
        "    total_sum_z_angular_costs_collected = 0.0 # Added\n",
        "    # Added: Accumulators for average episode length\n",
        "    total_episode_lengths_collected = 0\n",
        "    terminated_episodes_count = 0\n",
        "\n",
        "    for t in range(ppo_torch_args.traj_length):\n",
        "        obs_torch = torch.utils.dlpack.from_dlpack(env_state.obs).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mu, sigma = ppo_torch_agent.get_action(obs_torch)\n",
        "            action_dist = torch.distributions.Normal(mu, sigma)\n",
        "            action_torch = action_dist.sample()\n",
        "            log_prob_torch = action_dist.log_prob(action_torch).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        action_jax = jax.dlpack.from_dlpack(action_torch)\n",
        "\n",
        "        rng, step_rng = jax.random.split(rng)\n",
        "        next_env_state = jit_step(env_state, action_jax)\n",
        "\n",
        "        # Added: Update episode_step_counts and accumulate terminated episode lengths\n",
        "        # Increment step counts for all active environments\n",
        "        episode_step_counts = episode_step_counts + 1\n",
        "\n",
        "        # Identify environments that terminated in this step\n",
        "        terminated_mask = next_env_state.done # This is a JAX array of booleans\n",
        "\n",
        "        # Add lengths of newly terminated episodes to total_episode_lengths_collected\n",
        "        # Only sum episode_step_counts for environments where terminated_mask is True\n",
        "        terminated_lengths_this_step = episode_step_counts * terminated_mask\n",
        "        total_episode_lengths_collected += jnp.sum(terminated_lengths_this_step).item() # .item() to convert JAX scalar to Python scalar\n",
        "        terminated_episodes_count += jnp.sum(terminated_mask).item() # .item()\n",
        "\n",
        "        # Reset step counts for environments that terminated\n",
        "        episode_step_counts = episode_step_counts * (1 - terminated_mask)\n",
        "\n",
        "        current_obs_batch_np = obs_torch.cpu().numpy()\n",
        "        current_action_batch_np = action_torch.cpu().numpy()\n",
        "        current_log_prob_batch_np = log_prob_torch.cpu().numpy()\n",
        "\n",
        "        next_obs_batch_np = np.array(next_env_state.obs)\n",
        "        reward_batch_np = np.array(next_env_state.reward)\n",
        "        done_batch_np = np.array(next_env_state.done)\n",
        "\n",
        "        north_reward_batch_np = np.array(next_env_state.metrics['north_reward'])\n",
        "        healthy_reward_batch_np = np.array(next_env_state.metrics['reward_alive'])\n",
        "        ctrl_cost_batch_np = -np.array(next_env_state.metrics['reward_quadctrl'])\n",
        "        sideways_cost_batch_np = np.array(next_env_state.metrics['sideways_cost'])\n",
        "        orientation_cost_batch_np = np.array(next_env_state.metrics['orientation_cost']) # Added\n",
        "        z_angular_cost_batch_np = np.array(next_env_state.metrics['z_angular_cost']) # Added\n",
        "\n",
        "        for env_idx in range(ppo_torch_args.num_envs):\n",
        "            transition = make_transition(\n",
        "                current_obs_batch_np[env_idx],\n",
        "                current_action_batch_np[env_idx],\n",
        "                reward_batch_np[env_idx].reshape(1),\n",
        "                next_obs_batch_np[env_idx],\n",
        "                done_batch_np[env_idx].reshape(1),\n",
        "                current_log_prob_batch_np[env_idx]\n",
        "            )\n",
        "            ppo_torch_agent.put_data(transition)\n",
        "\n",
        "            total_sum_rewards_collected += reward_batch_np[env_idx]\n",
        "            total_sum_north_rewards_collected += north_reward_batch_np[env_idx]\n",
        "            total_sum_healthy_rewards_collected += healthy_reward_batch_np[env_idx]\n",
        "            total_sum_ctrl_costs_collected += ctrl_cost_batch_np[env_idx]\n",
        "            total_sum_sideways_costs_collected += sideways_cost_batch_np[env_idx]\n",
        "            total_sum_orientation_costs_collected += orientation_cost_batch_np[env_idx] # Added\n",
        "            total_sum_z_angular_costs_collected += z_angular_cost_batch_np[env_idx] # Added\n",
        "\n",
        "        env_state = next_env_state\n",
        "        current_total_steps += ppo_torch_args.num_envs\n",
        "\n",
        "    num_collected_transitions = ppo_torch_args.traj_length * ppo_torch_args.num_envs\n",
        "    avg_total_reward = total_sum_rewards_collected / num_collected_transitions\n",
        "    avg_north_reward = total_sum_north_rewards_collected / num_collected_transitions\n",
        "    avg_healthy_reward = total_sum_healthy_rewards_collected / num_collected_transitions\n",
        "    avg_ctrl_cost = total_sum_ctrl_costs_collected / num_collected_transitions\n",
        "    avg_sideways_cost = total_sum_sideways_costs_collected / num_collected_transitions\n",
        "    avg_orientation_cost = total_sum_orientation_costs_collected / num_collected_transitions # Added\n",
        "    avg_z_angular_cost = total_sum_z_angular_costs_collected / num_collected_transitions # Added\n",
        "\n",
        "    # Added: Calculate average episode length\n",
        "    avg_episode_length = total_episode_lengths_collected / terminated_episodes_count if terminated_episodes_count > 0 else 0.0\n",
        "\n",
        "    print(f\"Update {update_idx + 1} completed. Avg Total Reward: {avg_total_reward:.2f}. Total steps: {current_total_steps}\")\n",
        "    print(f\"  Avg Reward Components: North Reward: {avg_north_reward:.2f}, Healthy Reward: {avg_healthy_reward:.2f}, Ctrl Cost: {avg_ctrl_cost:.2f}, Sideways Cost: {avg_sideways_cost:.2f}\")\n",
        "    print(f\"  Added Costs: Orientation Cost: {avg_orientation_cost:.2f}, Z Angular Cost: {avg_z_angular_cost:.2f}\") # Added\n",
        "    # Added: Print average episode length\n",
        "    print(f\"  Avg Episode Length: {avg_episode_length:.2f} (from {terminated_episodes_count} terminations)\")\n",
        "\n",
        "    if ppo_torch_agent.data.size() >= num_collected_transitions:\n",
        "        policy_pg_loss, entropy_term_loss, value_fn_loss = ppo_torch_agent.train_net(update_idx)\n",
        "        print(f\"PPO agent trained for update {update_idx + 1}. Losses: PG={policy_pg_loss:.4f}, Ent={entropy_term_loss:.4f}, Val={value_fn_loss:.4f}\")\n",
        "\n",
        "        csv_writer.writerow([\n",
        "            update_idx + 1,\n",
        "            float(avg_total_reward),\n",
        "            float(avg_north_reward),\n",
        "            float(avg_healthy_reward),\n",
        "            float(avg_ctrl_cost),\n",
        "            float(avg_sideways_cost),\n",
        "            float(avg_orientation_cost), # Added\n",
        "            float(avg_z_angular_cost), # Added\n",
        "            policy_pg_loss,\n",
        "            entropy_term_loss,\n",
        "            value_fn_loss,\n",
        "            current_total_steps,\n",
        "            float(avg_episode_length) # Added: Log average episode length\n",
        "        ])\n",
        "        csv_file.flush()\n",
        "\n",
        "        ppo_torch_agent.data = ReplayBuffer(\n",
        "            action_prob_exist=True,\n",
        "            max_size=ppo_torch_args.traj_length * ppo_torch_args.num_envs,\n",
        "            state_dim=env.observation_size,\n",
        "            num_action=env.action_size\n",
        "        )\n",
        "\n",
        "csv_file.close()\n",
        "print(f\"New training data saved to {csv_filename}\")\n",
        "\n",
        "print(\"\\nPyTorch PPO training finished.\")\n",
        "\n",
        "# --- 5. Visualize Trained Policy (PyTorch PPO) ---\n",
        "print(\"\\nGenerating rollout with newly trained PyTorch PPO policy...\")\n",
        "eval_env = envs.create(env_name=env_name, episode_length=2000, batch_size=1)\n",
        "jit_reset_eval = jax.jit(eval_env.reset)\n",
        "jit_step_eval = jax.jit(eval_env.step)\n",
        "\n",
        "rng, eval_rng = jax.random.split(rng)\n",
        "eval_state = jit_reset_eval(eval_rng)\n",
        "rollout_eval_torch = [eval_state.pipeline_state]\n",
        "\n",
        "n_steps_eval = 1000\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps_eval):\n",
        "    obs_torch_eval = torch.utils.dlpack.from_dlpack(eval_state.obs).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu_eval, _ = ppo_torch_agent.get_action(obs_torch_eval)\n",
        "        action_torch_eval = mu_eval\n",
        "\n",
        "    action_jax_eval = jax.dlpack.from_dlpack(action_torch_eval.squeeze(0))\n",
        "\n",
        "    eval_state = jit_step_eval(eval_state, action_jax_eval)\n",
        "    rollout_eval_torch.append(eval_state.pipeline_state)\n",
        "\n",
        "    if eval_state.done[0]:\n",
        "        print(f\"Evaluation episode finished early at step {i+1}\")\n",
        "        break\n",
        "\n",
        "print(\"Rendering video from new PyTorch PPO trained policy...\")\n",
        "media.show_video(eval_env.render(rollout_eval_torch[::render_every]), fps=1.0 / eval_env.dt / render_every)\n"
      ],
      "metadata": {
        "id": "FBBQ73FRcW_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZVX-ZYwiBtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}